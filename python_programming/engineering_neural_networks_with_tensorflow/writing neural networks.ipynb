{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b7536b",
   "metadata": {},
   "source": [
    "# Writing Neural Networks\n",
    "\n",
    "Writing neural networks using tensorflow's low level api is an essential skill for implementing most research papers.  The ability to translate from mathematical equations into code, is incredibly powerful.  Not only because you can take the work of others to a functional place, but because you can come up with your own ideas, mathematically, prove some properties and then do an implementation.  The process of reading and understanding techniques through mathematics and then implementing them in code is the most productive way to make progress in your development as a scientist and researcher.  It's also very useful for being an effective engineer.  Through mathematics we can quickly understand and verify that some property ought to hold.  Once you understand the definitions and consequences of an idea in mathematics, verification of it's validity often takes a day or so, much faster than the verification of scientific quantities, because mathematics works in the world of absolute truth.  This means there aren't really grey areas, so you can make definitive statements.  While in general, science does not follow this binary notion of absolutes, hence the extensive use of statistics, scientific tools are primarily written in the language of mathematics.  A given idea or theory (of the real world) may seem enticing, but without scientific evidence to back it up, the theory means little. \n",
    "\n",
    "That said, once a theory can be validated through statistical analysis, it can be folded into a theoretical analysis.  It is from time to time even possible to draw conclusions beyond the phenomenon of inquiry which guides future discovery.  This has been seen several times from the world of mathematical physics.  One striking example of this is the discovery of blackholes which were theorized a century before they were physically discovered.\n",
    "\n",
    "Note: If you don't know how to write tensorflow code, please see [this](https://github.com/EricSchles/datascience_book/blob/master/python_programming/tensorflow_basics/Tensorflow%20Basics.ipynb) notebook.  \n",
    "\n",
    "## The Neural Network framework\n",
    "\n",
    "If you aren't already familar with neural networks, I suggest reading [this chapter](https://github.com/EricSchles/datascience_book/blob/master/5/An%20Introduction%20to%20Neural%20Networks%20-%2007.ipynb) first.  \n",
    "\n",
    "In general, neural nets are essentially a collection of linear regression models tied together through a meta optimization algorithm called backpropagation.  The linear regression models are sometimes tied together with an 'activation' function, which is just a secondary transform applied after the linear regression optimization takes place.  We've already seen that we can use stochastic gradient descent to optimize a linear regression model [here](https://github.com/EricSchles/datascience_book/blob/master/2/An%20Introduction%20to%20Regression%20-%2003.ipynb) and that there is a linear algebra equivalent [here](https://github.com/EricSchles/datascience_book/blob/master/5/An%20Introduction%20to%20Neural%20Networks%20-%2007.ipynb).  Neural networks essentially tie these two ideas together and do _both_ optimization strategies.  The linear algebra optimization strategy happens locally with the so called 'forward pass' and then the gradient is used explicitly to update the weights on the 'backward pass'.  But keep in mind there are two optimizations working together in tandem.  \n",
    "\n",
    "So really a neural network is just an ensemble of sort of linear models or models with easy derivates and each 'layer' of the network is just a given model, optimizing a bit of the ensemble in an explicit way.  The power of neural networks come from their flexability.  Unlike random forests or gradient boosted trees which either optimize in parallel or in sequence, neural networks can do both.  Some of the layers can optimize for certain inputs and others can optimize for others.  Or we can feed copies of the same data to a very wide neural network, which essentially acts like a random forest.  We'll see a number of architectures and ideas in this chapter for how to write down different neural network architectures, EVEN if they aren't necessarily useful, it's still good practice to see how to work with these different tools.\n",
    "\n",
    "## A first primitive example\n",
    "\n",
    "Our first example is going to be a purely linear model that simply does essentially linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c7f3f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: tf.Tensor(27385605000000.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, shape, learning_rate=0.5):\n",
    "        array = np.random.normal(0, 1, size=shape)\n",
    "        array = tf.cast(array, tf.float32)\n",
    "        self.weights = tf.Variable(array)\n",
    "        self.bias = tf.Variable(1.0)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return tf.tensordot(x, self.weights, axes=1) + self.bias\n",
    "    \n",
    "    def loss(self, y_pred, y_true):\n",
    "        return tf.reduce_mean(\n",
    "           tf.square(y_pred - y_true)\n",
    "        )\n",
    "    \n",
    "    def update_weights(self, X_train, y_true):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self.predict(X_train)\n",
    "            loss = self.loss(y_pred, y_true)\n",
    "        gradients = tape.gradient(\n",
    "            loss, [self.weights, self.bias]\n",
    "        )\n",
    "        self.weights.assign_sub(gradients[0] * self.learning_rate)\n",
    "        self.bias.assign_sub(gradients[1] * self.learning_rate)\n",
    "\n",
    "X, y = make_regression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "epochs = 10\n",
    "lin_reg = LinearRegression(X_train.shape[1])\n",
    "X_train = tf.cast(\n",
    "    tf.constant(X_train), \n",
    "    tf.float32\n",
    ")\n",
    "X_test = tf.cast(\n",
    "    tf.constant(X_test),\n",
    "    tf.float32\n",
    ")\n",
    "y_train = tf.cast(\n",
    "    tf.constant(y_train),\n",
    "    tf.float32\n",
    ")\n",
    "y_test = tf.cast(\n",
    "    tf.constant(y_test),\n",
    "    tf.float32\n",
    ")\n",
    "for i in range(epochs):\n",
    "    lin_reg.update_weights(X_train, y_train)\n",
    "    \n",
    "y_pred = lin_reg.predict(X_test)\n",
    "loss = lin_reg.loss(y_pred, y_test)\n",
    "print(\"MSE:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e99a5c",
   "metadata": {},
   "source": [
    "There are a couple of things to note here which are mandatory in order for tensorflow to train a mode successfully:\n",
    "\n",
    "1. You must cast everything to the same type - as an exercise try copy/pasting the above code into a new cell and remove all the type casting (the code with tf.cast).  It fails because all the X, y data is treated as doubles, not floats.\n",
    "\n",
    "2. You must call predict and your loss function inside of the gradient tape context but must apply your gradient updates outside the gradient tape context.  This choice has always felt somewhat arbitrary to me.  However, because of how distributed training works and the fact that only some variables are *trainable* while others are frozen, we need some way to manage state.  It's an annoying trade off, but it needed to be made somewhere.  Fortunately, the code is not particularly ugly, just very pedantic\n",
    "\n",
    "3. In general tensors products are not commutative, so the order of your parameters in your tensor product matter.  This can be seen in the predict function defined above.  If we tried changing around the order then our code would not work.\n",
    "\n",
    "4. Notice the use of tf.Variable for our weights and bias term.  If we made those tf.constant instead, then our gradient tape wouldn't let us update our variables.\n",
    "\n",
    "These basic concerns may seem like an impedement, but this code is far cleaner and scalable than the vanilla numpy implementation found [here in the section: A Naive Implementation of a Neural Network](https://github.com/EricSchles/datascience_book/blob/master/5/An%20Introduction%20to%20Neural%20Networks%20-%2007.ipynb).  Also, we don't need to worry about figuring out the derivative for each of our activation functions.  While some people love calculus (like myself, yes even in higher dimensions), many programmers don't.  Which is why automatic differentiation implemented in packages like tensorflow has openned up a world of mathematics to programmers.  \n",
    "\n",
    "Now that we've seen how the tensorflow framework can be used to train a neural network, let's do a multilayer preceptron.  This will be our first multilayer neural network or ensemble of layers.  We'll look at a deep neural network first, deep because it trains the layers in sequence.  And then we'll look at a wide neural network, because it will train the layers in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f3f0337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Variable:0' shape=(100, 100) dtype=float32, numpy=\n",
      "array([[-0.7212728 ,  1.1348922 ,  0.61802864, ...,  0.22765778,\n",
      "        -1.1133646 , -1.440366  ],\n",
      "       [ 1.4324218 , -0.60617477, -0.01505937, ..., -0.30103448,\n",
      "         0.7750409 ,  0.25534207],\n",
      "       [-1.3063195 , -0.8149815 ,  0.72941184, ..., -1.0462462 ,\n",
      "        -0.8580679 , -0.54124355],\n",
      "       ...,\n",
      "       [-1.530326  ,  0.01813192,  1.3527733 , ..., -1.6122704 ,\n",
      "         0.2747804 , -0.83125126],\n",
      "       [-0.5919334 , -0.40140167,  0.61836475, ..., -2.0201757 ,\n",
      "         1.2274144 ,  0.26729208],\n",
      "       [-1.280319  ,  1.3908894 , -0.7895513 , ..., -0.6349584 ,\n",
      "        -0.02296463,  0.11261811]], dtype=float32)>, <tf.Variable 'Variable:0' shape=(100, 100) dtype=float32, numpy=\n",
      "array([[ 0.24141675, -0.05211819,  1.6133287 , ..., -1.0225772 ,\n",
      "         0.29146394, -0.07150515],\n",
      "       [-0.7354816 ,  0.9732139 , -1.4986281 , ...,  0.17546143,\n",
      "         0.35929096, -1.2626256 ],\n",
      "       [-1.1389118 ,  0.25035045,  0.87148666, ..., -0.21154784,\n",
      "        -0.7046645 ,  0.9457408 ],\n",
      "       ...,\n",
      "       [ 0.12367791,  0.59043515,  0.45267135, ...,  0.8862408 ,\n",
      "         0.09009622,  0.27479312],\n",
      "       [-0.0099059 , -0.8839648 , -1.1433644 , ...,  0.18161796,\n",
      "        -0.43514585, -0.5853212 ],\n",
      "       [ 0.31005695, -0.02586419,  1.0871719 , ..., -0.9050438 ,\n",
      "        -0.24136585,  0.9024862 ]], dtype=float32)>, <tf.Variable 'Variable:0' shape=(100,) dtype=float32, numpy=\n",
      "array([ 0.07432874,  0.01261214,  0.49837747, -0.27405792, -1.8560808 ,\n",
      "        1.1271025 ,  2.6099792 ,  0.96671945,  0.14501268,  0.6016628 ,\n",
      "       -1.8609586 ,  0.8569713 ,  1.4927222 ,  1.0480088 , -0.9183509 ,\n",
      "        0.83021295,  2.3219593 , -0.7010747 , -0.3116421 , -0.06881578,\n",
      "        0.25993192,  0.96756935, -2.6785333 , -1.63      , -0.81057   ,\n",
      "       -0.26618424,  1.5306072 ,  0.8697262 ,  1.3329741 , -1.1018013 ,\n",
      "        1.2368717 , -0.7316104 , -0.19984156,  0.38585782, -0.7776979 ,\n",
      "        0.13862835, -0.13959643, -0.173952  , -0.5821878 ,  0.39241144,\n",
      "        0.6012204 ,  0.25906137, -1.3825555 , -1.6481147 ,  0.7164532 ,\n",
      "        0.41645065,  0.999816  ,  0.94395775, -1.2413324 ,  0.65325344,\n",
      "       -0.67841685,  1.4855471 ,  0.15707877,  0.52572435,  0.1853147 ,\n",
      "        0.978866  ,  0.16064095, -0.03190925, -1.0071989 ,  0.10842206,\n",
      "        2.420766  ,  0.31112012, -1.85025   , -0.40754566, -0.61951   ,\n",
      "       -1.9932455 ,  0.2387607 , -1.4563411 ,  1.9385277 , -1.0212802 ,\n",
      "        0.45934716,  1.0167506 ,  1.9324484 , -1.5942162 ,  0.1074748 ,\n",
      "        0.6097013 ,  0.29597896,  0.28502315, -0.56588066, -0.5579182 ,\n",
      "        0.10456559,  1.8317722 ,  1.1776414 ,  1.004412  ,  0.37365338,\n",
      "       -1.0874424 , -0.12696198, -0.35671985,  0.21009296, -0.32701078,\n",
      "       -0.32021344,  0.4114353 , -0.32931125,  0.91340846,  1.3790436 ,\n",
      "       -0.13364021,  1.5713866 , -1.3786983 ,  0.070533  ,  0.64727163],\n",
      "      dtype=float32)>, <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>, <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>, <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 13:28:29.149922: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x110dd7ba0\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "could not find registered platform with id: 0x110dd7ba0 [Op:__inference__update_step_xla_647]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps):\n\u001b[0;32m---> 80\u001b[0m     \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     pred \u001b[38;5;241m=\u001b[39m nn(X_test)\n\u001b[1;32m     82\u001b[0m     loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mloss(pred, y_test)\n",
      "Cell \u001b[0;32mIn [4], line 69\u001b[0m, in \u001b[0;36mNeuralNet.step\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(trainable_variables)\n\u001b[1;32m     68\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, trainable_variables)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1140\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_gradients_aggregation \u001b[38;5;129;01mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[1;32m   1139\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_gradients(grads_and_vars)\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:634\u001b[0m, in \u001b[0;36m_BaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    633\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables))\n\u001b[0;32m--> 634\u001b[0m iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1166\u001b[0m, in \u001b[0;36mOptimizer._internal_apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_internal_apply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[0;32m-> 1166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_merge_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_apply_gradients_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribution_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/distribute/merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[0;32m---> 51\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m distribution_strategy_context\u001b[38;5;241m.\u001b[39mget_replica_context()\u001b[38;5;241m.\u001b[39mmerge_call(\n\u001b[1;32m     54\u001b[0m       fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1216\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn\u001b[0;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step(grad, var)\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[0;32m-> 1216\u001b[0m     \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_grad_to_update_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[1;32m   1221\u001b[0m     _, var_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:2637\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2634\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   2635\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   2636\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 2637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2638\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2639\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[1;32m   2640\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:3710\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[1;32m   3708\u001b[0m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[1;32m   3709\u001b[0m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[0;32m-> 3710\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_non_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:3716\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   3712\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[1;32m   3713\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[1;32m   3714\u001b[0m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[1;32m   3715\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[0;32m-> 3716\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[1;32m   3718\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:595\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    594\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1211\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_grad_to_update_var\u001b[39m(var, grad):\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit_compile:\n\u001b[0;32m-> 1211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_step_xla\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_var_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step(grad, var)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mNotFoundError\u001b[0m: could not find registered platform with id: 0x110dd7ba0 [Op:__inference__update_step_xla_647]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Dense(tf.Module):\n",
    "    def __init__(self, output_size, name=None, final=False):\n",
    "        super().__init__(name=name)\n",
    "        if final:\n",
    "            weights = np.random.normal(0, 1, size=output_size)\n",
    "            weights = tf.cast(weights, tf.float32)\n",
    "            self.weights = tf.Variable(weights)\n",
    "        else:\n",
    "            weights = np.random.normal(0, 1, size=output_size * output_size)\n",
    "            weights = weights.reshape(output_size, output_size)\n",
    "            weights = tf.cast(weights, tf.float32)\n",
    "            self.weights = tf.Variable(weights)\n",
    "        #bias = np.array([1.0 for _ in range(output_size)])\n",
    "        self.bias = tf.Variable(1.0)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return tf.tensordot(x, self.weights, axes=1) + self.bias\n",
    "\n",
    "        \n",
    "class NeuralNet(tf.Module):\n",
    "    def __init__(self, X_in, X_out, optimizer):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer_one = Dense(X_out)\n",
    "        self.layer_two = Dense(X_out)\n",
    "        self.layer_three = Dense(X_out, final=True)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def _collect_trainable_variables(self):\n",
    "        return [\n",
    "            self.layer_one.weights,\n",
    "            self.layer_two.weights,\n",
    "            self.layer_three.weights,\n",
    "            self.layer_one.bias,\n",
    "            self.layer_two.bias,\n",
    "            self.layer_three.bias\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.predict(x)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # multiple layers go here\n",
    "        res = self.layer_one(x)\n",
    "        res = self.layer_two(res)\n",
    "        return self.layer_three(res)\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        return tf.reduce_mean(\n",
    "           tf.square(y_pred - y_true)\n",
    "        )\n",
    "    \n",
    "    def step(self, x, y):\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        y = tf.cast(y, tf.float32)\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self.predict(x)\n",
    "            loss = self.loss(pred, y)\n",
    "        trainable_variables = self._collect_trainable_variables()\n",
    "        print(trainable_variables)\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X, y = make_regression(n_samples=1000, n_features=100)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    learning_rate = 0.9\n",
    "    optimizer = tf.optimizers.Adam(learning_rate)\n",
    "    nn = NeuralNet(1000, X_train.shape[1], optimizer)\n",
    "    num_steps = 110\n",
    "    losses = []\n",
    "    for step in range(num_steps):\n",
    "        nn.step(X_train, y_train)\n",
    "        pred = nn(X_test)\n",
    "        loss = nn.loss(pred, y_test)\n",
    "        losses.append(loss)\n",
    "    plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d8d30c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
