{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8de1f039",
   "metadata": {},
   "source": [
    "# Writing Neural Networks\n",
    "\n",
    "In part one of writing neural networks we saw how to use tensorflow to write linear regression, convolutional neural networks and residual networks.  In this section we will introduce another major paradigm in neural networks - recurrence.  \n",
    "\n",
    "There are many kinds of recurrent neural networks, which serve as refinements and adjustments to the base framing.  Before we go through the details, I want to motivate this new class of networks by returning to the discussion in part one on the structure of data.  In theory, a multilayer perceptron can learn any pattern, given enough data, an infinite amount of time and a computer with infinite memory and computational speed.  That is, neural networks are so called universal approximators.  They aren't the only models that are thought to be as such either.  Technical tree based models are considered universal approximators as well.  Theoretically, this means you don't need information about the underlying structure of the data, for instance, you don't need to know that the data is normally distributed.  And in fact, neural networks and tree based models are considered 'parameter' free because they can approximate data from any distribution.  \n",
    "\n",
    "This is all well and good in theory, but as you remove assumptions like infinite memory, infinite time, infinite computational speed and all the data in the world; these claims start to fade.  While it is true that these two classes of models do work quiet well, the theoretical claims begin to seem meaningless in the face of real world constraints.  The original rise and fall of artificial intelligence in computational systems may in fact be due to the boldness of the claims around these universal approximators.  \n",
    "\n",
    "That said, through a series of engineering refinements and grounding these models in more realistic considerations, it is possible to get exceptional performance (in terms of accuracy and correctness) with these two classes of models.  The primary way in which this is done is by imposing a different kind of structural constraint on the models in question.  So it turns out, even though the constraint of a parametric model may be overkill (that is a model where the data is assumed to come from one and only one distribution), non-parametric models may still do well with some very specific structure.  \n",
    "\n",
    "In the last section we saw one structure - the convolutional neural network.  And technically, we saw additional structure through skip connections in the residual network.  By imposing extra structure on our models, and by assuming extra structure on our data, it is possible to get good performance on practical basis without strong theoretical assumptions.  \n",
    "\n",
    "Now that we've talked through the high level motivations, let's get practical.  A recurrent neural network sets up a recurrence relation, updating our representation by forward propagating a number of times in sequence and then updating each of the update steps through a technique called back propagation in time.  \n",
    "\n",
    "Personally, I find the diagramatic approach for understanding recurrent neural networks difficult to follow, so let's look at some code first and then we can look at a visual approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7f9ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
