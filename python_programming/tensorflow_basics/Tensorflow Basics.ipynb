{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tensorflow Basics\n",
    "\n",
    "This first notebook will introduce the basics of the tensorflow api, it will cover largerly the same material as the numpy section since I want to showcase the similarities of the two libraries first.  \n",
    "\n",
    "* why tensorflow?\n",
    "* introduction to tensors\n",
    "* tensorflow shapes\n",
    "* tensorflow slicing\n",
    "* tensorflow querying\n",
    "* linear algebra in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Tensorflow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In the previous section on numpy basics our answer was straight forward, why numpy?  Because it's fast and it lets you do vector programming.  But numpy does vector programming on a single computer, you cannot do anything distributed and there is no current support for GPU processing.  That is not the case for pytorch and tensorflow (to name a few).  On a fundamental level there isn't much different between these two libraries and numpy.  And in fact, whenever necessary we will still use numpy to do things.  Tensorflow or pytorch is not always faster than numpy but sometimes they are.  I am choosing tensorflow for this section because it's what I personally know best.  Both libraries are more or less equally good.\n",
    "\n",
    "That said, the research community does tend to favor pytorch over tensorflow, at least as of this writing.  Perhaps JAX will be the next thing.  Who's to say?  \n",
    "\n",
    "In any event, tensorflow lets you do vector programming just like numpy except in addition to all this speed and power, higher order tensors are more typical and expected in tensorflow (and pytorch) far less of an after thought.  And the api has more support for this as a consequence.  \n",
    "\n",
    "That said, the axis argument more or less accomplishes this with numpy, but numpy is more opinionated about which axis to operate over than tensorflow.  That is more of a feeling than anything I want to back up concretely.  And it is purely meant as a juxtaposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To really understand tensorflow and the power it brings, we need to understand tensors.  Because without them, tensorflow honestly doesn't make much sense, at least at first.  And even once you start to get used to the syntax, without the mental model of a tensor, you'll completely miss the point of using it.  \n",
    "\n",
    "So even though some of the writing here will be a straight lift from the numpy section, it's still worth it to go through the same examples because we'll be dealing with a new api."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Tensors are some of the most powerful objects around.  In fact, this book is basically just a \"how do I use tensors\" most of the time.  The chapter on linear regression?  That's just about tensors.  The chapter on classification?  More applications of tensors.  Much of machine learning is built on tensors.  Specifically, on matrices.  Because I define the matrix in another chapter, I won't go into a ton of detail about what they are, or how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* scalars\n",
    "* vectors\n",
    "* matrices\n",
    "* order-3 tensors and higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A scalar is an order zero tensor - because it's just a single number, like say the number `5`.  A vector is a one dimensional collection of numbers representing data or an equation, like: \n",
    "\n",
    "$$ \\begin{pmatrix}\n",
    "1  \\\\\n",
    "4  \\\\\n",
    "7 \n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A matrix is a two dimensional collect of numbers representing a system of equations, like:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An order three tensor looks like a data cube.  There is no easy way to show such a cube in latex, so you'll have to imagine this to some extent:\n",
    "\n",
    "$$ A_{1} = \n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$ A_{2} = \n",
    "\\begin{pmatrix}\n",
    "3 & 2 & 3 \\\\\n",
    "7 & 6 & 6 \\\\\n",
    "7 & 2 & 9\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$ A_{3} = \n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "7 & 6 & 4 \\\\\n",
    "6 & 2 & 19\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now imagine $A_{1}, A_{2}, A_{3}$ as one object.  This is an order three tensor.  It has three axes - $(i,j,k)$ and you can specify elements across these three axes.  So $(0,0,0) = 1$, $(1,0,0) = 4$, and $(3,0,3) = 6$.  Here the i is the row index, j is the column index and k is the matrix index.  You can also do this for order 4 and up to n, where n is any finite natural number you like.  Why might you want to ever do this in practice?  It turns out there are actually a ton of good reasons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here are just two of them:\n",
    "\n",
    "1. Let's say you want to model multivariate timeseries geospatial data.  This is naturally an order 4 tensor.  The first two dimensions will be each snapshot of multivariate data.  Your third dimension will be that snapshot overtime.  And your forth will be over time and different geographies.  Thinking about it this way is useful for capturing shared weights between time and geographies.  How you model your data matters.  And by ignoring the time or geospatial components of your data, you might lose some important information.\n",
    "\n",
    "2. You can get a performance boost, statistically speaking.  As this paper shows: https://arxiv.org/pdf/1811.06569.pdf you can get a decent accuracy boost by treating your neural network as a higher order tensor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensorflow Shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now that you know what a tensor is, the syntax of tensorflow will seem more obvious and straight forward.  Let's start by showing how to represent each of the tensors we've discussed thus far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-20 11:49:20.879168: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-06-20 11:49:20.879218: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# order 0 tensor\n",
    "import tensorflow as tf\n",
    "\n",
    "scalar = tf.constant([0])\n",
    "print(scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "You may think we've done nothing new here.  But actually we have!  For starters, tensorflow attaches types to anything passed into it.  And it does this _implicitly_.  You never have to name the types.  That by itself would be a feat of engineering prowess.  Let's see what I'm talking about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.int32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The `dtype` property tells us what kind of data is in our tensor.  \n",
    "\n",
    "Since there are mathematical consequences to what's in our tensor, in tensorflow we can only define one type per tensor.  Usually floats are the most flexible.  Unlike in numpy, in tensorflow you can only have one type in your tensor.  Let's see what happens when we try to put a string and an integer together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't convert Python sequence with mixed types to Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    172\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    278\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 279\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[1;32m    282\u001b[0m tensor_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    303\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't convert Python sequence with mixed types to Tensor."
     ]
    }
   ],
   "source": [
    "tf.constant([\"hello\", 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we get an error, because tensorflow doesn't know how to convert the types into one dtype across the entire tensor.  Now let's see what happens if we do integers and floats, does tensorflow handle that case any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1. , 1.5], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([1, 1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does!  That said, notice the dtype here, tensorflow implicit converts the integer into a float32.  This may seem like a simple thing but it's actually very nuanced!  Tensorflow can have one and exactly one dtype in the tensor and unfortunately, which tensorflow handles this type conversion for you and well, it doesn't always do the best job.  Especially interoperating with pandas and numpy.  For instance, if a neural network we've written expects a float64 and we give it a float32 tensor, the code will raise an error.  \n",
    "\n",
    "So the semi strict typing in tensorflow can be a pain.  But with pain, in this case comes gain.  The trade off is you get more performance, the ability to use GPUs and distribute your computation, so it's well worth it!  Plus keras is built in for fast prototyping of different network architectures.  In any event, I digress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now that we have seen an order 0 tensor and an order 1 tensor, by accident, let's define another order 1 tensor, called a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 4 7], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "vector = tf.constant([1, 4, 7])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are a few things to note here:\n",
    "\n",
    "1. a vector is a collection of scalars.\n",
    "2. a vector represents a mathematical object, not just an array.\n",
    "\n",
    "Because this is a mathematical object, we can do things like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[60]], dtype=int32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_one = tf.constant([[1, 4, 7]])\n",
    "vector_two = tf.constant([[2, 4, 6]])\n",
    "\n",
    "tf.matmul(vector_one, tf.transpose(vector_two))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note the differences between this and what we would do in numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector_one = np.array([1, 4, 7])\n",
    "vector_two = np.array([2, 4, 6])\n",
    "\n",
    "np.matmul(vector_one, vector_two.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in tensorflow we have to explicitly specify that our tensors have two axes, whereas in numpy, by tranposing, numpy is able to figure out that we ought to have two axes to work over, and the numpy matmul does the right thing.  If we tried to do the same thing in tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:CPU:0}} In[0] and In[1] ndims must be == 2: 1 [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m vector_one \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m7\u001b[39m])\n\u001b[1;32m      2\u001b[0m vector_two \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m6\u001b[39m])\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector_one\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector_two\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7214\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:CPU:0}} In[0] and In[1] ndims must be == 2: 1 [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "vector_one = tf.constant([1, 4, 7])\n",
    "vector_two = tf.constant([2, 4, 6])\n",
    "\n",
    "tf.matmul(vector_one, tf.transpose(vector_two))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we get an error, this is because tensorflow does less for you and expects you to be more explicit.  Some of this may be a maturity thing, but it's also a design choice to force users to be more explicit about the tensors they want and how you can interact with them.  To some, mostly programmers, who are thinking about axes as strict constructs rather than through the mathematician's conventional lens I'm sure that the latter is more palatable whereas for mathematicians the former is much more preferred.  These are all design choices, but it's important to be aware of the differences!\n",
    "\n",
    "But I digress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "If you've ever taken a linear algebra course the answer that's produced will seem surprising.  That's because technically numpy defaults to an array of scalars for a one dimensional array passed to the `tf.constant` method, rather than a vector.  The difference here is important.\n",
    "\n",
    "Because algebraic objects are defined in part by the algebraic operators attached to them, this detail matters.  Specifically, here the \"multiplication\" attached to our vectors is the inner product in this case.  If we want the outer product, which is what most folks who have taken linear algebra would expect, then we need to tell tensorflow that we are working with tensors or order 1 aka vectors and not a collection of scalars.\n",
    "\n",
    "We do that by using the `reshape` method a powerful tool that will allow us to represent tensors of any order we like.  But first let's start with the basics of turning a collection of scalars into an order 1 tensor, aka a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
       "array([[ 2,  4,  6],\n",
       "       [ 8, 16, 24],\n",
       "       [14, 28, 42]], dtype=int32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_one = tf.constant([1, 4, 7])\n",
    "vector_two = tf.constant([2, 4, 6])\n",
    "\n",
    "vector_one = tf.reshape(vector_one, [3, 1])\n",
    "vector_two = tf.reshape(vector_two, [1, 3])\n",
    "\n",
    "tf.matmul(vector_one, vector_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences in programming with tensorflow versus numpy should start to become more apparent at this point - in general tensorflow tensors don't have many methods defined on the objects themselves.  Instead most of the methods are defined on the tensorflow library object itself.  This modular design allows us to decouple our algorithms from our data structures, which means our data structures can be as small as possible, which allowing us more flexibility over our methods!  In general, I really like this shift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "By reshaping our vectors to the appropriate shapes, we were able to produce a matrix!  This is a general fact of linear algebra - you can get a matrix by applying a matrix multiplication (`matmul`), also known as the outer product, to two vectors.  This \"trick\" of taking two lower dimensional tensors to create a higher order one will actually work for _any_ tensor we like.  If we want to recover an order 3 tensor we simply need to multiply a matrix by a vector.  That's because the order is additive, by tensor product!  Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 5 10 15]\n",
      "  [ 1  2  3]\n",
      "  [ 3  6  9]]\n",
      "\n",
      " [[ 1  2  3]\n",
      "  [ 1  2  3]\n",
      "  [ 1  2  3]]\n",
      "\n",
      " [[ 1  2  3]\n",
      "  [ 2  4  6]\n",
      "  [ 1  2  3]]], shape=(3, 3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[5, 1, 3], \n",
    "                 [1, 1 , 1], \n",
    "                 [1, 2 , 1]])\n",
    "b = tf.constant([1, 2, 3])\n",
    "print(tf.tensordot(a, b, axes=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here `a` is a matrix, `b` is a tensor.  And by taking the tensor product of the two of them, we recover an order 3 tensor!  Notice we have to provide an axes or the `tensorproduct` method.  This is because a tensor product can be defined on any order.  We've already seen an order 1 tensor product, the inner product.  And we've seen an order 2 tensor product, the outer product.  In higher spaces, we generally refer to the product as simply the tensor product where the order comes from context.  However, please take care to be clear about the shapes of your tensors, otherwise you'll end up doing the _wrong multiplication_.  \n",
    "\n",
    "I'll leave as an exercise creating tensors of order 4, 5, and 6.  Happy multiplying!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensorflow Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now that we've seen how to create our tensors the next step is to be able to index into them.  The number of axes that you specify will determine how deep a slice you get back.  For instance, if you are working with an order 3 tensor and you specify one axes, you'll get back a matrix.  If you specify two, you'll get back a vector.  And if you specify all three you'll get back a scalar.  Let's see some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
       "array([[5, 1, 3],\n",
       "       [1, 1, 1],\n",
       "       [1, 2, 1]], dtype=int32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting back a matrix\n",
    "a = tf.constant([[ 5, 1 ,3], \n",
    "              [ 1, 1 ,1], \n",
    "              [ 1, 2 ,1]])\n",
    "b = tf.constant([1, 2, 3])\n",
    "order_3_tensor = tf.tensordot(a, b, axes=0)\n",
    "\n",
    "order_3_tensor[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The tensorflow syntax for slicing should be somewhat familar if you read the numpy section already, but maybe not.  In vanilla Python you can choose a number of elements by slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "listing = list(range(20))\n",
    "\n",
    "print(listing[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The `[:]` just gives back a \"slice\" that's equal to the whole array, because we didn't specify any start an end.  But if say we just wanted the last 3 elements we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "listing = list(range(20))\n",
    "\n",
    "print(listing[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Of course, we can fully specify the start and end as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "listing = list(range(20))\n",
    "\n",
    "print(listing[5:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Of course, that's only for a one dimensional array.  In tensorflow, we are dealing with _many_ dimensions, which is why the syntax looks different.  In the example above we had:\n",
    "\n",
    "`order_3_tensor[:, :, 0]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The first axis is selected completely so like with vanilla Python lists, we simply do `[:` for the whole thing.  Next we get the entire second axis so we have: `[:, :`.  And finally, we just want the first \"element\" along the third axis: `[:, :, 0]`.  At first, this syntax seems confusing to everyone.  But once it clicks, by understanding tensorflow arrays as tensors, then everything in the syntax becomes _obvious_.\n",
    "\n",
    "Let's move onto our second example.  This time we'll get back just a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([5, 1, 1], dtype=int32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting back a vector\n",
    "a = tf.constant([[ 5, 1 ,3], \n",
    "              [ 1, 1 ,1], \n",
    "              [ 1, 2 ,1]])\n",
    "b = tf.constant([1, 2, 3])\n",
    "order_3_tensor = tf.tensordot(a, b, axes=0)\n",
    "\n",
    "order_3_tensor[:, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As you can see, we've specified _two_ axes and therefore, we get back an order 1 tensor.  Now, let's move onto the final example and get back just a scalar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=5>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting back a scalar\n",
    "a = tf.constant([[ 5, 1 ,3], \n",
    "              [ 1, 1 ,1], \n",
    "              [ 1, 2 ,1]])\n",
    "b = tf.constant([1, 2, 3])\n",
    "order_3_tensor = tf.tensordot(a, b, axes=0)\n",
    "\n",
    "order_3_tensor[0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Something to notice, the first order 2 slice of our order 3 tensor is just the matrix `a`.  This is because the first element in our vector `b` is a 1.  I'll leave as an exercise, to see if you can write a slice that recovers the first row of matrix a.  Your answer should look like this:\n",
    "\n",
    "`[5, 1, 3]`\n",
    "\n",
    "Here is the starter code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "a = tf.constant([[ 5, 1 ,3], \n",
    "              [ 1, 1 ,1], \n",
    "              [ 1, 2 ,1]])\n",
    "b = tf.constant([1, 2, 3])\n",
    "order_3_tensor = tf.tensordot(a, b, axes=0)\n",
    "\n",
    "# put in your slices here\n",
    "#order_3_tensor[, , ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's talk about a few extra things that happen in tensorflow but not numpy.  We'll start with the tripple dot syntax.  For this we'll work with a tensor of order 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 3, 3, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([[ 5, 1 ,3], \n",
    "              [ 1, 1 ,1], \n",
    "              [ 1, 2 ,1]])\n",
    "b = tf.constant([\n",
    "    [1, 2, 3],\n",
    "    [ 1, 1 ,1], \n",
    "    [ 1, 2 ,1]           \n",
    "])\n",
    "\n",
    "order_4_tensor = tf.tensordot(a, b, axes=0)\n",
    "order_4_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's just select elements by the last dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3, 3, 1), dtype=int32, numpy=\n",
       "array([[[[5],\n",
       "         [5],\n",
       "         [5]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [1]],\n",
       "\n",
       "        [[3],\n",
       "         [3],\n",
       "         [3]]],\n",
       "\n",
       "\n",
       "       [[[1],\n",
       "         [1],\n",
       "         [1]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [1]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [1]]],\n",
       "\n",
       "\n",
       "       [[[1],\n",
       "         [1],\n",
       "         [1]],\n",
       "\n",
       "        [[2],\n",
       "         [2],\n",
       "         [2]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [1]]]], dtype=int32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_4_tensor[..., 0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are specifically getting the first row in the last dimension, while perserving the shape of the other three dimensions.  Now suppose we just wanted an order 3 tensor.  We can easily recover that with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3, 3), dtype=int32, numpy=\n",
       "array([[[5, 5, 5],\n",
       "        [1, 1, 1],\n",
       "        [3, 3, 3]],\n",
       "\n",
       "       [[1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1],\n",
       "        [2, 2, 2],\n",
       "        [1, 1, 1]]], dtype=int32)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_4_tensor[..., 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, the data in both tensors is the same, except the extra dimension from the 4 dimensional tensor is preserved when specifying a slice syntax instead of a single index.  We can also select from the front of our tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 3, 3), dtype=int32, numpy=\n",
       "array([[[[ 5, 10, 15],\n",
       "         [ 5,  5,  5],\n",
       "         [ 5, 10,  5]],\n",
       "\n",
       "        [[ 1,  2,  3],\n",
       "         [ 1,  1,  1],\n",
       "         [ 1,  2,  1]],\n",
       "\n",
       "        [[ 3,  6,  9],\n",
       "         [ 3,  3,  3],\n",
       "         [ 3,  6,  3]]]], dtype=int32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_4_tensor[0:1, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3, 3), dtype=int32, numpy=\n",
       "array([[[ 5, 10, 15],\n",
       "        [ 5,  5,  5],\n",
       "        [ 5, 10,  5]],\n",
       "\n",
       "       [[ 1,  2,  3],\n",
       "        [ 1,  1,  1],\n",
       "        [ 1,  2,  1]],\n",
       "\n",
       "       [[ 3,  6,  9],\n",
       "        [ 3,  3,  3],\n",
       "        [ 3,  6,  3]]], dtype=int32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_4_tensor[0, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionally, the tripple dots makes the last dimension of our tensor the 'index' dimension which convientently let's us slice into the tensor easily while preserving our overall dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: there is more than one way to recover the row in question.\n",
    "\n",
    "## Tensorflow Querying\n",
    "\n",
    "So far we've looked at the mathematical advantages of tensorflow over vanilla Python.  There was another claim I made, that tensorflow is blazing fast.  Let's verify that claim and go further than we did last time.  Now we can benchmark tensorflow against numpy to see how the two of them stack up as well!\n",
    "\n",
    "Let's see some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 µs ± 1.35 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit array = np.random.normal(0, 1, size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253 µs ± 87 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit array = tf.random.normal([10000], mean=0, stddev=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.72 ms ± 14.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def generate_array():\n",
    "    return [random.gauss(0, 1) for _ in range(10000)]\n",
    "\n",
    "%timeit generate_array()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As you can see, numpy and tensorflow are an _order_ of magnitude faster.  There are a lot of examples similar to this.  We'll look at just a few more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "array_one = np.random.normal(0, 1, size=10000)\n",
    "array_two = np.random.normal(0, 1, size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.67 µs ± 101 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit array_one + array_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_one = tf.random.normal([10000], mean=0, stddev=1)\n",
    "array_two = tf.random.normal([10000], mean=0, stddev=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.7 µs ± 214 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit tf.add(array_one, array_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.6 µs ± 271 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit array_one + array_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def add_array_elements(array_one, array_two):\n",
    "    return [array_one[index] + array_two[index]\n",
    "            for index in range(len(array_one))]\n",
    "\n",
    "array_one = generate_array()\n",
    "array_two = generate_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412 µs ± 1.55 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit add_array_elements(array_one, array_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This time, numpy is actually the clear winner.  With a much lower average and a much tighter bound, clearly numpy is better at adding arrays!  Granted, this is on a CPU.  If we were on a GPU it's likely that tensorflow would be faster.  But given how fast numpy is at adding vectors, I'm not sure it would be _much_ faster.  Notice there is more than one way to add tensors in tensorflow. \n",
    "\n",
    "You can also do multiplication just as easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "array_one = np.random.normal(0, 1, size=10000)\n",
    "array_two = np.random.normal(0, 1, size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.74 µs ± 42.5 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit array_one * array_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_one = tf.random.normal([10000], mean=0, stddev=1)\n",
    "array_two = tf.random.normal([10000], mean=0, stddev=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.1 µs ± 223 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit array_one * array_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.7 µs ± 269 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit tf.multiply(array_one, array_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def multiply_array_elements(array_one, array_two):\n",
    "    return [array_one[index] * array_two[index]\n",
    "            for index in range(len(array_one))]\n",
    "\n",
    "array_one = generate_array()\n",
    "array_two = generate_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415 µs ± 1.02 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit multiply_array_elements(array_one, array_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As you can see the orders of magnitude are similar for multiplication as they were for addition.  This is actually a _much_ bigger deal than may be obvious from these two examples.  All of linear algebra relies on these two operations.  That means linear regression, logistic regression, neural networks are all around 100 times faster implemented with numpy as compared to vanilla Python.  Of course, that is a blanket statement.  There are things you can do to make vanilla Python move faster.  And you can implement numpy poorly.  So this is a statement that needs to be taken with a grain of salt.  But still, numpy is faster for the things that matter to folks working in statistics and machine learning.  And that's just a fact.\n",
    "\n",
    "So even though numpy is the clear winner on these fundamental operations, some things to consider - this was done on CPU not GPU or TPU.  Second, this computation was not distributed.  For really big or long running work loads or really big data, tensorflow is going to do well.  But there is an important lesson here - you need to take care to optimize your code.  Unoptimized tensorflow code will actually be slower than numpy!  So it all comes down to the details.  \n",
    "\n",
    "It's also important to note, that writing really performant tensorflow code may be pretty _ugly_ (as far as programmer snooty-ness goes).  So make sure that you are aware of the trade off and making sure your team can support the code you are writing.  If they can't, careful documentation can help.  But building the skills of your team to be able to support your work is paramount!  If your code is performant but unmaintainable by others, who cares?  \n",
    "\n",
    "Of course, even if tensorflow ends up being slower because of the trade offs you have to make, it still comes with a ton of nice to have stuff by way of keras and other things.  So, no need to throw the baby out with the bath water even if you could get more performance out of numpy!  \n",
    "\n",
    "Tensorflow, even if not always the fastest, is still very fast.  Since tensorflow is _so fast_.  It can actually be used as a minimal in memory database.  Here we'll go over some of the basics for querying data in tensorflow.  Some of the syntax here will be confusing at first, but with time and practice it will become clear.\n",
    "\n",
    "Let's look at a simple example of selecting a specific section of elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.309"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = tf.random.normal([10000], mean=0, stddev=1)\n",
    "\n",
    "len(array[array > 0.5])/len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "I did a little stylizing here returning the percentage of the array over 0.5.  But basically this shows us the querying syntax.  This syntax is definitely _not_ obvious on first blush.  That said, once you get used to it, it's pretty powerful.  What's going on here is the following:\n",
    "\n",
    "the inner bit of syntax: `array > 0.5` is a boolean statement.  That is, implicitly every element of the array is checked for the condition, element of array greater than 0.5.  If the element meets the condition `True` is returned, otherwise False is returned.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then a boolean array is passed to the array as a slice:\n",
    "\n",
    "`array[boolean statement goes here]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Then indices where the index in question is `True` is returned.  Any indices that return `False` are ommited.  In this way, you can \"semantically slice\" your array.  To make this concrete, let's look at just the result of `array > 05`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10000,), dtype=bool, numpy=array([False, False, False, ..., False,  True,  True])>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As you can see, this is just an array of boolean values.  And if we counted up the number of times that resultant array has the value `True` it would equal the size of the semantically sliced array: `array[array > 0.5]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3051, shape=(), dtype=int32)\n",
      "3051\n"
     ]
    }
   ],
   "source": [
    "array = tf.random.normal([10000], mean=0, stddev=1)\n",
    "filtered_array = (array > 0.5)\n",
    "filtered_array = tf.cast(filtered_array, tf.int32)\n",
    "print(tf.reduce_sum(filtered_array))\n",
    "print(len(array[array > 0.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The way I counted the number of `True`'s may be confusing, so let's look at that: \n",
    "\n",
    "By casting the `True`'s and `False`'s as type `int` we turn the `True`'s into `1`'s and the `False`'s into `0`'s. \n",
    "\n",
    "There are some very useful primitives in this example:\n",
    "\n",
    "* tf.cast\n",
    "* tf.reduce_sum\n",
    "\n",
    "Because tensorflow tensors need to always have the same type across all elements, if you are combining two tensors with different types being able to cast is how you'll turn tensors of one type into another.  It's important to to take care when type casting!  Inherently type casting is a lossy process.  So if you cast from floats to integers or even from floats of one precision to floats of a different precision you will likely looks information.  Technically the same is true for integers but the chances of needing a 32 bit integer are usually quiet small.  \n",
    "\n",
    "As far as reduce_sum goes, there is a lot of utility here!  Many architectures make use of reduce_sum because it lets you sum across whatever axis you like.  \n",
    "\n",
    "Let's look at a few more examples of tf.reduce_sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum across the zeroth axis\n",
      "tf.Tensor(\n",
      "[[[ 7 14 21]\n",
      "  [ 7  7  7]\n",
      "  [ 7 14  7]]\n",
      "\n",
      " [[ 4  8 12]\n",
      "  [ 4  4  4]\n",
      "  [ 4  8  4]]\n",
      "\n",
      " [[ 5 10 15]\n",
      "  [ 5  5  5]\n",
      "  [ 5 10  5]]], shape=(3, 3, 3), dtype=int32)\n",
      "\n",
      "\n",
      "sum across the 1st axis\n",
      "tf.Tensor(\n",
      "[[[ 9 18 27]\n",
      "  [ 9  9  9]\n",
      "  [ 9 18  9]]\n",
      "\n",
      " [[ 3  6  9]\n",
      "  [ 3  3  3]\n",
      "  [ 3  6  3]]\n",
      "\n",
      " [[ 4  8 12]\n",
      "  [ 4  4  4]\n",
      "  [ 4  8  4]]], shape=(3, 3, 3), dtype=int32)\n",
      "\n",
      "\n",
      "sum across the 2nd axis\n",
      "tf.Tensor(\n",
      "[[[15 25 25]\n",
      "  [ 3  5  5]\n",
      "  [ 9 15 15]]\n",
      "\n",
      " [[ 3  5  5]\n",
      "  [ 3  5  5]\n",
      "  [ 3  5  5]]\n",
      "\n",
      " [[ 3  5  5]\n",
      "  [ 6 10 10]\n",
      "  [ 3  5  5]]], shape=(3, 3, 3), dtype=int32)\n",
      "\n",
      "\n",
      "sum across the 3rd axis\n",
      "tf.Tensor(\n",
      "[[[30 15 20]\n",
      "  [ 6  3  4]\n",
      "  [18  9 12]]\n",
      "\n",
      " [[ 6  3  4]\n",
      "  [ 6  3  4]\n",
      "  [ 6  3  4]]\n",
      "\n",
      " [[ 6  3  4]\n",
      "  [12  6  8]\n",
      "  [ 6  3  4]]], shape=(3, 3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[ 5, 1 ,3], \n",
    "              [ 1, 1 ,1], \n",
    "              [ 1, 2 ,1]])\n",
    "b = tf.constant([\n",
    "    [1, 2, 3],\n",
    "    [ 1, 1 ,1], \n",
    "    [ 1, 2 ,1]           \n",
    "])\n",
    "\n",
    "tensor = tf.tensordot(a, b, axes=0)\n",
    "\n",
    "print(\"sum across the zeroth axis\")\n",
    "print(tf.reduce_sum(tensor, axis=0))\n",
    "print()\n",
    "print()\n",
    "print(\"sum across the 1st axis\")\n",
    "print(tf.reduce_sum(tensor, axis=1))\n",
    "print()\n",
    "print()\n",
    "print(\"sum across the 2nd axis\")\n",
    "print(tf.reduce_sum(tensor, axis=2))\n",
    "print()\n",
    "print()\n",
    "print(\"sum across the 3rd axis\")\n",
    "print(tf.reduce_sum(tensor, axis=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see by default reduce sum reduces the dimension of the tensor by 1 as it sums across the axis of interest.  This can be very powerful if we have an 'index' axis to keep track of our tensors, like say a product or time axis.  We can actually reduce along as many axes as we like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum across the zeroth and 1st axis\n",
      "tf.Tensor(\n",
      "[[16 32 48]\n",
      " [16 16 16]\n",
      " [16 32 16]], shape=(3, 3), dtype=int32)\n",
      "\n",
      "\n",
      "sum across the 0th, 1st and 2nd axis\n",
      "tf.Tensor([48 80 80], shape=(3,), dtype=int32)\n",
      "\n",
      "\n",
      "sum across the 1st, 0th and 2nd axis\n",
      "tf.Tensor([48 80 80], shape=(3,), dtype=int32)\n",
      "\n",
      "\n",
      "sum across all axes\n",
      "tf.Tensor(208, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[ 5, 1 ,3], \n",
    "              [ 1, 1 ,1], \n",
    "              [ 1, 2 ,1]])\n",
    "b = tf.constant([\n",
    "    [1, 2, 3],\n",
    "    [ 1, 1 ,1], \n",
    "    [ 1, 2 ,1]           \n",
    "])\n",
    "\n",
    "tensor = tf.tensordot(a, b, axes=0)\n",
    "\n",
    "print(\"sum across the zeroth and 1st axis\")\n",
    "print(tf.reduce_sum(tensor, axis=[0, 1]))\n",
    "print()\n",
    "print()\n",
    "print(\"sum across the 0th, 1st and 2nd axis\")\n",
    "print(tf.reduce_sum(tensor, axis=[0, 1, 2]))\n",
    "print()\n",
    "print()\n",
    "print(\"sum across the 1st, 0th and 2nd axis\")\n",
    "print(tf.reduce_sum(tensor, axis=[1,0,2]))\n",
    "print()\n",
    "print()\n",
    "print(\"sum across all axes\")\n",
    "print(tf.reduce_sum(tensor, axis=[0,1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the sum is regardless of which axis we start with because addition is commutative for the integer numbers.\n",
    "\n",
    "We've show a simple example of how to query with tensorflow, but these examples can be as sophisticated as they are in any SQL dialect.  Next let's look at a complex querying statement - one with two statements:\n",
    "\n",
    "Here we will select all the elements between 0.5 and 0.7 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0632"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = tf.random.normal([10000], mean=0, stddev=1)\n",
    "\n",
    "result = array[\n",
    "    (array > 0.5) &\n",
    "    (array < 0.7)\n",
    "]\n",
    "len(result)/len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice here the use of the `&` symbol.  This stands for 'and' when dealing with boolean statements.  Specifically those where both numbers are represented as binary.  Therefore we refer to `&` as `binary and`.  \n",
    "\n",
    "The way this operator works is as follows:\n",
    "\n",
    "1 `&` 1 = 1\n",
    "\n",
    "1 `&` 0 = 0\n",
    "\n",
    "0 `&` 1 = 0\n",
    "\n",
    "If we have longer binary numbers, then we simply apply the `&` element wise across the \"string\" of binary numbers:\n",
    "\n",
    "10 `&` 01 = 00\n",
    "\n",
    "There are other \"binary\" operators:\n",
    "\n",
    "* OR = `|`\n",
    "* NOT = `~`\n",
    "\n",
    "With these three operators we can create very sophisticated queries into our numpy arrays.  For instance, let's get all the numbers between 0.5 and 0.7 or the numbers less than 0.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6086"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = tf.random.normal([10000], mean=0, stddev=1)\n",
    "\n",
    "result = array[\n",
    "    ((array > 0.5) &\n",
    "    (array < 0.7)) |\n",
    "    (array < 0.1)\n",
    "]\n",
    "len(result)/len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Notice, if we want contiguous elements we should use an `&` statement.  If we want elements from different sections of the array, we should use an `|` statement.  Now let's say we wanted everything that's not amongst those elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3914"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = tf.random.normal([10000], mean=0, stddev=1)\n",
    "\n",
    "result = array[\n",
    "    ~(((array > 0.5) &\n",
    "    (array < 0.7)) |\n",
    "    (array < 0.1))\n",
    "]\n",
    "len(result)/len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Often times the `~` operator may seem unnecessary, but if you're querying from a variable it can make life very easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3914"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = tf.random.normal([10000], mean=0, stddev=1)\n",
    "\n",
    "original_query = ((array > 0.5) & (array < 0.7)) | (array < 0.1)\n",
    "result = array[~original_query]\n",
    "len(result)/len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now that we've seen how to query into our array's, it's time to move onto the real heart of tensorflow - it's use in writing programs dealing with linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Algebra and Tensorflow\n",
    "\n",
    "In this section we will cover theoretical and practical notions regarding linear algebra.\n",
    "\n",
    "Matrix Basics:\n",
    "\n",
    "* Row Reduced Echelon Form\n",
    "* Linear Independence\n",
    "* Invertability\n",
    "* Dot Product\n",
    "* Matrix as a function\n",
    "* Determinants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Row Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Row reduction although of little practical importance is useful tool for understanding many concepts in linear algebra.  Through row reduction, linear transforms, invertability, bases, subspaces, and the importance of shape are revealed.  Additionally, there are many \"practical\" decompositions that are similar to row reduced echelon form but are harder to grasp mathematically.\n",
    "\n",
    "The reason for the lack of practical importance of row reduction stems from the lack of precision of floating point numbers.  It is very, very hard to computer \"zero\".  Often it is the case that your computation will be off by a negliable amount, therefore the row reduction goes on forever.  Therefore, the row reduction algorithm we will implement will be incomplete in that we won't get an exact solution, but rather one that is \"close enough\".\n",
    "\n",
    "We will accomplish this by rounding down, so that if a floating point number is ever less than 0.00001 it will be rounded down to zero.  This is a reasonable assumption for some systems, however is a completely unreasonable assumption for others.  Given our purpose we will allow this assumption to be made.  However, this code should not be used in real world situations, as it will possibly or even likely given incorrect results.\n",
    "\n",
    "First lets look at an example of how we might do row reduction by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Matrix:\n",
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
      "array([[1, 2],\n",
      "       [3, 4]], dtype=int32)>\n",
      "\n",
      "R2->R1*3-R2\n",
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
      "array([[1, 2],\n",
      "       [0, 2]], dtype=int32)>\n",
      "\n",
      "R2->R2*0.5\n",
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
      "array([[1, 2],\n",
      "       [0, 1]], dtype=int32)>\n",
      "\n",
      "R1->R1-R2*2\n",
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
      "array([[-1,  0],\n",
      "       [ 0,  1]], dtype=int32)>\n",
      "\n",
      "R1->R1*-1, end matrix\n",
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
      "array([[1, 0],\n",
      "       [0, 1]], dtype=int32)>\n"
     ]
    }
   ],
   "source": [
    "# fix me\n",
    "import tensorflow as tf\n",
    "\n",
    "matrix = tf.Variable([[1,2], [3,4]])\n",
    "print(\"Start Matrix:\")\n",
    "print(matrix)\n",
    "print()\n",
    "matrix[1, :].assign( matrix[0,:]*3 - matrix[1, :] )\n",
    "print(\"R2->R1*3-R2\")\n",
    "print(matrix)\n",
    "print()\n",
    "array = matrix[1, :]\n",
    "array = tf.cast(array, tf.float32)\n",
    "array *= 0.5\n",
    "array = tf.cast(array, tf.int32)\n",
    "matrix[1, :].assign(array)\n",
    "print(\"R2->R2*0.5\")\n",
    "print(matrix)\n",
    "print()\n",
    "matrix[0, :].assign(matrix[1, :]*2 - matrix[0, :] )\n",
    "print(\"R1->R1-R2*2\")\n",
    "print(matrix)\n",
    "print()\n",
    "array = matrix[0, :]\n",
    "array *= -1\n",
    "matrix[0, :].assign(array)\n",
    "print(\"R1->R1*-1, end matrix\")\n",
    "print(matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As you can see the goal is to go from any initial matrix to one in row reduced echelon form.  That is, from any matrix to the identity matrix.  In order to see the value of this technique, let's review one of the ways we can \"view\" a matrix - as a system of equations.  It is important to note, there are _many_ ways to think of matrices and this is but one.\n",
    "\n",
    "Additionally, note that there are a number of differences here from what we've seen so far:\n",
    "\n",
    "* tf.Variable instead of tf.constant\n",
    "* tensor.assign\n",
    "\n",
    "tf.Variable does exactly what it sounds like - it allows you to mutate the state of a matrix.  tf.constant requires that the elements of our matrix be immutable.  Of course, we can apply operations onto a constant matrix, as we've seen throughout.  But mutating the state of our matrix through assignment is not possible with a constant matrix.  \n",
    "\n",
    "That said, it isn't entirely straightforward to update the internal state.  For instance, if we tried to update the state of the above tensor with floats instead of integers we'd run into errors, which is far from ideal for all the future things we have planned regarding machine learning.  This is not to say that tensorflow is a poor choice for machine learning, but merely that care needs to be taken when mutating the state of tensorflow tensors.  \n",
    "\n",
    "For our example, assign seems to have done the trick.  But it won't always work for us.  Another tactic is to create new tensors from the old ones, so we never mutate the state of our previous tensor.  That said, this requires a lot of extra overhead and variable creation.  In general, it just makes the code uglier.  So it was avoided here and should be avoided whenever possible.  You want as few objects as possible floating around at any given time.\n",
    "\n",
    "Moving onto the mathematics behind what we just did:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we are treating our matrix as a system of equations then we have the following equivalence:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Is equivalent to:\n",
    "\n",
    "$$ 1*x + 2*y $$ \n",
    "\n",
    "$$ 3*x + 4*y $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Of course every equation ought to have an \"equality\", otherwise it's not really an equation.  To do this in matrix notation is straight forward, we just tack on a vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ \\left( \n",
    "\\begin{array}{cc|c} \n",
    "1 & 2 & 5 \\\\ \n",
    "3 & 4 & 8 \\end{array} \n",
    "\\right) $$\n",
    "\n",
    "Which is equivalent to:\n",
    "\n",
    "$$ 1*x + 2*y = 5 $$ \n",
    "\n",
    "$$ 3*x + 4*y = 8 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now that we have a right hand side, we can simply apply all the transformations to both the left hand side matrix and the right hand side vector, in effect solving the system of equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Matrix:\n",
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
      "array([[1, 2],\n",
      "       [3, 4]], dtype=int32)>\n",
      "\n",
      "R2->R1*3-R2\n",
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
      "array([[1, 2],\n",
      "       [0, 2]], dtype=int32)>\n",
      "\n",
      "R2->R2*0.5\n",
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
      "array([[1, 2],\n",
      "       [0, 1]], dtype=int32)>\n",
      "\n",
      "R1->R1-R2*2\n",
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
      "array([[-1,  0],\n",
      "       [ 0,  1]], dtype=int32)>\n",
      "\n",
      "R1->R1*-1, end matrix\n",
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
      "array([[1, 0],\n",
      "       [0, 1]], dtype=int32)>\n",
      "tf.Tensor([-1  3], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# fix me\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "matrix = tf.Variable([[1,2], [3,4]])\n",
    "vector = np.array([5, 8])\n",
    "print(\"Start Matrix:\")\n",
    "print(matrix)\n",
    "print()\n",
    "matrix[1, :].assign(matrix[0,:]*3 - matrix[1, :])\n",
    "vector[1] = (vector[0]*3 - vector[1])\n",
    "print(\"R2->R1*3-R2\")\n",
    "print(matrix)\n",
    "print()\n",
    "array = matrix[1, :]\n",
    "array = tf.cast(array, tf.float32)\n",
    "array *= 0.5\n",
    "array = tf.cast(array, tf.int32)\n",
    "matrix[1, :].assign(array)\n",
    "vector[1] *= 0.5\n",
    "print(\"R2->R2*0.5\")\n",
    "print(matrix)\n",
    "print()\n",
    "matrix[0, :].assign(matrix[1, :]*2 - matrix[0, :] )\n",
    "vector[0] = vector[1]*2 - vector[0]\n",
    "print(\"R1->R1-R2*2\")\n",
    "print(matrix)\n",
    "print()\n",
    "array = matrix[0, :]\n",
    "array *= -1\n",
    "matrix[0, :].assign(array)\n",
    "vector[0] = vector[0] * -1\n",
    "print(\"R1->R1*-1, end matrix\")\n",
    "vector = tf.constant(vector)\n",
    "print(matrix)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Therefore we have the unique solution to the system of equations:\n",
    "\n",
    "$$ x = -1, y = 3 $$\n",
    "\n",
    "In case it wasn't clear, we apply transforms from the left hand side to the ride hand side as well, thus giving us a unique solution, assuming both equations hold true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a technical aside, there are a bunch of things we need to go over:\n",
    "\n",
    "* Tensorflow was built for 2-D tensors and above, more or less\n",
    "* tf.assign does not work all the time on the elements of a 1-D tensor, so we cannot assign individual elements (called scalars)\n",
    "* tf.Variable does not play nice with 1-D tensors, after every you do an operations, the resultant 1-D tensor is turned into a tf.constant, so item assignment is no longer possible.  \n",
    "* It is NEVER possible to multiply a specific element in a 1-D tensor by a constant value.\n",
    "* For some reason, 1-D tensors in tensorflow have to be integers it looks like?  When I tried to convert them to floats and then back again, I couldn't do any further assignments.  And when I started off with floats, none of the assignments work.\n",
    "\n",
    "This is all to say, tensorflow is just simply not built for 1-D tensors, so we fall back to numpy for those operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we see how to do this by hand, let's pull out the component pieces and then write a general algorithm for doing row reduction:\n",
    "\n",
    "1. check to see if a row is in the correct form.\n",
    "2. find row pivot\n",
    "3. subtract one row or a multiple of one row from another\n",
    "4. multiply a row by a constant.\n",
    "5. carry out the same steps on the accompanying vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now for our algorithm:\n",
    "\n",
    "```\n",
    "index_of_currect_incorrect_row = 0\n",
    "while row not in correct form:\n",
    "    if row pivot is not 1:\n",
    "        multiply by inverse of row pivot\n",
    "    do necessary subtractions so all non-pivot\n",
    "    values are zero\n",
    "    carry out all calculations on accompanying vector\n",
    "    index_of_current_incorrect_row += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Notice that we used a term in our algorithm called the pivot.  This is a term for the element in a row that ought to be 1.  The column within the row that should be 1 is the same as the row number.  So the zeroth row should have a one in the zeroth column of the row.  The first row should have a 1 in the first column.  And so on and so forth.  In general, there ought to be 1s along the diagonal of the matrix (this is the element where the row index and column index are the same).\n",
    "\n",
    "Now then, let's implement our algorithm!  Recall that we will be rounding to make sure this process doesn't go on forever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
       " array([[ 1.,  0.],\n",
       "        [-0.,  1.]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([-2. ,  3.5], dtype=float32)>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def vector_assign(vector, index_to_change, new_value):\n",
    "    vector_size = vector.shape[0]\n",
    "    vector_values = []\n",
    "    for i in range(vector_size):\n",
    "        if i == index_to_change:\n",
    "            vector_values.append(new_value)\n",
    "        else:\n",
    "            vector_values.append(vector[i])\n",
    "    return tf.Variable(vector_values)\n",
    "\n",
    "def subtract_rows(matrix, row_index, pivot_index):\n",
    "    row_one = matrix[row_index, :]\n",
    "    row_two = matrix[pivot_index, :]\n",
    "    value = row_one[pivot_index]\n",
    "    return vector_assign(matrix, row_index, row_one - value*row_two)\n",
    "\n",
    "def multiply_row(row, value):\n",
    "    return value * tf.ones(len(row))\n",
    "    \n",
    "def invert_pivot(matrix, pivot_index):\n",
    "    row = matrix[pivot_index, :]\n",
    "    value = row[pivot_index]\n",
    "    return vector_assign(matrix, pivot_index, row * multiply_row(row, 1/value))\n",
    "    \n",
    "def get_identity(matrix):\n",
    "    return tf.eye(matrix.shape[1])\n",
    "\n",
    "def is_pivot_one(matrix, pivot_index):\n",
    "    return matrix[pivot_index, pivot_index] == 1\n",
    "\n",
    "def invert_vector_pivot(matrix, vector, pivot_index):\n",
    "    value = matrix[pivot_index, pivot_index]\n",
    "    return vector_assign(vector, pivot_index, vector[pivot_index]*1/value)\n",
    "\n",
    "def subtract_vector_elems(matrix, vector, row_index, pivot_index):\n",
    "    value = matrix[row_index, pivot_index]\n",
    "    new_value = vector[row_index] - value*vector[pivot_index]\n",
    "    return vector_assign(vector, row_index, new_value)\n",
    "\n",
    "def row_reduce(matrix, vector):\n",
    "    cur_index = 0\n",
    "    while (matrix != get_identity(matrix)).numpy().any():\n",
    "        if not is_pivot_one(matrix, cur_index):\n",
    "            vector = invert_vector_pivot(matrix, vector, cur_index)\n",
    "            matrix = invert_pivot(matrix, cur_index)\n",
    "            matrix = tf.Variable(matrix.numpy().round(5))\n",
    "            vector = tf.Variable(vector.numpy().round(5))\n",
    "        rows = list(range(matrix.shape[1]))\n",
    "        rows.remove(cur_index)\n",
    "        for row_index in rows:\n",
    "            vector = subtract_vector_elems(\n",
    "                matrix, vector, row_index, cur_index\n",
    "            )\n",
    "            matrix = subtract_rows(matrix, row_index, cur_index)\n",
    "            matrix = tf.Variable(matrix.numpy().round(5))\n",
    "            vector = tf.Variable(vector.numpy().round(5))\n",
    "        cur_index += 1\n",
    "    return matrix, vector\n",
    "\n",
    "matrix = tf.Variable([[1.0,2.0], [3.0,4.0]])\n",
    "vector = tf.Variable([5.0, 8.0])\n",
    "row_reduce(matrix, vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The first thing to notice is that we needed the ability to do assignment.  Unfortunately, this is more or less impossible with floating point type numbers in tensorflow, so I went ahead and implemented my own assignment operation.  This honestly felt a little riddiculous, but without the math just doesn't work out.  And since floating point is really the type needed to handle tensors in machine learning, we simply cannot live without it!  Of course, my assignment operator is incredibly slow, since it uses a looping mechanism, but it's good enough for the toy examples in this chapter.  If you need a real assignment operator, I'd suggest just converting to numpy and then converting back to tensorflow later.  That feels kind of gross, because we lose all the power of GPUs but hopefully you are doing assignment operations sparingly at best, at least in this fashion.  Most of the operations you should typically be doing will be implemented for you.  The tensorflow library *generally* has a lot of powerful tools in it for machine learning and linear algebra.  So please take advantage of them!\n",
    "\n",
    "\n",
    "Notice, we get the same result as before.  This is because we've been working with integers.  Things get very dicey as our numbers get smaller than one or greater than one.  However, that's why we round our matrix and vector so that they are _always_ within five significant digits after the decimal point.  Anything smaller than that is just rounded down to zero.  This means we can be \"precise enough\" for toy applications.  However in the real world, this assumption will lead us to incorrect results, which is why most scientific packages do not implement a row reduction scheme.  \n",
    "\n",
    "In any event, the power of row reduction should be obvious at this point:\n",
    "\n",
    "Assuming a solution exists, we can solve any system of equations, of attribitrary size and do so with great ease.  Let's consider this 100 x 100 system of equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7939.28719496727\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "matrix = tf.random.normal([10000], mean=100, stddev=1)\n",
    "matrix = tf.reshape(matrix, [100, 100])\n",
    "vector = tf.random.normal([100], mean=100, stddev=1)\n",
    "\n",
    "start = time.time()\n",
    "row_reduce(matrix, vector)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Our little \"toy\" solver finds a unique solution in a little over four tenths of a second!!!  If you tried to do this by hand, I guarantee you, it would take _much_ longer.  And the chance of being error free rapidly approaches zero.  Just for fun, let's see how long it takes to do 1000 x 1000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3740.573699235916\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "matrix = tf.random.normal([1000000], mean=100, stddev=1)\n",
    "matrix = tf.reshape(matrix, [1000,1000])\n",
    "vector = tf.random.normal([1000], mean=100, stddev=1)\n",
    "\n",
    "start = time.time()\n",
    "row_reduce(matrix, vector)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As said earlier, a lot of ideas flow from treating a matrix as a system of equations.  One notion already hinted at is that of \"solutions\" to the system.  Depending on the \"shape\" of the system, i.e. the number of rows and columns of the matrix, we know how many solutions are possible, apriori, or in other words, without actually solving the system!\n",
    "\n",
    "If the number of rows and columns are equal and the number of columns is linearly independent, then there is one and only one solution to the system.  In other words, you can be sure that one value for each of the parameters, uniquely \"solves\" the system.  Why might we care about this?\n",
    "\n",
    "Well as your number of equations grow, it becomes increasingly hard to know if a solution exists for your system.  Let us consider a real world example for clarity.  For this we will look at the Leontief Input-Output model.\n",
    "\n",
    "In this model of the economy there are several sectors, each sector produces some outputs, but also requires some inputs.  In the simplest possible economy each industry at least partially depends on every other sector.\n",
    "\n",
    "For our example we will consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-10 11:22:08.985434: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-07-10 11:22:08.985480: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Manufacturing</th>\n",
       "      <th>Technology</th>\n",
       "      <th>Real Estate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Manufacturing</th>\n",
       "      <td>9.814930</td>\n",
       "      <td>78.696404</td>\n",
       "      <td>58.786930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Technology</th>\n",
       "      <td>40.336216</td>\n",
       "      <td>1.696825</td>\n",
       "      <td>45.574345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Real Estate</th>\n",
       "      <td>53.444981</td>\n",
       "      <td>7.911551</td>\n",
       "      <td>57.174728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Manufacturing  Technology  Real Estate\n",
       "Manufacturing       9.814930   78.696404    58.786930\n",
       "Technology         40.336216    1.696825    45.574345\n",
       "Real Estate        53.444981    7.911551    57.174728"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "matrix = tf.random.normal([1000000], mean=100, stddev=1)\n",
    "\n",
    "supply_matrix = tf.random.uniform([3,3], 0, 100)\n",
    "demand_vector = tf.random.uniform([3], 0, 100)\n",
    "\n",
    "df = pd.DataFrame(supply_matrix)\n",
    "df.columns = [\"Manufacturing\", \"Technology\", \"Real Estate\"]\n",
    "df.index = [\"Manufacturing\", \"Technology\", \"Real Estate\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We are making use of a dataframe in this example, only to make the matrix as clear as possible, but we can simply see this as an \"annotated\" matrix for now.  The product of each industry is defined on the diagonal of the matrix, so the (Manufacturing, Manufacturing) element is the output for Manufacturing, the industry.  Everything else in the Manufacturing row are inputs to the Manufacturing industry.  In general we can think of this matrix as our \"Supply\" of goods into the economy.\n",
    "\n",
    "As for the vector that we've defined, this is our \"Demand\" for goods in the economy.  The reason for the equality is because the market \"clears\" and is therefore in equilibrium when Supply equals Demand.  So if we wish to find this equilibrium point, i.e. when utility is maximized, we need only solve this system of equations.\n",
    "\n",
    "So we simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
       " array([[ 1.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [-0., -0.,  1.]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([-4.88526, -3.68123,  6.27397], dtype=float32)>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_reduce(supply_matrix, demand_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As you can see, we were able to find a unique \"solution\", that is a unique value for the proportions that Manufacturing, Technology, and Real Estate out to produce in order to maximize utility!  Note, there is no special significance to the values of these values.  They are all less than one, merely by coincidence.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "I've already used this term, early in the text, but I failed to define it.  We will fix that here by giving a definition and providing some evidence of it's use.\n",
    "\n",
    "Linear independence is best seen through the second view or a matrix which we will discuss, the geometrix view.  Below is an example of a matrix as a 2-D representation of vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vector_one = tf.constant([5, 1])\n",
    "vector_two = tf.constant([4, -3])\n",
    "V = np.array([vector_one, vector_two])\n",
    "\n",
    "plt.quiver([0, 0], [0, 0], V[:,0], V[:,1], color=['r','b'], scale=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Two vectors are considered linearly dependent if:\n",
    "\n",
    "1. You can multiply by a constant to go from one vector to another.  This is called a dilation.\n",
    "\n",
    "Let's look at a picture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vector_one = tf.constant([5, 1])\n",
    "vector_two = vector_one * 2\n",
    "V = np.array([vector_one, vector_two])\n",
    "\n",
    "plt.quiver([0, 0], [0, 0], V[:,0], V[:,1], color=['r','b'], scale=21)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As you can see the two vectors \"rest\" on top of one another.  In other words, one vector completely captures the other vector.\n",
    "\n",
    "A set of vectors are considered linearly dependent as well if:\n",
    "\n",
    "2. You can add or subtract any two of them together times a scalar to produce another vector in the set.\n",
    "\n",
    "Let's look at another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vector_one = tf.constant([5.0, 1.0])\n",
    "vector_two = tf.constant([3.0, 7.0])\n",
    "vector_three = vector_one*2 - vector_two/2\n",
    "V = np.array([vector_one, vector_two, vector_three])\n",
    "\n",
    "plt.quiver([0, 0, 0], [0, 0, 0], V[:,0], V[:,1], color=['r','b', 'g'], scale=21)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here our green vector is a \"linear combination\" of our first two vectors, so by definition, the set of all three vectors is linearly dependent.  This all may seem arbitrary to this point, put let's see what happens if our system of equations is not linearly independent and we try to solve it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[1., 0., 0.],\n",
      "       [0., 1., 0.],\n",
      "       [0., 0., 1.]], dtype=float32)>, <tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([-2.51863,  0.68794,  0.77175], dtype=float32)>)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} slice index 3 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(row_reduce(matrix_two, vector))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mtype\u001b[39m(matrix), \u001b[38;5;28mtype\u001b[39m(matrix_two)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrow_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn [3], line 46\u001b[0m, in \u001b[0;36mrow_reduce\u001b[0;34m(matrix, vector)\u001b[0m\n\u001b[1;32m     44\u001b[0m cur_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (matrix \u001b[38;5;241m!=\u001b[39m get_identity(matrix))\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_pivot_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_index\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     47\u001b[0m         vector \u001b[38;5;241m=\u001b[39m invert_vector_pivot(matrix, vector, cur_index)\n\u001b[1;32m     48\u001b[0m         matrix \u001b[38;5;241m=\u001b[39m invert_pivot(matrix, cur_index)\n",
      "Cell \u001b[0;32mIn [3], line 32\u001b[0m, in \u001b[0;36mis_pivot_one\u001b[0;34m(matrix, pivot_index)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_pivot_one\u001b[39m(matrix, pivot_index):\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmatrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpivot_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpivot_index\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:1348\u001b[0m, in \u001b[0;36m_SliceHelperVar\u001b[0;34m(var, slice_spec)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_SliceHelperVar\u001b[39m(var, slice_spec):\n\u001b[1;32m   1308\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a slice helper object given a variable.\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \n\u001b[1;32m   1310\u001b[0m \u001b[38;5;124;03m  This allows creating a sub-tensor from part of the current contents\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1345\u001b[0m \n\u001b[1;32m   1346\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1348\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_slice_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7214\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} slice index 3 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vector_one = tf.random.uniform([3], 0, 100)\n",
    "vector_two = tf.random.uniform([3], 0, 100)\n",
    "vector_three = tf.identity(vector_one)\n",
    "matrix = tf.stack([vector_one, vector_two, vector_three], axis=1)\n",
    "matrix_two = tf.random.uniform([3, 3], 0, 100)\n",
    "vector = tf.random.uniform([3], 0, 100)\n",
    "\n",
    "print(matrix.shape == matrix_two.shape)\n",
    "print(row_reduce(matrix_two, vector))\n",
    "type(matrix), type(matrix_two)\n",
    "print(row_reduce(matrix, vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We get an error when we try to do the row reduction!  This is because we fail to row reduce our matrix.  Let's verify this by looking at another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Matrix:\n",
      "[[ 1  2  3]\n",
      " [ 3  4  5]\n",
      " [ 9 14 19]]\n",
      "\n",
      "Vector Start:\n",
      "[5 8 9]\n",
      "\n",
      "R2->R1*3-R2\n",
      "[[ 1  2  3]\n",
      " [ 0  2  4]\n",
      " [ 9 14 19]]\n",
      "\n",
      "[5 7 9]\n",
      "\n",
      "R2->R1*9-R2\n",
      "[[1 2 3]\n",
      " [0 2 4]\n",
      " [0 4 8]]\n",
      "\n",
      "[ 5  7 36]\n",
      "\n",
      "R2->R2*0.5\n",
      "[[1 2 3]\n",
      " [0 1 2]\n",
      " [0 4 8]]\n",
      "\n",
      "[ 5  3 36]\n",
      "\n",
      "R2->R1*9-R2\n",
      "[[1 2 3]\n",
      " [0 1 2]\n",
      " [0 0 0]]\n",
      "\n",
      "[  5   3 -24]\n",
      "\n",
      "R1->R1-R2*2\n",
      "[[-1  0  1]\n",
      " [ 0  1  2]\n",
      " [ 0  0  0]]\n",
      "\n",
      "[  1   3 -24]\n",
      "\n",
      "R1->R1*-1, end matrix\n",
      "[[ 1  0 -1]\n",
      " [ 0  1  2]\n",
      " [ 0  0  0]]\n",
      "\n",
      "[ -1   3 -24]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector_one = np.array([1, 2, 3])\n",
    "vector_two = np.array([3, 4, 5])\n",
    "vector_three = 3 * vector_one + 2 * vector_two\n",
    "matrix = np.array([vector_one, vector_two, vector_three])\n",
    "vector = np.array([5, 8, 9])\n",
    "print(\"Start Matrix:\")\n",
    "print(matrix)\n",
    "print()\n",
    "print(\"Vector Start:\")\n",
    "print(vector)\n",
    "print()\n",
    "matrix[1, :] = matrix[0,:]*3 - matrix[1, :]\n",
    "vector[1] = vector[0]*3 - vector[1]\n",
    "print(\"R2->R1*3-R2\")\n",
    "print(matrix)\n",
    "print()\n",
    "print(vector)\n",
    "print()\n",
    "matrix[2, :] = matrix[0,:]*9 - matrix[2, :]\n",
    "vector[2] = vector[0]*9 - vector[2]\n",
    "print(\"R2->R1*9-R2\")\n",
    "print(matrix)\n",
    "print()\n",
    "print(vector)\n",
    "print()\n",
    "matrix[1, :] = matrix[1, :] * (0.5 * np.ones(3))\n",
    "vector[1] *= 0.5\n",
    "print(\"R2->R2*0.5\")\n",
    "print(matrix)\n",
    "print()\n",
    "print(vector)\n",
    "print()\n",
    "matrix[2, :] = matrix[1,:]*4 - matrix[2, :]\n",
    "vector[2] = vector[1]*4 - vector[2]\n",
    "print(\"R2->R1*9-R2\")\n",
    "print(matrix)\n",
    "print()\n",
    "print(vector)\n",
    "print()\n",
    "\n",
    "matrix[0, :] = matrix[1, :]*2 - matrix[0, :]\n",
    "vector[0] = vector[1]*2 - vector[0]\n",
    "print(\"R1->R1-R2*2\")\n",
    "print(matrix)\n",
    "print()\n",
    "print(vector)\n",
    "print()\n",
    "matrix[0, :] = matrix[0, :] * (-1 * np.ones(3))\n",
    "vector[0] *= -1\n",
    "print(\"R1->R1*-1, end matrix\")\n",
    "print(matrix)\n",
    "print()\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As you can see, this is the farest we can take this.  We can't row reduce the matrix any further because we've elminated the pivot the final row.  This is because vector three is a linear combination of vector one and vector two.  Therefore the columns aren't linearly independent.  \n",
    "\n",
    "What we've seen so far generalizes to _any_ system of equations.  Additionally, any system of equations such that the number of rows and number of columns are unequal will not be row reducible, unless there are \"extra\" rows or columns that are linear combinations of a set of a \"square\" subset which are linearly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To make this concrete consider this linear independent matrix:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Now if we add the following vector:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    " 4 \\\\\n",
    " 6 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We get:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 4\\\\\n",
    "3 & 4 & 6\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let's see what happens if we try to row reduce this matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Matrix:\n",
      "[[1 2 4]\n",
      " [3 4 6]]\n",
      "\n",
      "R2->R1*3-R2\n",
      "[[1 2 4]\n",
      " [0 2 6]]\n",
      "\n",
      "R2->R2*0.5\n",
      "[[1 2 4]\n",
      " [0 1 3]]\n",
      "\n",
      "R1->R1-R2*2\n",
      "[[-1  0  2]\n",
      " [ 0  1  3]]\n",
      "\n",
      "R1->R1*-1, end matrix\n",
      "[[ 1  0 -2]\n",
      " [ 0  1  3]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 4])\n",
    "b = np.array([3, 4, 6])\n",
    "\n",
    "matrix = np.array([a, b])\n",
    "\n",
    "print(\"Start Matrix:\")\n",
    "print(matrix)\n",
    "print()\n",
    "matrix[1, :] = matrix[0,:]*3 - matrix[1, :]\n",
    "vector[1] = vector[0]*3 - vector[1]\n",
    "print(\"R2->R1*3-R2\")\n",
    "print(matrix)\n",
    "print()\n",
    "matrix[1, :] = matrix[1, :] * (0.5 * np.ones(3))\n",
    "vector[1] *= 0.5\n",
    "print(\"R2->R2*0.5\")\n",
    "print(matrix)\n",
    "print()\n",
    "matrix[0, :] = matrix[1, :]*2 - matrix[0, :]\n",
    "vector[0] = vector[1]*2 - vector[0]\n",
    "print(\"R1->R1-R2*2\")\n",
    "print(matrix)\n",
    "print()\n",
    "matrix[0, :] = matrix[0, :] * (-1 * np.ones(3))\n",
    "vector[0] *= -1\n",
    "print(\"R1->R1*-1, end matrix\")\n",
    "print(matrix)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As you can see there is a 2 x 2 identity matrix embedded in this 2 x 3 matrix.  We can't row reduce any further than this, but clearly the third vector is surpurflous and doesn't contribute to us finding a unique solution.  In other words, any system of equations like this will have _infinitely_ many equations.  A unique solution for x and y.  But an infinite number of solutions for the z variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Invertability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "It turns out that the uniqueness of solutions of systems of equations has a consequence, specifically that if a matrix has a unique solution and it's individual component vectors are linearly independent, then the matrix is invertable.  Consequentially, invertability is not a quality for all matrices and therefore you cannot always take an inverse.  \n",
    "\n",
    "In order to find the inverse of the matrix we simply apply all the same steps to an identity matrix that we apply to the original matrix as we row reduce it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inverse_subtract_rows(matrix, inverse, row_index, pivot_index):\n",
    "    row_one = matrix[row_index, :]\n",
    "    row_two = matrix[pivot_index, :]\n",
    "    inverse_row_one = inverse[row_index, :]\n",
    "    inverse_row_two = inverse[pivot_index, :]\n",
    "    value = row_one[pivot_index]\n",
    "    matrix[row_index, :] = row_one - value*row_two\n",
    "    inverse[row_index, :] = inverse_row_one - value*inverse_row_two\n",
    "    return matrix, inverse\n",
    "    \n",
    "def inverse_invert_pivot(matrix, inverse, pivot_index):\n",
    "    row = matrix[pivot_index, :]\n",
    "    inverse_row = inverse[pivot_index, :]\n",
    "    value = row[pivot_index]\n",
    "    matrix[pivot_index, :] = row * multiply_row(row, 1/value)\n",
    "    inverse[pivot_index, :] = inverse_row * multiply_row(inverse_row, 1/value)\n",
    "    return matrix, inverse\n",
    "    \n",
    "def find_inverse(matrix):\n",
    "    cur_index = 0\n",
    "    inverse = np.identity(matrix.shape[1])\n",
    "    while (matrix != get_identity(matrix)).any():\n",
    "        if not is_pivot_one(matrix, cur_index):\n",
    "            matrix, inverse = inverse_invert_pivot(matrix, inverse, cur_index)\n",
    "            matrix = matrix.round(5)\n",
    "        rows = list(range(matrix.shape[1]))\n",
    "        rows.remove(cur_index)\n",
    "        for row_index in rows:\n",
    "            matrix, inverse = inverse_subtract_rows(\n",
    "                matrix, inverse, row_index, cur_index\n",
    "            )\n",
    "            matrix = matrix.round(5)\n",
    "        cur_index += 1\n",
    "    return inverse\n",
    "\n",
    "matrix = np.array([[1, 2], [3, 4]])\n",
    "inverse = find_inverse(matrix)\n",
    "\n",
    "matrix = np.array([[1, 2], [3, 4]])\n",
    "matrix @ inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As you can see we were able to find the inverse by modifying the the row reduction code.  When you do a matrix mulitplication of the original matrix and it's inverse you get back the identity matrix, because this is the matrix equivalent of a \"1\".\n",
    "\n",
    "Being able to invert a matrix is incredibly useful as we'll come to see for linear regression and other modeling techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix Multiplication\n",
    "\n",
    "Here we'll take a deeper look at matrix multiplication, specificially we'll consider the dot product.  The matrix multiplication we just did:\n",
    "\n",
    "`matrix @ inverse` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For each row in the left hand side matrix, carry out the dot product with the column on the right hand side.  Therefore this will only work if the number of columns on the left hand side equals the number of rows on the right hand side.  \n",
    "\n",
    "Let's look at a quick example with two matrices:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\\n",
    "\\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "\\end{pmatrix}\n",
    "%\n",
    "\\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "7 & 10 \\\\\n",
    "15 & 22 \\\\\n",
    "\\end{pmatrix}\n",
    "\\\n",
    "$$\n",
    "\n",
    "to get the top left cell of the resulting matrix, we do:\n",
    "\n",
    "1(1) + 2(3) = 7\n",
    "\n",
    "to get the top right cell of the resulting matrix, we do:\n",
    "\n",
    "1(2) + 2(4) = 10\n",
    "\n",
    "to get the bottom left cell of the resulting matrix, we do:\n",
    "\n",
    "3(1) + 4(3) = 15\n",
    "\n",
    "to get the bottom right cell of the resulting matrix, we do:\n",
    "\n",
    "3(2) + 4(4) = 22\n",
    "\n",
    "In general the algorithm for the ith row and jth column is:\n",
    "\n",
    "$$ \\sum_{k=1}^{n}a_{ik}b_{kj} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note, we can also define the dot product for now vectors, simply by taking the transpose of the right hand side vector:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "3 \\\\\n",
    "\\end{pmatrix}\n",
    "%\n",
    "\\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "7\n",
    "\\end{pmatrix}\n",
    "\\\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Notice, in this case the length of the two vectors must be equal, otherwise the dot product is not defined.\n",
    "\n",
    "Because everything in linear algebra has both a geometric and algebriac interpretation we'll consider both of these when interpretting this new operator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us consider the geometrix interpretation first:\n",
    "\n",
    "$$ X \\cdot Y = |X||Y|\\cos\\theta $$\n",
    "\n",
    "Here $\\theta$ is the angle between the two vectors.  \n",
    "\n",
    "This formulation allows us to make the following statement:\n",
    "\n",
    "If: \n",
    "\n",
    "$$ X \\cdot Y = 0 $$\n",
    "\n",
    "Then\n",
    "\n",
    "$X$ is perpendicular to $Y$!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As for the algebriac interpretation:\n",
    "\n",
    "In general, the dot product can be defined as the length of the projection of $X$ onto the unit vector in the direction of $Y$.  So we get a sense of the \"magnitude\" of X and Y by taking their dot product.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix as a Function\n",
    "\n",
    "There won't be any real new concepts in this section, but we are going take a moment to notice some facts about matrices which will be valuable for how we think about them.  We treat a matrix in one of two ways:\n",
    "\n",
    "1. As holding data\n",
    "2. As \"storing\" transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The first representation of a matrix will probably be more familiar to most of you, if you've ever worked with excel, csvs or a database.  All of that data is stored as matrices.  \n",
    "\n",
    "We've sort of been seeing this second representation throughout our discussion of linear algebra thus far.  Matrix multiplication is the major vehicle for applying our matrix.  If we consider the following example:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\\n",
    "\\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "\\end{pmatrix}\n",
    "%\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "3 & 3 \\\\\n",
    "7 & 4 \\\\\n",
    "\\end{pmatrix}\n",
    "\\\n",
    "$$\n",
    "\n",
    "We can treat our first matrix:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "As our data matrix.  And our second matrix:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "along with matrix multiplication as our function.  It transforms our input data and produces our resultant matrix:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "3 & 3 \\\\\n",
    "7 & 4 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "So we can sort of think of our second matrix as a function.  Where the matrix's values are the parameters to the function and the matrix plus the multiplication \"is\" the function.\n",
    "\n",
    "A consequence of this way of thinking about matrices is that we can \"combine\" functions by applying matrix multiplication to individual matrices in order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So for instance, say we had:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & -1 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "If we do:\n",
    "\n",
    "$$\n",
    "\\\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & -1 \\\\\n",
    "\\end{pmatrix}\n",
    "%\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "-1 & -1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\\n",
    "$$\n",
    "\n",
    "Then \n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "-1 & -1 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Is the same as first taking _any_ compatiable matrix and multiplying it first by:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & -1 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "And then by\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Let's verify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "first_matrix = np.array([[1, 0], [0, -1]])\n",
    "second_matrix = np.array([[1, 0], [1, 1]])\n",
    "third_matrix =  second_matrix @ first_matrix \n",
    "random_matrix = matrix_two = np.random.rand(2, 2) * 100\n",
    "\n",
    "first_intermediate_result = random_matrix @ second_matrix\n",
    "second_intermediate_result = first_intermediate_result @ first_matrix \n",
    "(second_intermediate_result == random_matrix @ third_matrix).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Notice, the order in which the first and second matrix are combined matter!  So for instance, if we did this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_matrix = np.array([[1, 0], [0, -1]])\n",
    "second_matrix = np.array([[1, 0], [1, 1]])\n",
    "third_matrix =  first_matrix @ second_matrix \n",
    "random_matrix = matrix_two = np.random.rand(2, 2) * 100\n",
    "\n",
    "first_intermediate_result = random_matrix @ second_matrix\n",
    "second_intermediate_result = first_intermediate_result @ first_matrix \n",
    "(second_intermediate_result == random_matrix @ third_matrix).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "All we did was switch the order of the matrix multiplication of the third_matrix.  This is because matrix multiplication is not commutative.  This means the order of the matrix multiplication will matter and in general, if we have two matrices, A and B then:\n",
    "\n",
    "$$\n",
    "AB \\neq BA\n",
    "$$\n",
    "\n",
    "We've actually already used this fact.  The above was how we were able to arrive at the inverse of A given a row reduction scheme for A.  We applied each transform, storing the individual transforms in a matrix.  This necesary implies we can move in the other direction as well:\n",
    "\n",
    "Going from one matrix to many \"component\" transformations.  The process of deducing these set of component transformations is called matrix decomposition.\n",
    "\n",
    "## Determinants\n",
    "\n",
    "The determinant of a matrix is a very helpful tool for understanding how a transformation matrix \"acts\" on a data matrix.\n",
    "\n",
    "The determinant conveys two pieces of information:\n",
    "\n",
    "1. the magnitude of the transform\n",
    "2. whether the transform preserves or reverses orientation\n",
    "\n",
    "This allows us to tell ahead of time how our matrix will transform other matrices and what we can expect as a result.  Computing determinants is easy with numpy, let's look at the detminerant of our \"third_matrix\" from the last example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(third_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now let's compare this with our other two matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 1.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(first_matrix), np.linalg.det(second_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As you can see, we get the magnitude from the first matrix and second matrix and we get the orientation change from the first matrix.  As should be clear, the determinant is a \"lossy\" measure, because we represent a matrix as a scalar.  However, there is still power in looking at representations like this.  We can assess the individual effects of component transformations.  And then see how their combined effects will augment and orient other matrices."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
