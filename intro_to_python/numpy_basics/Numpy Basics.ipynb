{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numpy Basics\n",
    "\n",
    "Probably the biggest shift when getting started with data science is the syntax of numpy and pandas because it differs so much from other programming paradigms.  In this section we will walk through some numpy basics:\n",
    "\n",
    "* why numpy?\n",
    "* introduction to tensors\n",
    "* numpy shapes\n",
    "* numpy slicing\n",
    "* numpy querying\n",
    "* linear algebra in numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Numpy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Technically, anything you can do in numpy you can do in plain old python.  And arguably the syntax will be easier to understand, that is, unless you use numpy as it is intended.  Numpy _can_ be used for all the basic stuff that you'll find most of the examples for on the internet.  But it's really _intended_ to be used for a new paradigm of programming.  One that's caught on in the statistical and deep learning communities (which have at least some overlap).  \n",
    "\n",
    "Numpy's api and computation is optimized for the manipulation of algebraic structures.  You can use it to do most of the computation that you can do with vanilla Python, or any other programming lanugage.  But you probably shouldn't.  We can think of the numpy api as sort of a directed language.  It's not quiet that, because numpy is mostly \"about\" syntax change.  You are thinking about the world from a difference lense.  But I digress.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "* incredible speed - numpy is _much_ faster than vanilla python (it can even outperform Java sometimes)\n",
    "* a beautiful and well organized api\n",
    "* tons of utility functions\n",
    "* amazing documentation\n",
    "* it's completely free (whaaaaat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To really understand numpy and the power it brings, we need to understand tensors.  Because without them, numpy honestly doesn't make much sense, at least at first.  And even once you start to get used to the syntax, without the mental model of a tensor, you'll completely miss the point of using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Tensors are some of the most powerful objects around.  In fact, this book is basically just a \"how do I use tensors\" most of the time.  The chapter on linear regression?  That's just about tensors.  The chapter on classification?  More applications of tensors.  Much of machine learning is built on tensors.  Specifically, on matrices.  Because I define the matrix in another chapter, I won't go into a ton of detail about what they are, or how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* scalars\n",
    "* vectors\n",
    "* matrices\n",
    "* order-3 tensors and higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A scalar is an order zero tensor - because it's just a single number, like say the number `5`.  A vector is a one dimensional collection of numbers representing data or an equation, like: \n",
    "\n",
    "$$ \\begin{pmatrix}\n",
    "1  \\\\\n",
    "4  \\\\\n",
    "7 \n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A matrix is a two dimensional collect of numbers representing a system of equations, like:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An order three tensor looks like a data cube.  There is no easy way to show such a cube in latex, so you'll have to imagine this to some extent:\n",
    "\n",
    "$$ A_{1} = \n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$ A_{2} = \n",
    "\\begin{pmatrix}\n",
    "3 & 2 & 3 \\\\\n",
    "7 & 6 & 6 \\\\\n",
    "7 & 2 & 9\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$ A_{3} = \n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "7 & 6 & 4 \\\\\n",
    "6 & 2 & 19\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now imagine $A_{1}, A_{2}, A_{3}$ as one object.  This is an order three tensor.  It has three axes - $(i,j,k)$ and you can specify elements across these three axes.  So $(0,0,0) = 1$, $(1,0,0) = 4$, and $(3,0,3) = 6$.  Here the i is the row index, j is the column index and k is the matrix index.  You can also do this for order 4 and up to n, where n is any finite natural number you like.  Why might you want to ever do this in practice?  It turns out there are actually a ton of good reasons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here are just two of them:\n",
    "\n",
    "1. Let's say you want to model multivariate timeseries geospatial data.  This is naturally an order 4 tensor.  The first two dimensions will be each snapshot of multivariate data.  Your third dimension will be that snapshot overtime.  And your forth will be over time and different geographies.  Thinking about it this way is useful for capturing shared weights between time and geographies.  How you model your data matters.  And by ignoring the time or geospatial components of your data, you might lose some important information.\n",
    "\n",
    "2. You can get a performance boost, statistically speaking.  As this paper shows: https://arxiv.org/pdf/1811.06569.pdf you can get a decent accuracy boost by treating your neural network as a higher order tensor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Numpy Shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now that you know what a tensor is, the syntax of numpy will seem obvious and straight forward.  Let's start by showing how to represent each of the tensors we've discussed thus far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# order 0 tensor\n",
    "import numpy as np\n",
    "\n",
    "scalar = np.array([1])\n",
    "print(scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "You may think we've done nothing new here.  But actually we have!  For starters, numpy attaches types to anything passed into it.  And it does this _implicitly_.  You never have to name the types.  That by itself would be a feat of engineering prowess.  Let's see what I'm talking about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The `dtype` property tells us what kind of data is in our tensor.  Since there are mathematical consequences to what's in our tensor, it's best to define one type per tensor.  Usually floats are the most flexible.  Of course, you can define a tensor with multiple types.  But for any serious mathematical computation, this is discouraged.  However, there are lots of programming instances when defining multiple types in a data structure is useful and important, which is why this paradigm is supported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hello', '1'], dtype='<U5')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([\"hello\", 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now that we have seen an order 0 tensor and an order 1 tensor, by accident, let's define another order 1 tensor, called a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 7]\n"
     ]
    }
   ],
   "source": [
    "vector = np.array([1, 4, 7])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are a few things to note here:\n",
    "\n",
    "1. a vector is a collection of scalars.\n",
    "2. a vector represents a mathematical object, not just an array.\n",
    "\n",
    "Because this is a mathematical object, we can do things like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_one = np.array([1, 4, 7])\n",
    "vector_two = np.array([2, 4, 6])\n",
    "\n",
    "np.matmul(vector_one, vector_two.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "If you've ever taken a linear algebra course the answer that's produced will seem surprising.  That's because technically numpy defaults to an array of scalars for a one dimensional array passed to the `np.array` method, rather than a vector.  The difference here is important.\n",
    "\n",
    "Because algebraic objects are defined in part by the algebraic operators attached to them, this detail matters.  Specifically, here the \"multiplication\" attached to our vectors is the inner product in this case.  If we want the outer product, which is what most folks who have taken linear algebra would expect, then we need to tell numpy that we are working with tensors or order 1 aka vectors and not a collection of scalars.\n",
    "\n",
    "We do that by using the `reshape` method a powerful tool that will allow us to represent tensors of any order we like.  But first let's start with the basics of turning a collection of scalars into an order 1 tensor, aka a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector one:\n",
      "[[1]\n",
      " [4]\n",
      " [7]]\n",
      "vector two: [[2 4 6]]\n",
      "result:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2,  4,  6],\n",
       "       [ 8, 16, 24],\n",
       "       [14, 28, 42]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_one = np.array([1, 4, 7])\n",
    "vector_two = np.array([2, 4, 6])\n",
    "\n",
    "vector_one = vector_one.reshape(3, 1)\n",
    "vector_two = vector_two.reshape(1, 3)\n",
    "print(\"vector one:\")\n",
    "print(vector_one)\n",
    "print(\"vector two:\", vector_two)\n",
    "print(\"result:\")\n",
    "np.matmul(vector_one, vector_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "By reshaping our vectors to the appropriate shapes, we were able to produce a matrix!  This is a general fact of linear algebra - you can get a matrix by applying a matrix multiplication (`matmul`), also known as the outer product, to two vectors.  This \"trick\" of taking two lower dimensional tensors to create a higher order one will actually work for _any_ tensor we like.  If we want to recover an order 3 tensor we simply need to multiply a matrix by a vector.  That's because the order is additive, by tensor product!  Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 5 10 15]\n",
      "  [ 1  2  3]\n",
      "  [ 3  6  9]]\n",
      "\n",
      " [[ 1  2  3]\n",
      "  [ 1  2  3]\n",
      "  [ 1  2  3]]\n",
      "\n",
      " [[ 1  2  3]\n",
      "  [ 2  4  6]\n",
      "  [ 1  2  3]]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[ 5, 1 ,3], \n",
    "              [ 1, 1 ,1], \n",
    "              [ 1, 2 ,1]])\n",
    "b = np.array([1, 2, 3])\n",
    "print(np.tensordot(a, b, axes=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here `a` is a matrix, `b` is a tensor.  And by taking the tensor product of the two of them, we recover an order 3 tensor!  Notice we have to provide an axes or the `tensorproduct` method.  This is because a tensor product can be defined on any order.  We've already seen an order 1 tensor product, the inner product.  And we've seen an order 2 tensor product, the outer product.  In higher spaces, we generally refer to the product as simply the tensor product where the order comes from context.  However, please take care to be clear about the shapes of your tensors, otherwise you'll end up doing the _wrong multiplication_.  \n",
    "\n",
    "I'll leave as an exercise creating tensors of order 4, 5, and 6.  Happy multiplying!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Numpy Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now that we've seen how to create our tensors the next step is to be able to index into them.  The number of axes that you specify will determine how deep a slice you get back.  For instance, if you are working with an order 3 tensor and you specify one axes, you'll get back a matrix.  If you specify two, you'll get back a vector.  And if you specify all three you'll get back a scalar.  Let's see some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 1, 3],\n",
       "       [1, 1, 1],\n",
       "       [1, 2, 1]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting back a matrix\n",
    "a = np.array([[ 5, 1 ,3], \n",
    "              [ 1, 1 ,1], \n",
    "              [ 1, 2 ,1]])\n",
    "b = np.array([1, 2, 3])\n",
    "order_3_tensor = np.tensordot(a, b, axes=0)\n",
    "\n",
    "order_3_tensor[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The numpy syntax for slicing may seem somewhat familar, but maybe not.  In vanilla Python you can choose a number of elements by slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "listing = list(range(20))\n",
    "\n",
    "print(listing[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The `[:]` just gives back a \"slice\" that's equal to the whole array, because we didn't specify any start an end.  But if say we just wanted the last 3 elements we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "listing = list(range(20))\n",
    "\n",
    "print(listing[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Of course, we can fully specify the start and end as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "listing = list(range(20))\n",
    "\n",
    "print(listing[5:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Of course, that's only for a one dimensional array.  In numpy, we are dealing with _many_ dimensions, which is why the syntax looks different.  In the example above we had:\n",
    "\n",
    "`order_3_tensor[:, :, 0]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The first axis is selected completely so like with vanilla Python lists, we simply do `[:` for the whole thing.  Next we get the entire second axis so we have: `[:, :`.  And finally, we just want the first \"element\" along the third axis: `[:, :, 0]`.  At first, this syntax seems confusing to everyone.  But once it clicks, by understanding numpy arrays as tensors, then everything in the syntax becomes _obvious_.\n",
    "\n",
    "Let's move onto our second example.  This time we'll get back just a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 1, 1])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting back a vector\n",
    "a = np.array([[ 5, 1 ,3], \n",
    "              [ 1, 1 ,1], \n",
    "              [ 1, 2 ,1]])\n",
    "b = np.array([1, 2, 3])\n",
    "order_3_tensor = np.tensordot(a, b, axes=0)\n",
    "\n",
    "order_3_tensor[:, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As you can see, we've specified _two_ axes and therefore, we get back an order 1 tensor.  Now, let's move onto the final example and get back just a scalar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting back a scalar\n",
    "a = np.array([[ 5, 1 ,3], \n",
    "              [ 1, 1 ,1], \n",
    "              [ 1, 2 ,1]])\n",
    "b = np.array([1, 2, 3])\n",
    "order_3_tensor = np.tensordot(a, b, axes=0)\n",
    "\n",
    "order_3_tensor[0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Something to notice, the first order 2 slice of our order 3 tensor is just the matrix `a`.  This is because the first element in our vector `b` is a 1.  I'll leave as an exercise, to see if you can write a slice that recovers the first row of matrix a.  Your answer should look like this:\n",
    "\n",
    "`[5, 1, 3]`\n",
    "\n",
    "Here is the starter code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "a = np.array([[ 5, 1 ,3], \n",
    "              [ 1, 1 ,1], \n",
    "              [ 1, 2 ,1]])\n",
    "b = np.array([1, 2, 3])\n",
    "order_3_tensor = np.tensordot(a, b, axes=0)\n",
    "\n",
    "# put in your slices here\n",
    "#order_3_tensor[, , ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: there is more than one way to recover the row in question.\n",
    "\n",
    "## Numpy Querying\n",
    "\n",
    "So far we've looked at the mathematical advantages of numpy.  There was another claim I made, that numpy is blazing fast.  And I wasn't kidding.  Compare numpy with vanilla python on _many_ operations and you'll see it's power.  That's because numpy is written in C with it's api specified in Python.  This means it can take advantage of the high level-ness of Python, while keeping the performance of C.  Truly, numpy is a modern marvel.\n",
    "\n",
    "Let's see some examples of numpy's performance in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364 µs ± 20.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit array = np.random.normal(0, 1, size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.55 ms ± 256 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def generate_array():\n",
    "    return [random.gauss(0, 1) for _ in range(10000)]\n",
    "\n",
    "%timeit generate_array()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As you can see, numpy is an _order_ of magnitude faster.  There are a lot of examples similar to this.  We'll look at just a few more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "array_one = np.random.normal(0, 1, size=10000)\n",
    "array_two = np.random.normal(0, 1, size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.15 µs ± 16.2 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit array_one + array_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def add_array_elements(array_one, array_two):\n",
    "    return [array_one[index] + array_two[index]\n",
    "            for index in range(len(array_one))]\n",
    "\n",
    "array_one = generate_array()\n",
    "array_two = generate_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715 µs ± 1.35 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit add_array_elements(array_one, array_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This time numpy is _2 orders_ of magnitude faster.  And the syntax was _much_ clearer.  Granted, it is less clear what we are doing, unless you know the numpy api.  Whenever you add two vectors in numpy, this adds each element together.  So beware!  You can also do multiplication just as easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "array_one = np.random.normal(0, 1, size=10000)\n",
    "array_two = np.random.normal(0, 1, size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57 µs ± 82.3 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit array_one * array_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def multiply_array_elements(array_one, array_two):\n",
    "    return [array_one[index] * array_two[index]\n",
    "            for index in range(len(array_one))]\n",
    "\n",
    "array_one = generate_array()\n",
    "array_two = generate_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727 µs ± 6.71 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit multiply_array_elements(array_one, array_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As you can see the orders of magnitude are similar for multiplication as they were for addition.  This is actually a _much_ bigger deal than may be obvious from these two examples.  All of linear algebra relies on these two operations.  That means linear regression, logistic regression, neural networks are all around 100 times faster implemented with numpy as compared to vanilla Python.  Of course, that is a blanket statement.  There are things you can do to make vanilla Python move faster.  And you can implement numpy poorly.  So this is a statement that needs to be taken with a grain of salt.  But still, numpy is faster for the things that matter to folks working in statistics and machine learning.  And that's just a fact.\n",
    "\n",
    "Since numpy is _so fast_.  It can actually be used as a minimal in memory database.  Here we'll go over some of the basics for querying data in numpy.  Some of the syntax here will be confusing at first, but with time and practice it will become clear.\n",
    "\n",
    "Let's look at a simple example of selecting a specific section of elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3142"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.random.normal(0, 1, size=10000)\n",
    "\n",
    "len(array[array > 0.5])/len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "I did a little stylizing here returning the percentage of the array over 0.5.  But basically this shows us the querying syntax.  This syntax is definitely _not_ obvious on first blush.  That said, once you get used to it, it's pretty powerful.  What's going on here is the following:\n",
    "\n",
    "the inner bit of syntax: `array > 0.5` is a boolean statement.  That is, implicitly every element of the array is checked for the condition, element of array greater than 0.5.  If the element meets the condition `True` is returned, otherwise False is returned.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then a boolean array is passed to the array as a slice:\n",
    "\n",
    "`array[boolean statement goes here]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Then indices where the index in question is `True` is returned.  Any indices that return `False` are ommited.  In this way, you can \"semantically slice\" your array.  To make this concrete, let's look at just the result of `array > 05`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False, ..., False, False, False])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As you can see, this is just an array of boolean values.  And if we counted up the number of times that resultant array has the value `True` it would equal the size of the semantically sliced array: `array[array > 0.5]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3142\n",
      "3142\n"
     ]
    }
   ],
   "source": [
    "print((array > 0.5).astype(int).sum())\n",
    "print(len(array[array > 0.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The way I counted the number of `True`'s may be confusing, so let's look at that: \n",
    "\n",
    "By casting the `True`'s and `False`'s as type `int` we turn the `True`'s into `1`'s and the `False`'s into `0`'s. \n",
    "\n",
    "We've show a simple example of how to query with numpy, but these examples can be as sophisticated as they are in any SQL dialect.  Next let's look at a complex querying statement - one with two statements:\n",
    "\n",
    "Here we will select all the elements between 0.5 and 0.7 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0666"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.random.normal(0, 1, size=10000)\n",
    "\n",
    "result = array[\n",
    "    (array > 0.5) &\n",
    "    (array < 0.7)\n",
    "]\n",
    "len(result)/len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice here the use of the `&` symbol.  This stands for 'and' when dealing with boolean statements.  Specifically those where both numbers are represented as binary.  Therefore we refer to `&` as `binary and`.  \n",
    "\n",
    "The way this operator works is as follows:\n",
    "\n",
    "1 `&` 1 = 1\n",
    "\n",
    "1 `&` 0 = 0\n",
    "\n",
    "0 `&` 1 = 0\n",
    "\n",
    "If we have longer binary numbers, then we simply apply the `&` element wise across the \"string\" of binary numbers:\n",
    "\n",
    "10 `&` 01 = 00\n",
    "\n",
    "There are other \"binary\" operators:\n",
    "\n",
    "* OR = `|`\n",
    "* NOT = `~`\n",
    "\n",
    "With these three operators we can create very sophisticated queries into our numpy arrays.  For instance, let's get all the numbers between 0.5 and 0.7 or the numbers less than 0.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6026"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = array[\n",
    "    ((array > 0.5) &\n",
    "    (array < 0.7)) |\n",
    "    (array < 0.1)\n",
    "]\n",
    "len(result)/len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Notice, if we want contiguous elements we should use an `&` statement.  If we want elements from different sections of the array, we should use an `|` statement.  Now let's say we wanted everything that's not amongst those elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3974"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = array[\n",
    "    ~(((array > 0.5) &\n",
    "    (array < 0.7)) |\n",
    "    (array < 0.1))\n",
    "]\n",
    "len(result)/len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Often times the `~` operator may seem unnecessary, but if you're querying from a variable it can make life very easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3974"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_query = ((array > 0.5) & (array < 0.7)) | (array < 0.1)\n",
    "result = array[~original_query]\n",
    "len(result)/len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now that we've seen how to query into our array's, it's time to move onto the real heart of numpy - it's use in writing programs dealing with linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Algebra and Numpy\n",
    "\n",
    "In this section we will cover theoretical and practical notions regarding linear algebra.\n",
    "\n",
    "Matrix Basics:\n",
    "\n",
    "* Row Reduced Echelon Form\n",
    "* Linear Independence\n",
    "* Invertability\n",
    "* Dot Product\n",
    "* Matrix as a function\n",
    "* Determinants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Row Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Row reduction although of little practical importance is useful tool for understanding many concepts in linear algebra.  Through row reduction, linear transforms, invertability, bases, subspaces, and the importance of shape are revealed.  Additionally, there are many \"practical\" decompositions that are similar to row reduced echelon form but are harder to grasp mathematically.\n",
    "\n",
    "The reason for the lack of practical importance of row reduction stems from the lack of precision of floating point numbers.  It is very, very hard to computer \"zero\".  Often it is the case that your computation will be off by a negliable amount, therefore the row reduction goes on forever.  Therefore, the row reduction algorithm we will implement will be incomplete in that we won't get an exact solution, but rather one that is \"close enough\".\n",
    "\n",
    "We will accomplish this by rounding down, so that if a floating point number is ever less than 0.00001 it will be rounded down to zero.  This is a reasonable assumption for some systems, however is a completely unreasonable assumption for others.  Given our purpose we will allow this assumption to be made.  However, this code should not be used in real world situations, as it will possibly or even likely given incorrect results.\n",
    "\n",
    "First lets look at an example of how we might do row reduction by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Matrix:\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "\n",
      "R2->R1*3-R2\n",
      "[[1 2]\n",
      " [0 2]]\n",
      "\n",
      "R2->R2*0.5\n",
      "[[1 2]\n",
      " [0 1]]\n",
      "\n",
      "R1->R1-R2*2\n",
      "[[-1  0]\n",
      " [ 0  1]]\n",
      "\n",
      "R1->R1*-1, end matrix\n",
      "[[1 0]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrix = np.array([[1,2], [3,4]])\n",
    "print(\"Start Matrix:\")\n",
    "print(matrix)\n",
    "print()\n",
    "matrix[1, :] = matrix[0,:]*3 - matrix[1, :] \n",
    "print(\"R2->R1*3-R2\")\n",
    "print(matrix)\n",
    "print()\n",
    "matrix[1, :] = matrix[1, :] * (0.5 * np.ones(2))\n",
    "print(\"R2->R2*0.5\")\n",
    "print(matrix)\n",
    "print()\n",
    "matrix[0, :] = matrix[1, :]*2 - matrix[0, :]\n",
    "print(\"R1->R1-R2*2\")\n",
    "print(matrix)\n",
    "print()\n",
    "matrix[0, :] = matrix[0, :] * (-1 * np.ones(2))\n",
    "print(\"R1->R1*-1, end matrix\")\n",
    "print(matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As you can see the goal is to go from any initial matrix to one in row reduced echelon form.  That is, from any matrix to the identity matrix.  In order to see the value of this technique, let's review one of the ways we can \"view\" a matrix - as a system of equations.  It is important to note, there are _many_ ways to think of matrices and this is but one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we are treating our matrix as a system of equations then we have the following equivalence:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Is equivalent to:\n",
    "\n",
    "$$ 1*x + 2*y $$ \n",
    "\n",
    "$$ 3*x + 4*y $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Of course every equation ought to have an \"equality\", otherwise it's not really an equation.  To do this in matrix notation is straight forward, we just tack on a vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ \\left( \n",
    "\\begin{array}{cc|c} \n",
    "1 & 2 & 5 \\\\ \n",
    "3 & 4 & 8 \\end{array} \n",
    "\\right) $$\n",
    "\n",
    "Which is equivalent to:\n",
    "\n",
    "$$ 1*x + 2*y = 5 $$ \n",
    "\n",
    "$$ 3*x + 4*y = 8 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now that we have a right hand side, we can simply apply all the transformations to both the left hand side matrix and the right hand side vector, in effect solving the system of equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Matrix:\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "\n",
      "Vector Start:\n",
      "[5 8]\n",
      "\n",
      "R2->R1*3-R2\n",
      "[[1 2]\n",
      " [0 2]]\n",
      "\n",
      "[5 7]\n",
      "\n",
      "R2->R2*0.5\n",
      "[[1 2]\n",
      " [0 1]]\n",
      "\n",
      "[5 3]\n",
      "\n",
      "[[1 0]\n",
      " [2 1]]\n",
      "R1->R1-R2*2\n",
      "[[-1  0]\n",
      " [ 0  1]]\n",
      "\n",
      "[1 3]\n",
      "\n",
      "R1->R1*-1, end matrix\n",
      "[[1 0]\n",
      " [0 1]]\n",
      "\n",
      "[-1  3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrix = np.array([[1,2], [3,4]])\n",
    "vector = np.array([5, 8])\n",
    "print(\"Start Matrix:\")\n",
    "print(matrix)\n",
    "print()\n",
    "print(\"Vector Start:\")\n",
    "print(vector)\n",
    "print()\n",
    "matrix[1, :] = matrix[0,:]*3 - matrix[1, :]\n",
    "vector[1] = vector[0]*3 - vector[1]\n",
    "print(\"R2->R1*3-R2\")\n",
    "print(matrix)\n",
    "print()\n",
    "print(vector)\n",
    "print()\n",
    "matrix[1, :] = matrix[1, :] * (0.5 * np.ones(2))\n",
    "vector[1] *= 0.5\n",
    "print(\"R2->R2*0.5\")\n",
    "print(matrix)\n",
    "print()\n",
    "print(vector)\n",
    "print()\n",
    "matrix[0, :] = matrix[1, :]*2 - matrix[0, :]\n",
    "vector[0] = vector[1]*2 - vector[0]\n",
    "print(\"R1->R1-R2*2\")\n",
    "print(matrix)\n",
    "print()\n",
    "print(vector)\n",
    "print()\n",
    "matrix[0, :] = matrix[0, :] * (-1 * np.ones(2))\n",
    "vector[0] *= -1\n",
    "print(\"R1->R1*-1, end matrix\")\n",
    "print(matrix)\n",
    "print()\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Therefore we have the unique solution to the system of equations:\n",
    "\n",
    "$$ x = -1, y = 3 $$\n",
    "\n",
    "In case it wasn't clear, we apply transforms from the left hand side to the ride hand side as well, thus giving us a unique solution, assuming both equations hold true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we see how to do this by hand, let's pull out the component pieces and then write a general algorithm for doing row reduction:\n",
    "\n",
    "1. check to see if a row is in the correct form.\n",
    "2. find row pivot\n",
    "3. subtract one row or a multiple of one row from another\n",
    "4. multiply a row by a constant.\n",
    "5. carry out the same steps on the accompanying vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now for our algorithm:\n",
    "\n",
    "```\n",
    "index_of_currect_incorrect_row = 0\n",
    "while row not in correct form:\n",
    "    if row pivot is not 1:\n",
    "        multiply by inverse of row pivot\n",
    "    do necessary subtractions so all non-pivot\n",
    "    values are zero\n",
    "    carry out all calculations on accompanying vector\n",
    "    index_of_current_incorrect_row += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Notice that we used a term in our algorithm called the pivot.  This is a term for the element in a row that ought to be 1.  The column within the row that should be 1 is the same as the row number.  So the zeroth row should have a one in the zeroth column of the row.  The first row should have a 1 in the first column.  And so on and so forth.  In general, there ought to be 1s along the diagonal of the matrix (this is the element where the row index and column index are the same).\n",
    "\n",
    "Now then, let's implement our algorithm!  Recall that we will be rounding to make sure this process doesn't go on forever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 0],\n",
       "        [0, 1]]),\n",
       " array([-1,  3]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def subtract_rows(matrix, row_index, pivot_index):\n",
    "    row_one = matrix[row_index, :]\n",
    "    row_two = matrix[pivot_index, :]\n",
    "    value = row_one[pivot_index]\n",
    "    matrix[row_index, :] = row_one - value*row_two\n",
    "    return matrix\n",
    "\n",
    "def multiply_row(row, value):\n",
    "    return (value) * np.ones(len(row))\n",
    "    \n",
    "def invert_pivot(matrix, pivot_index):\n",
    "    row = matrix[pivot_index, :]\n",
    "    value = row[pivot_index]\n",
    "    matrix[pivot_index, :] = row * multiply_row(row, 1/value)\n",
    "    return matrix\n",
    "    \n",
    "def get_identity(matrix):\n",
    "    return np.identity(matrix.shape[1])\n",
    "\n",
    "def is_pivot_one(matrix, pivot_index):\n",
    "    return matrix[pivot_index, row_index] == 1\n",
    "\n",
    "def invert_vector_pivot(matrix, vector, pivot_index):\n",
    "    value = matrix[pivot_index, pivot_index]\n",
    "    vector[pivot_index] = vector[pivot_index]*1/value\n",
    "    return vector \n",
    "\n",
    "def subtract_vector_elems(matrix, vector, row_index, pivot_index):\n",
    "    value = matrix[row_index, pivot_index]\n",
    "    vector[row_index] = vector[row_index] - value*vector[pivot_index]\n",
    "    return vector\n",
    "\n",
    "def row_reduce(matrix, vector):\n",
    "    cur_index = 0\n",
    "    while (matrix != get_identity(matrix)).any():\n",
    "        if not is_pivot_one(matrix, cur_index):\n",
    "            vector = invert_vector_pivot(matrix, vector, cur_index)\n",
    "            matrix = invert_pivot(matrix, cur_index)\n",
    "            matrix = matrix.round(5)\n",
    "            vector = vector.round(5)\n",
    "        rows = list(range(matrix.shape[1]))\n",
    "        rows.remove(cur_index)\n",
    "        for row_index in rows:\n",
    "            vector = subtract_vector_elems(\n",
    "                matrix, vector, row_index, cur_index\n",
    "            )\n",
    "            matrix = subtract_rows(matrix, row_index, cur_index)\n",
    "            matrix = matrix.round(5)\n",
    "            vector.round(5)\n",
    "        cur_index += 1\n",
    "    return matrix, vector\n",
    "\n",
    "matrix = np.array([[1,2], [3,4]])\n",
    "vector = np.array([5, 8])\n",
    "row_reduce(matrix, vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Notice, we get the same result as before.  This is because we've been working with integers.  Things get very dicey as our numbers get smaller than one or greater than one.  However, that's why we round our matrix and vector so that they are _always_ within five significant digits after the decimal point.  Anything smaller than that is just rounded down to zero.  This means we can be \"precise enough\" for toy applications.  However in the real world, this assumption will lead us to incorrect results, which is why most scientific packages do not implement a row reduction scheme.  \n",
    "\n",
    "In any event, the power of row reduction should be obvious at this point:\n",
    "\n",
    "Assuming a solution exists, we can solve any system of equations, of attribitrary size and do so with great ease.  Let's consider this 100 x 100 system of equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4182713031768799\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "matrix = np.random.rand(100,100) * 100\n",
    "vector = np.random.rand(100) * 100\n",
    "\n",
    "start = time.time()\n",
    "row_reduce(matrix, vector)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Our little \"toy\" solver finds a unique solution in a little over four tenths of a second!!!  If you tried to do this by hand, I guarantee you, it would take _much_ longer.  And the chance of being error free rapidly approaches zero.  Just for fun, let's see how long it takes to do 1000 x 1000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3740.573699235916\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "matrix = np.random.rand(1000,1000) * 100\n",
    "vector = np.random.rand(1000) * 100\n",
    "\n",
    "start = time.time()\n",
    "row_reduce(matrix, vector)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As said earlier, a lot of ideas flow from treating a matrix as a system of equations.  One notion already hinted at is that of \"solutions\" to the system.  Depending on the \"shape\" of the system, i.e. the number of rows and columns of the matrix, we know how many solutions are possible, apriori, or in other words, without actually solving the system!\n",
    "\n",
    "If the number of rows and columns are equal and the number of columns is linearly independent, then there is one and only one solution to the system.  In other words, you can be sure that one value for each of the parameters, uniquely \"solves\" the system.  Why might we care about this?\n",
    "\n",
    "Well as your number of equations grow, it becomes increasingly hard to know if a solution exists for your system.  Let us consider a real world example for clarity.  For this we will look at the Leontief Input-Output model.\n",
    "\n",
    "In this model of the economy there are several sectors, each sector produces some outputs, but also requires some inputs.  In the simplest possible economy each industry at least partially depends on every other sector.\n",
    "\n",
    "For our example we will consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Manufacturing</th>\n",
       "      <th>Technology</th>\n",
       "      <th>Real Estate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Manufacturing</th>\n",
       "      <td>84.858511</td>\n",
       "      <td>13.412168</td>\n",
       "      <td>40.370916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Technology</th>\n",
       "      <td>28.237123</td>\n",
       "      <td>12.023302</td>\n",
       "      <td>88.507121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Real Estate</th>\n",
       "      <td>38.635010</td>\n",
       "      <td>60.052319</td>\n",
       "      <td>12.739408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Manufacturing  Technology  Real Estate\n",
       "Manufacturing      84.858511   13.412168    40.370916\n",
       "Technology         28.237123   12.023302    88.507121\n",
       "Real Estate        38.635010   60.052319    12.739408"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "supply_matrix = np.random.rand(3,3) * 100\n",
    "demand_vector = np.random.rand(3) * 100\n",
    "\n",
    "df = pd.DataFrame(supply_matrix)\n",
    "df.columns = [\"Manufacturing\", \"Technology\", \"Real Estate\"]\n",
    "df.index = [\"Manufacturing\", \"Technology\", \"Real Estate\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We are making use of a dataframe in this example, only to make the matrix as clear as possible, but we can simply see this as an \"annotated\" matrix for now.  The product of each industry is defined on the diagonal of the matrix, so the (Manufacturing, Manufacturing) element is the output for Manufacturing, the industry.  Everything else in the Manufacturing row are inputs to the Manufacturing industry.  In general we can think of this matrix as our \"Supply\" of goods into the economy.\n",
    "\n",
    "As for the vector that we've defined, this is our \"Demand\" for goods in the economy.  The reason for the equality is because the market \"clears\" and is therefore in equilibrium when Supply equals Demand.  So if we wish to find this equilibrium point, i.e. when utility is maximized, we need only solve this system of equations.\n",
    "\n",
    "So we simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [-0., -0.,  1.]]),\n",
       " array([0.7443059 , 0.35999748, 0.37158   ]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_reduce(supply_matrix, demand_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As you can see, we were able to find a unique \"solution\", that is a unique value for the proportions that Manufacturing, Technology, and Real Estate out to produce in order to maximize utility!  Note, there is no special significance to the values of these values.  They are all less than one, merely by coincidence.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "I've already used this term, early in the text, but I failed to define it.  We will fix that here by giving a definition and providing some evidence of it's use.\n",
    "\n",
    "Linear independence is best seen through the second view or a matrix which we will discuss, the geometrix view.  Below is an example of a matrix as a 2-D representation of vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vector_one = np.array([5, 1])\n",
    "vector_two = np.array([4, -3])\n",
    "V = np.array([vector_one, vector_two])\n",
    "\n",
    "plt.quiver([0, 0], [0, 0], V[:,0], V[:,1], color=['r','b'], scale=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Two vectors are considered linearly dependent if:\n",
    "\n",
    "1. You can multiply by a constant to go from one vector to another.  This is called a dilation.\n",
    "\n",
    "Let's look at a picture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vector_one = np.array([5, 1])\n",
    "vector_two = vector_one * 2\n",
    "V = np.array([vector_one, vector_two])\n",
    "\n",
    "plt.quiver([0, 0], [0, 0], V[:,0], V[:,1], color=['r','b'], scale=21)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As you can see the two vectors \"rest\" on top of one another.  In other words, one vector completely captures the other vector.\n",
    "\n",
    "A set of vectors are considered linearly dependent as well if:\n",
    "\n",
    "2. You can add or subtract any two of them together times a scalar to produce another vector in the set.\n",
    "\n",
    "Let's look at another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vector_one = np.array([5, 1])\n",
    "vector_two = np.array([3, 7])\n",
    "vector_three = vector_one*2 - vector_two*0.5\n",
    "V = np.array([vector_one, vector_two, vector_three])\n",
    "\n",
    "plt.quiver([0, 0, 0], [0, 0, 0], V[:,0], V[:,1], color=['r','b', 'g'], scale=21)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here our green vector is a \"linear combination\" of our first two vectors, so by definition, the set of all three vectors is linearly dependent.  This all may seem arbitrary to this point, put let's see what happens if our system of equations is not linearly independent and we try to solve it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(array([[ 1.,  0.,  0.],\n",
      "       [-0.,  1.,  0.],\n",
      "       [ 0.,  0.,  1.]]), array([0.42516846, 0.40225278, 1.22888   ]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eric/.local/lib/python3.7/site-packages/ipykernel_launcher.py:34: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/home/eric/.local/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  app.launch_new_instance()\n",
      "/home/eric/.local/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in multiply\n",
      "  app.launch_new_instance()\n",
      "/home/eric/.local/lib/python3.7/site-packages/ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-69726b08ef31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_two\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_two\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-87-c671c84a378e>\u001b[0m in \u001b[0;36mrow_reduce\u001b[0;34m(matrix, vector)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mcur_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mget_identity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_pivot_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minvert_vector_pivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minvert_pivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-c671c84a378e>\u001b[0m in \u001b[0;36mis_pivot_one\u001b[0;34m(matrix, pivot_index)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_pivot_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpivot_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpivot_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minvert_vector_pivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpivot_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vector_one = np.random.rand(3) * 100\n",
    "vector_two = np.random.rand(3) * 100\n",
    "vector_three = vector_one.copy()\n",
    "matrix = np.array([vector_one, vector_two, vector_three]).T\n",
    "matrix_two = np.random.rand(3, 3) * 100\n",
    "vector = np.random.rand(3) * 100\n",
    "\n",
    "print(matrix.shape == matrix_two.shape)\n",
    "print(row_reduce(matrix_two, vector))\n",
    "type(matrix), type(matrix_two)\n",
    "print(row_reduce(matrix, vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We get an error when we try to do the row reduction!  This is because we fail to row reduce our matrix.  Let's verify this by looking at another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Matrix:\n",
      "[[ 1  2  3]\n",
      " [ 3  4  5]\n",
      " [ 9 14 19]]\n",
      "\n",
      "Vector Start:\n",
      "[5 8 9]\n",
      "\n",
      "R2->R1*3-R2\n",
      "[[ 1  2  3]\n",
      " [ 0  2  4]\n",
      " [ 9 14 19]]\n",
      "\n",
      "[5 7 9]\n",
      "\n",
      "R2->R1*9-R2\n",
      "[[1 2 3]\n",
      " [0 2 4]\n",
      " [0 4 8]]\n",
      "\n",
      "[ 5  7 36]\n",
      "\n",
      "R2->R2*0.5\n",
      "[[1 2 3]\n",
      " [0 1 2]\n",
      " [0 4 8]]\n",
      "\n",
      "[ 5  3 36]\n",
      "\n",
      "R2->R1*9-R2\n",
      "[[1 2 3]\n",
      " [0 1 2]\n",
      " [0 0 0]]\n",
      "\n",
      "[  5   3 -24]\n",
      "\n",
      "R1->R1-R2*2\n",
      "[[-1  0  1]\n",
      " [ 0  1  2]\n",
      " [ 0  0  0]]\n",
      "\n",
      "[  1   3 -24]\n",
      "\n",
      "R1->R1*-1, end matrix\n",
      "[[ 1  0 -1]\n",
      " [ 0  1  2]\n",
      " [ 0  0  0]]\n",
      "\n",
      "[ -1   3 -24]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector_one = np.array([1, 2, 3])\n",
    "vector_two = np.array([3, 4, 5])\n",
    "vector_three = 3 * vector_one + 2 * vector_two\n",
    "matrix = np.array([vector_one, vector_two, vector_three])\n",
    "vector = np.array([5, 8, 9])\n",
    "print(\"Start Matrix:\")\n",
    "print(matrix)\n",
    "print()\n",
    "print(\"Vector Start:\")\n",
    "print(vector)\n",
    "print()\n",
    "matrix[1, :] = matrix[0,:]*3 - matrix[1, :]\n",
    "vector[1] = vector[0]*3 - vector[1]\n",
    "print(\"R2->R1*3-R2\")\n",
    "print(matrix)\n",
    "print()\n",
    "print(vector)\n",
    "print()\n",
    "matrix[2, :] = matrix[0,:]*9 - matrix[2, :]\n",
    "vector[2] = vector[0]*9 - vector[2]\n",
    "print(\"R2->R1*9-R2\")\n",
    "print(matrix)\n",
    "print()\n",
    "print(vector)\n",
    "print()\n",
    "matrix[1, :] = matrix[1, :] * (0.5 * np.ones(3))\n",
    "vector[1] *= 0.5\n",
    "print(\"R2->R2*0.5\")\n",
    "print(matrix)\n",
    "print()\n",
    "print(vector)\n",
    "print()\n",
    "matrix[2, :] = matrix[1,:]*4 - matrix[2, :]\n",
    "vector[2] = vector[1]*4 - vector[2]\n",
    "print(\"R2->R1*9-R2\")\n",
    "print(matrix)\n",
    "print()\n",
    "print(vector)\n",
    "print()\n",
    "\n",
    "matrix[0, :] = matrix[1, :]*2 - matrix[0, :]\n",
    "vector[0] = vector[1]*2 - vector[0]\n",
    "print(\"R1->R1-R2*2\")\n",
    "print(matrix)\n",
    "print()\n",
    "print(vector)\n",
    "print()\n",
    "matrix[0, :] = matrix[0, :] * (-1 * np.ones(3))\n",
    "vector[0] *= -1\n",
    "print(\"R1->R1*-1, end matrix\")\n",
    "print(matrix)\n",
    "print()\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As you can see, this is the farest we can take this.  We can't row reduce the matrix any further because we've elminated the pivot the final row.  This is because vector three is a linear combination of vector one and vector two.  Therefore the columns aren't linearly independent.  \n",
    "\n",
    "What we've seen so far generalizes to _any_ system of equations.  Additionally, any system of equations such that the number of rows and number of columns are unequal will not be row reducible, unless there are \"extra\" rows or columns that are linear combinations of a set of a \"square\" subset which are linearly independent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To make this concrete consider this linear independent matrix:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Now if we add the following vector:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    " 4 \\\\\n",
    " 6 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We get:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 4\\\\\n",
    "3 & 4 & 6\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let's see what happens if we try to row reduce this matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Matrix:\n",
      "[[1 2 4]\n",
      " [3 4 6]]\n",
      "\n",
      "R2->R1*3-R2\n",
      "[[1 2 4]\n",
      " [0 2 6]]\n",
      "\n",
      "R2->R2*0.5\n",
      "[[1 2 4]\n",
      " [0 1 3]]\n",
      "\n",
      "R1->R1-R2*2\n",
      "[[-1  0  2]\n",
      " [ 0  1  3]]\n",
      "\n",
      "R1->R1*-1, end matrix\n",
      "[[ 1  0 -2]\n",
      " [ 0  1  3]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 4])\n",
    "b = np.array([3, 4, 6])\n",
    "\n",
    "matrix = np.array([a, b])\n",
    "\n",
    "print(\"Start Matrix:\")\n",
    "print(matrix)\n",
    "print()\n",
    "matrix[1, :] = matrix[0,:]*3 - matrix[1, :]\n",
    "vector[1] = vector[0]*3 - vector[1]\n",
    "print(\"R2->R1*3-R2\")\n",
    "print(matrix)\n",
    "print()\n",
    "matrix[1, :] = matrix[1, :] * (0.5 * np.ones(3))\n",
    "vector[1] *= 0.5\n",
    "print(\"R2->R2*0.5\")\n",
    "print(matrix)\n",
    "print()\n",
    "matrix[0, :] = matrix[1, :]*2 - matrix[0, :]\n",
    "vector[0] = vector[1]*2 - vector[0]\n",
    "print(\"R1->R1-R2*2\")\n",
    "print(matrix)\n",
    "print()\n",
    "matrix[0, :] = matrix[0, :] * (-1 * np.ones(3))\n",
    "vector[0] *= -1\n",
    "print(\"R1->R1*-1, end matrix\")\n",
    "print(matrix)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As you can see there is a 2 x 2 identity matrix embedded in this 2 x 3 matrix.  We can't row reduce any further than this, but clearly the third vector is surpurflous and doesn't contribute to us finding a unique solution.  In other words, any system of equations like this will have _infinitely_ many equations.  A unique solution for x and y.  But an infinite number of solutions for the z variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Invertability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "It turns out that the uniqueness of solutions of systems of equations has a consequence, specifically that if a matrix has a unique solution and it's individual component vectors are linearly independent, then the matrix is invertable.  Consequentially, invertability is not a quality for all matrices and therefore you cannot always take an inverse.  \n",
    "\n",
    "In order to find the inverse of the matrix we simply apply all the same steps to an identity matrix that we apply to the original matrix as we row reduce it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inverse_subtract_rows(matrix, inverse, row_index, pivot_index):\n",
    "    row_one = matrix[row_index, :]\n",
    "    row_two = matrix[pivot_index, :]\n",
    "    inverse_row_one = inverse[row_index, :]\n",
    "    inverse_row_two = inverse[pivot_index, :]\n",
    "    value = row_one[pivot_index]\n",
    "    matrix[row_index, :] = row_one - value*row_two\n",
    "    inverse[row_index, :] = inverse_row_one - value*inverse_row_two\n",
    "    return matrix, inverse\n",
    "    \n",
    "def inverse_invert_pivot(matrix, inverse, pivot_index):\n",
    "    row = matrix[pivot_index, :]\n",
    "    inverse_row = inverse[pivot_index, :]\n",
    "    value = row[pivot_index]\n",
    "    matrix[pivot_index, :] = row * multiply_row(row, 1/value)\n",
    "    inverse[pivot_index, :] = inverse_row * multiply_row(inverse_row, 1/value)\n",
    "    return matrix, inverse\n",
    "    \n",
    "def find_inverse(matrix):\n",
    "    cur_index = 0\n",
    "    inverse = np.identity(matrix.shape[1])\n",
    "    while (matrix != get_identity(matrix)).any():\n",
    "        if not is_pivot_one(matrix, cur_index):\n",
    "            matrix, inverse = inverse_invert_pivot(matrix, inverse, cur_index)\n",
    "            matrix = matrix.round(5)\n",
    "        rows = list(range(matrix.shape[1]))\n",
    "        rows.remove(cur_index)\n",
    "        for row_index in rows:\n",
    "            matrix, inverse = inverse_subtract_rows(\n",
    "                matrix, inverse, row_index, cur_index\n",
    "            )\n",
    "            matrix = matrix.round(5)\n",
    "        cur_index += 1\n",
    "    return inverse\n",
    "\n",
    "matrix = np.array([[1, 2], [3, 4]])\n",
    "inverse = find_inverse(matrix)\n",
    "\n",
    "matrix = np.array([[1, 2], [3, 4]])\n",
    "matrix @ inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As you can see we were able to find the inverse by modifying the the row reduction code.  When you do a matrix mulitplication of the original matrix and it's inverse you get back the identity matrix, because this is the matrix equivalent of a \"1\".\n",
    "\n",
    "Being able to invert a matrix is incredibly useful as we'll come to see for linear regression and other modeling techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix Multiplication\n",
    "\n",
    "Here we'll take a deeper look at matrix multiplication, specificially we'll consider the dot product.  The matrix multiplication we just did:\n",
    "\n",
    "`matrix @ inverse` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For each row in the left hand side matrix, carry out the dot product with the column on the right hand side.  Therefore this will only work if the number of columns on the left hand side equals the number of rows on the right hand side.  \n",
    "\n",
    "Let's look at a quick example with two matrices:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\\n",
    "\\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "\\end{pmatrix}\n",
    "%\n",
    "\\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "7 & 10 \\\\\n",
    "15 & 22 \\\\\n",
    "\\end{pmatrix}\n",
    "\\\n",
    "$$\n",
    "\n",
    "to get the top left cell of the resulting matrix, we do:\n",
    "\n",
    "1(1) + 2(3) = 7\n",
    "\n",
    "to get the top right cell of the resulting matrix, we do:\n",
    "\n",
    "1(2) + 2(4) = 10\n",
    "\n",
    "to get the bottom left cell of the resulting matrix, we do:\n",
    "\n",
    "3(1) + 4(3) = 15\n",
    "\n",
    "to get the bottom right cell of the resulting matrix, we do:\n",
    "\n",
    "3(2) + 4(4) = 22\n",
    "\n",
    "In general the algorithm for the ith row and jth column is:\n",
    "\n",
    "$$ \\sum_{k=1}^{n}a_{ik}b_{kj} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note, we can also define the dot product for now vectors, simply by taking the transpose of the right hand side vector:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "3 \\\\\n",
    "\\end{pmatrix}\n",
    "%\n",
    "\\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "7\n",
    "\\end{pmatrix}\n",
    "\\\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Notice, in this case the length of the two vectors must be equal, otherwise the dot product is not defined.\n",
    "\n",
    "Because everything in linear algebra has both a geometric and algebriac interpretation we'll consider both of these when interpretting this new operator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us consider the geometrix interpretation first:\n",
    "\n",
    "$$ X \\cdot Y = |X||Y|\\cos\\theta $$\n",
    "\n",
    "Here $\\theta$ is the angle between the two vectors.  \n",
    "\n",
    "This formulation allows us to make the following statement:\n",
    "\n",
    "If: \n",
    "\n",
    "$$ X \\cdot Y = 0 $$\n",
    "\n",
    "Then\n",
    "\n",
    "$X$ is perpendicular to $Y$!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As for the algebriac interpretation:\n",
    "\n",
    "In general, the dot product can be defined as the length of the projection of $X$ onto the unit vector in the direction of $Y$.  So we get a sense of the \"magnitude\" of X and Y by taking their dot product.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix as a Function\n",
    "\n",
    "There won't be any real new concepts in this section, but we are going take a moment to notice some facts about matrices which will be valuable for how we think about them.  We treat a matrix in one of two ways:\n",
    "\n",
    "1. As holding data\n",
    "2. As \"storing\" transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The first representation of a matrix will probably be more familiar to most of you, if you've ever worked with excel, csvs or a database.  All of that data is stored as matrices.  \n",
    "\n",
    "We've sort of been seeing this second representation throughout our discussion of linear algebra thus far.  Matrix multiplication is the major vehicle for applying our matrix.  If we consider the following example:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\\n",
    "\\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "\\end{pmatrix}\n",
    "%\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "3 & 3 \\\\\n",
    "7 & 4 \\\\\n",
    "\\end{pmatrix}\n",
    "\\\n",
    "$$\n",
    "\n",
    "We can treat our first matrix:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "As our data matrix.  And our second matrix:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "along with matrix multiplication as our function.  It transforms our input data and produces our resultant matrix:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "3 & 3 \\\\\n",
    "7 & 4 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "So we can sort of think of our second matrix as a function.  Where the matrix's values are the parameters to the function and the matrix plus the multiplication \"is\" the function.\n",
    "\n",
    "A consequence of this way of thinking about matrices is that we can \"combine\" functions by applying matrix multiplication to individual matrices in order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So for instance, say we had:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & -1 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "If we do:\n",
    "\n",
    "$$\n",
    "\\\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & -1 \\\\\n",
    "\\end{pmatrix}\n",
    "%\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "-1 & -1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\\n",
    "$$\n",
    "\n",
    "Then \n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "-1 & -1 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Is the same as first taking _any_ compatiable matrix and multiplying it first by:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & -1 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "And then by\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Let's verify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "first_matrix = np.array([[1, 0], [0, -1]])\n",
    "second_matrix = np.array([[1, 0], [1, 1]])\n",
    "third_matrix =  second_matrix @ first_matrix \n",
    "random_matrix = matrix_two = np.random.rand(2, 2) * 100\n",
    "\n",
    "first_intermediate_result = random_matrix @ second_matrix\n",
    "second_intermediate_result = first_intermediate_result @ first_matrix \n",
    "(second_intermediate_result == random_matrix @ third_matrix).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Notice, the order in which the first and second matrix are combined matter!  So for instance, if we did this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_matrix = np.array([[1, 0], [0, -1]])\n",
    "second_matrix = np.array([[1, 0], [1, 1]])\n",
    "third_matrix =  first_matrix @ second_matrix \n",
    "random_matrix = matrix_two = np.random.rand(2, 2) * 100\n",
    "\n",
    "first_intermediate_result = random_matrix @ second_matrix\n",
    "second_intermediate_result = first_intermediate_result @ first_matrix \n",
    "(second_intermediate_result == random_matrix @ third_matrix).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "All we did was switch the order of the matrix multiplication of the third_matrix.  This is because matrix multiplication is not commutative.  This means the order of the matrix multiplication will matter and in general, if we have two matrices, A and B then:\n",
    "\n",
    "$$\n",
    "AB \\neq BA\n",
    "$$\n",
    "\n",
    "We've actually already used this fact.  The above was how we were able to arrive at the inverse of A given a row reduction scheme for A.  We applied each transform, storing the individual transforms in a matrix.  This necesary implies we can move in the other direction as well:\n",
    "\n",
    "Going from one matrix to many \"component\" transformations.  The process of deducing these set of component transformations is called matrix decomposition.\n",
    "\n",
    "## Determinants\n",
    "\n",
    "The determinant of a matrix is a very helpful tool for understanding how a transformation matrix \"acts\" on a data matrix.\n",
    "\n",
    "The determinant conveys two pieces of information:\n",
    "\n",
    "1. the magnitude of the transform\n",
    "2. whether the transform preserves or reverses orientation\n",
    "\n",
    "This allows us to tell ahead of time how our matrix will transform other matrices and what we can expect as a result.  Computing determinants is easy with numpy, let's look at the detminerant of our \"third_matrix\" from the last example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(third_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now let's compare this with our other two matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 1.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(first_matrix), np.linalg.det(second_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As you can see, we get the magnitude from the first matrix and second matrix and we get the orientation change from the first matrix.  As should be clear, the determinant is a \"lossy\" measure, because we represent a matrix as a scalar.  However, there is still power in looking at representations like this.  We can assess the individual effects of component transformations.  And then see how their combined effects will augment and orient other matrices."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
