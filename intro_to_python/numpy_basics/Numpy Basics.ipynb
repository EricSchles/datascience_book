{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Basics\n",
    "\n",
    "Probably the biggest shift when getting started with data science is the syntax of numpy and pandas because it differs so much from other programming paradigms.  In this section we will walk through some numpy basics:\n",
    "\n",
    "* why numpy?\n",
    "* introduction to tensors\n",
    "* numpy shapes\n",
    "* numpy slicing\n",
    "* numpy querying\n",
    "* linear algebra in numpy\n",
    "\n",
    "## Why Numpy?\n",
    "\n",
    "Technically, anything you can do in numpy you can do in plain old python.  And arguably the syntax will be easier to understand, that is, unless you use numpy as it is intended.  Numpy _can_ be used for all the basic stuff that you'll find most of the examples for on the internet.  But it's really _intended_ to be used for a new paradigm of programming.  One that's caught on in the statistical and deep learning communities (which have at least some overlap).  \n",
    "\n",
    "Numpy's api and computation is optimized for the manipulation of algebraic structures.  You can use it to do most of the computation that you can do with vanilla Python, or any other programming lanugage.  But you probably shouldn't.  We can think of the numpy api as sort of a directed language.  It's not quiet that, because numpy is mostly \"about\" syntax change.  You are thinking about the world from a difference lense.  But I digress.\n",
    "\n",
    "Here are just a few of the benefits of numpy:\n",
    "\n",
    "* incredible speed - numpy is _much_ faster than vanilla python (it can even outperform Java sometimes)\n",
    "* a beautiful and well organized api\n",
    "* tons of utility functions\n",
    "* amazing documentation\n",
    "* it's completely free (whaaaaat)\n",
    "\n",
    "To really understand numpy and the power it brings, we need to understand tensors.  Because without them, numpy honestly doesn't make much sense, at least at first.  And even once you start to get used to the syntax, without the mental model of a tensor, you'll completely miss the point of using it.\n",
    "\n",
    "## Tensors\n",
    "\n",
    "Tensors are some of the most powerful objects around.  In fact, this book is basically just a \"how do I use tensors\" most of the time.  The chapter on linear regression?  That's just about tensors.  The chapter on classification?  More applications of tensors.  Much of machine learning is built on tensors.  Specifically, on matrices.  Because I define the matrix in another chapter, I won't go into a ton of detail about what they are, or how to use them.  \n",
    "\n",
    "Basically, tensors are the generalization of matrices.  Examples of tensors include:\n",
    "\n",
    "* scalars\n",
    "* vectors\n",
    "* matrices\n",
    "* order-3 tensors and higher\n",
    "\n",
    "A scalar is an order zero tensor - because it's just a single number, like say the number `5`.  A vector is a one dimensional collection of numbers representing data or an equation, like: \n",
    "\n",
    "$$ \\begin{pmatrix}\n",
    "1  \\\\\n",
    "4  \\\\\n",
    "7 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "A matrix is a two dimensional collect of numbers representing a system of equations, like:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "An order three tensor looks like a data cube.  There is no easy way to show such a cube in latex, so you'll have to imagine this to some extent:\n",
    "\n",
    "$$ A_{1} = \n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$ A_{2} = \n",
    "\\begin{pmatrix}\n",
    "3 & 2 & 3 \\\\\n",
    "7 & 6 & 6 \\\\\n",
    "7 & 2 & 9\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$ A_{3} = \n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "7 & 6 & 4 \\\\\n",
    "6 & 2 & 19\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Now imagine $A_{1}, A_{2}, A_{3}$ as one object.  This is an order three tensor.  It has three axes - $(i,j,k)$ and you can specify elements across these three axes.  So $(0,0,0) = 1$, $(1,0,0) = 4$, and $(3,0,3) = 6$.  Here the i is the row index, j is the column index and k is the matrix index.  You can also do this for order 4 and up to n, where n is any finite natural number you like.  Why might you want to ever do this in practice?  It turns out there are actually a ton of good reasons.\n",
    "\n",
    "Here are just two of them:\n",
    "\n",
    "1. Let's say you want to model multivariate timeseries geospatial data.  This is naturally an order 4 tensor.  The first two dimensions will be each snapshot of multivariate data.  Your third dimension will be that snapshot overtime.  And your forth will be over time and different geographies.  Thinking about it this way is useful for capturing shared weights between time and geographies.  How you model your data matters.  And by ignoring the time or geospatial components of your data, you might lose some important information.\n",
    "\n",
    "2. You can get a performance boost, statistically speaking.  As this paper shows: https://arxiv.org/pdf/1811.06569.pdf you can get a decent accuracy boost by treating your neural network as a higher order tensor.\n",
    "\n",
    "## Numpy Shapes\n",
    "\n",
    "Now that you know what a tensor is, the syntax of numpy will seem obvious and straight forward.  Let's start by showing how to represent each of the tensors we've discussed thus far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# order 0 tensor\n",
    "import numpy as np\n",
    "\n",
    "scalar = np.array([1])\n",
    "print(scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may think we've done nothing new here.  But actually we have!  For starters, numpy attaches types to anything passed into it.  And it does this _implicitly_.  You never have to name the types.  That by itself would be a feat of engineering prowess.  Let's see what I'm talking about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dtype` property tells us what kind of data is in our tensor.  Since there are mathematical consequences to what's in our tensor, it's best to define one type per tensor.  Usually floats are the most flexible.  Of course, you can define a tensor with multiple types.  But for any serious mathematical computation, this is discouraged.  However, there are lots of programming instances when defining multiple types in a data structure is useful and important, which is why this paradigm is supported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hello', '1'], dtype='<U5')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([\"hello\", 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen an order 0 tensor and an order 1 tensor, by accident, let's define another order 1 tensor, called a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 7]\n"
     ]
    }
   ],
   "source": [
    "vector = np.array([1, 4, 7])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to note here:\n",
    "\n",
    "1. a vector is a collection of scalars.\n",
    "2. a vector represents a mathematical object, not just an array.\n",
    "\n",
    "Because this is a mathematical object, we can do things like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_one = np.array([1, 4, 7])\n",
    "vector_two = np.array([2, 4, 6])\n",
    "\n",
    "np.matmul(vector_one, vector_two.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've ever taken a linear algebra course the answer that's produced will seem surprising.  That's because technically numpy defaults to an array of scalars for a one dimensional array passed to the `np.array` method, rather than a vector.  The difference here is important.\n",
    "\n",
    "Because algebraic objects are defined in part by the algebraic operators attached to them, this detail matters.  Specifically, here the \"multiplication\" attached to our vectors is the inner product in this case.  If we want the outer product, which is what most folks who have taken linear algebra would expect, then we need to tell numpy that we are working with tensors or order 1 aka vectors and not a collection of scalars.\n",
    "\n",
    "We do that by using the `reshape` method a powerful tool that will allow us to represent tensors of any order we like.  But first let's start with the basics of turning a collection of scalars into an order 1 tensor, aka a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector one:\n",
      "[[1]\n",
      " [4]\n",
      " [7]]\n",
      "vector two: [[2 4 6]]\n",
      "result:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2,  4,  6],\n",
       "       [ 8, 16, 24],\n",
       "       [14, 28, 42]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_one = np.array([1, 4, 7])\n",
    "vector_two = np.array([2, 4, 6])\n",
    "\n",
    "vector_one = vector_one.reshape(3, 1)\n",
    "vector_two = vector_two.reshape(1, 3)\n",
    "print(\"vector one:\")\n",
    "print(vector_one)\n",
    "print(\"vector two:\", vector_two)\n",
    "print(\"result:\")\n",
    "np.matmul(vector_one, vector_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By reshaping our vectors to the appropriate shapes, we were able to produce a matrix!  This is a general fact of linear algebra - you can get a matrix by applying a matrix multiplication (`matmul`), also known as the outer product, to two vectors.  This \"trick\" of taking two lower dimensional tensors to create a higher order one will actually work for _any_ tensor we like.  If we want to recover an order 3 tensor we simply need to multiply a matrix by a vector.  That's because the order is additive, by tensor product!  Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 5 10 15]\n",
      "  [ 1  2  3]\n",
      "  [ 3  6  9]]\n",
      "\n",
      " [[ 1  2  3]\n",
      "  [ 1  2  3]\n",
      "  [ 1  2  3]]\n",
      "\n",
      " [[ 1  2  3]\n",
      "  [ 2  4  6]\n",
      "  [ 1  2  3]]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[ 5, 1 ,3], \n",
    "              [ 1, 1 ,1], \n",
    "              [ 1, 2 ,1]])\n",
    "b = np.array([1, 2, 3])\n",
    "print(np.tensordot(a, b, axes=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `a` is a matrix, `b` is a tensor.  And by taking the tensor product of the two of them, we recover an order 3 tensor!  Notice we have to provide an axes or the `tensorproduct` method.  This is because a tensor product can be defined on any order.  We've already seen an order 1 tensor product, the inner product.  And we've seen an order 2 tensor product, the outer product.  In higher spaces, we generally refer to the product as simply the tensor product where the order comes from context.  However, please take care to be clear about the shapes of your tensors, otherwise you'll end up doing the _wrong multiplication_.  \n",
    "\n",
    "I'll leave as an exercise creating tensors of order 4, 5, and 6.  Happy multiplying!\n",
    "\n",
    "## Numpy Slicing\n",
    "\n",
    "Now that we've seen how to create our tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
