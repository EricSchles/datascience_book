{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas in Engineering\n",
    "\n",
    "Welcome to ideas for engineering!  The goal of this notebook and adjoining powerpoint is to explain the principals of clean code in clear and simple terms.  We will be exploring some basic ideas in this tutorial:\n",
    "\n",
    "* what is clean code?\n",
    "* The ideology of writing a function\n",
    "* Documentation - why and how\n",
    "* An introduction to test driven development\n",
    "* The power of classes\n",
    "* debugging with:\n",
    "    * code.interact\n",
    "    * IPython.embed\n",
    "* How to and when to make a pull request\n",
    "* The power of continuous integration\n",
    "* The power of continuous deployment\n",
    "* Automation - CI and CD together\n",
    "\n",
    "## What is clean code?\n",
    "\n",
    "Clean code is code that is clear and easily readable.  It's code that doesn't just explain what's happening, it explains why it's happening.  The goal of code, especially in a language like Python is to be clear, and obvious.  We don't write Python because it's the fastest language in the world, but because it has the power to _optimize developer time_, which is a much more expensive resource than compute time.  That said, we should always try to performance tune our code, once all the major functionality has been written.  \n",
    "\n",
    "Python in particular, lends itself to being clean by default, because there is one and preferably only one way one to do things.  This means, most code blocks should be easily identifiable, no matter what the context.  So you should always be able to know _what_ the code does, but you may not necessarily understand _why_ the code does it.  The general rules of programming are as follows:\n",
    "\n",
    "* each line should do at most one simple thing\n",
    "* each function should be a single transformation or semantic action\n",
    "* each class should be a combination of transformations and data which are semantically related\n",
    "* ideally you should only have a few classes per file\n",
    "* where possible you should reuse code\n",
    "\n",
    "Let's see some examples of patters of clean code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello_there_friends\n",
      "hello_there_friends\n"
     ]
    }
   ],
   "source": [
    "def remove_whitespace(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes whitespace between words\n",
    "    \n",
    "    Parameters:\n",
    "    * name - the string which may or may not\n",
    "    have whitespace.\n",
    "    \n",
    "    Returns:\n",
    "    A string without whitespace between characters.\n",
    "    \n",
    "    Examples:\n",
    "    >>> remove_whitespace(\"Hello There\")\n",
    "    \"HelloThere\"\n",
    "    >>> remove_whitespace(\"HelloThere\")\n",
    "    \"HelloThere\"\n",
    "    \"\"\"\n",
    "    return \"\".join(name.split(\" \"))\n",
    "\n",
    "def get_upper_case_indices(name: str) -> list:\n",
    "    \"\"\"\n",
    "    Gets the indices of all upper case words\n",
    "    \n",
    "    Parameters:\n",
    "    * name - looks for uppercase \n",
    "    characters in this string\n",
    "    \n",
    "    Returns:\n",
    "    A list of indices of the uppercase characters\n",
    "    \n",
    "    Examples:\n",
    "    >>> get_upper_case_indices(\"HelloThere\")\n",
    "    [0, 5]\n",
    "    >>> get_upper_case_indices(\"HelloThereFriends\")\n",
    "    [0, 5, 10]\n",
    "    \"\"\"\n",
    "    upper_case_indices = []\n",
    "    for index, letter in enumerate(name):\n",
    "        if letter.isupper():\n",
    "            upper_case_indices.append(index)\n",
    "    return upper_case_indices\n",
    "\n",
    "def get_lower_case_words(upper_case_indices: list, name: str) -> list:\n",
    "    \"\"\"\n",
    "    Gets a list of the words, in lower case, split on uppercase\n",
    "    characters\n",
    "    \n",
    "    Parameters:\n",
    "    * upper_case_indices - a list of integers corresponding\n",
    "    to upper case letters in the string\n",
    "    * name - the string to split and process\n",
    "    \n",
    "    Returns:\n",
    "    A list of words in lower case\n",
    "    \n",
    "    Examples:\n",
    "    >>> get_lower_case_words([0, 5], \"HelloThere\")\n",
    "    [\"hello\", \"there\"]\n",
    "    >>> get_lower_case_words([0, 5, 10], \"HelloThereFriends\")\n",
    "    >>> [\"hello\", \"there\", \"friends\"]\n",
    "    \"\"\"\n",
    "    start = 0\n",
    "    lower_case_words = []\n",
    "    for index in upper_case_indices[1:]:\n",
    "        lower_case_words.append(\n",
    "            name[start:index].lower()\n",
    "        )\n",
    "        start = index\n",
    "    lower_case_words.append(\n",
    "        name[index:].lower()\n",
    "    )\n",
    "    return lower_case_words\n",
    "\n",
    "def connect_words(lower_case_words: list) -> str:\n",
    "    \"\"\"\n",
    "    Connects a list of words via a '_'\n",
    "    \n",
    "    Parameters:\n",
    "    * lower_case_words - a list of lower case words\n",
    "    \n",
    "    Returns:\n",
    "    A string of concatenated words, with '_' between\n",
    "    each word.\n",
    "    \"\"\"\n",
    "    return \"_\".join(lower_case_words)\n",
    "\n",
    "def to_snake_case(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Takes a camel case string\n",
    "    and makes it snake case\n",
    "\n",
    "    Parameters:\n",
    "    - name - the string to translate\n",
    "\n",
    "    Returns:\n",
    "    The snake cased string\n",
    "    \n",
    "    Example:\n",
    "    >>> to_snake_case(\"HelloThere\")\n",
    "    'hello_there'\n",
    "    >>> to_snake_case(\"hello_there\")\n",
    "    'hello_there'\n",
    "    >>> to_snake_case(\"Hello There\")\n",
    "    'hello_there'\n",
    "    \"\"\"\n",
    "    name = remove_whitespace(name)\n",
    "    upper_case_indices = get_upper_case_indices(name)\n",
    "    lower_case_words = get_lower_case_words(\n",
    "        upper_case_indices, name\n",
    "    )\n",
    "    return connect_words(lower_case_words)\n",
    "\n",
    "print(to_snake_case(\"HelloThereFriends\"))\n",
    "print(to_snake_case(\"Hello There Friends\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take these methods and actually make a class, which offers a rich array of string processing functionality, for very little extra work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StringProcessor:\n",
    "    \"\"\"\n",
    "    An object for processing strings.  The main methods of interest are:\n",
    "    * to_camel_case\n",
    "    * to_snake_case\n",
    "    \n",
    "    The preferred way to instantiate the class is as follows:\n",
    "    >>> processor = StringProcessing()\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def remove_whitespace(self, name: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes whitespace between words\n",
    "\n",
    "        Parameters:\n",
    "        * name - the string which may or may not\n",
    "        have whitespace.\n",
    "\n",
    "        Returns:\n",
    "        A string without whitespace between characters.\n",
    "\n",
    "        Examples:\n",
    "        >>> processor = StringProcessor()\n",
    "        >>> processor.remove_whitespace(\"Hello There\")\n",
    "        'HelloThere'\n",
    "        >>> processor.remove_whitespace(\"HelloThere\")\n",
    "        'HelloThere'\n",
    "        \"\"\"\n",
    "        return \"\".join(name.split(\" \"))\n",
    "\n",
    "    def get_upper_case_indices(self, name: str) -> list:\n",
    "        \"\"\"\n",
    "        Gets the indices of all upper case words\n",
    "\n",
    "        Parameters:\n",
    "        * name - looks for uppercase \n",
    "        characters in this string\n",
    "\n",
    "        Returns:\n",
    "        A list of indices of the uppercase characters\n",
    "\n",
    "        Examples:\n",
    "        >>> processor = StringProcessor()\n",
    "        >>> processor.get_upper_case_indices(\"HelloThere\")\n",
    "        [0, 5]\n",
    "        >>> processor.get_upper_case_indices(\"HelloThereFriends\")\n",
    "        [0, 5, 10]\n",
    "        \"\"\"\n",
    "        upper_case_indices = []\n",
    "        for index, letter in enumerate(name):\n",
    "            if letter.isupper():\n",
    "                upper_case_indices.append(index)\n",
    "        return upper_case_indices\n",
    "\n",
    "    def get_lower_case_words(self, upper_case_indices: list, name: str) -> list:\n",
    "        \"\"\"\n",
    "        Gets a list of the words, in lower case, split on uppercase\n",
    "        characters\n",
    "\n",
    "        Parameters:\n",
    "        * upper_case_indices - a list of integers corresponding\n",
    "        to upper case letters in the string\n",
    "        * name - the string to split and process\n",
    "\n",
    "        Returns:\n",
    "        A list of words in lower case\n",
    "\n",
    "        Examples:\n",
    "        >>> processor = StringProcessor()\n",
    "        >>> processor.get_lower_case_words([0, 5], \"HelloThere\")\n",
    "        ['hello', 'there']\n",
    "        >>> processor.get_lower_case_words([0, 5, 10], \"HelloThereFriends\")\n",
    "        ['hello', 'there', 'friends']\n",
    "        \"\"\"\n",
    "        start = 0\n",
    "        lower_case_words = []\n",
    "        for index in upper_case_indices[1:]:\n",
    "            lower_case_words.append(\n",
    "                name[start:index].lower()\n",
    "            )\n",
    "            start = index\n",
    "        lower_case_words.append(\n",
    "            name[index:].lower()\n",
    "        )\n",
    "        return lower_case_words\n",
    "\n",
    "    def connect_words(self, lower_case_words: list) -> str:\n",
    "        \"\"\"\n",
    "        Connects a list of words via a '_'\n",
    "\n",
    "        Parameters:\n",
    "        * lower_case_words - a list of lower case words\n",
    "\n",
    "        Returns:\n",
    "        A string of concatenated words, with '_' between\n",
    "        each word.\n",
    "        \n",
    "        Examples:\n",
    "        >>> processor = StringProcessor()\n",
    "        >>> processor.connect_words(['hello', 'there'])\n",
    "        'hello_there'\n",
    "        >>> processor.connect_words(['hello', 'there', 'friends'])\n",
    "        'hello_there_friends'\n",
    "        \"\"\"\n",
    "        return \"_\".join(lower_case_words)\n",
    "\n",
    "    def to_snake_case(self, name: str) -> str:\n",
    "        \"\"\"\n",
    "        Takes a camel case string\n",
    "        and makes it snake case\n",
    "\n",
    "        Parameters:\n",
    "        - name - the string to translate\n",
    "\n",
    "        Returns:\n",
    "        The snake cased string\n",
    "\n",
    "        Example:\n",
    "        >>> processor = StringProcessor()\n",
    "        >>> processor.to_snake_case(\"HelloThere\")\n",
    "        'hello_there'\n",
    "        >>> processor.to_snake_case(\"hello_there\")\n",
    "        'hello_there'\n",
    "        >>> processor.to_snake_case(\"Hello There\")\n",
    "        'hello_there'\n",
    "        \"\"\"\n",
    "        name = self.remove_whitespace(name)\n",
    "        upper_case_indices = self.get_upper_case_indices(name)\n",
    "        if upper_case_indices == []:\n",
    "            return name\n",
    "        lower_case_words = self.get_lower_case_words(\n",
    "            upper_case_indices, name\n",
    "        )\n",
    "        return self.connect_words(lower_case_words)\n",
    "    \n",
    "    def split(self, name: str) -> list:\n",
    "        \"\"\"\n",
    "        Split words on either \"_\" or \" \" \n",
    "        if present in name.\n",
    "        \n",
    "        Parameters:\n",
    "        * name - the string to segment\n",
    "        \n",
    "        Returns:\n",
    "        A tokenized list of words, separated\n",
    "        by either \"_\" or whitespace\n",
    "        \n",
    "        Examples:\n",
    "        >>> processor = StringProcessor()\n",
    "        >>> processor.split(\"hello_there\")\n",
    "        ['hello', 'there']\n",
    "        >>> processor.split(\"hello there\")\n",
    "        ['hello', 'there']\n",
    "        >>> processor.split(\"hello there friends\")\n",
    "        ['hello', 'there', 'friends']\n",
    "        \"\"\"\n",
    "        if \"_\" in name:\n",
    "            return name.split(\"_\")\n",
    "        elif \" \" in name:\n",
    "            return name.split(\" \")\n",
    "        else:\n",
    "            return [name]\n",
    "        \n",
    "    def capitalize_words(self, words: list) -> list:\n",
    "        \"\"\"\n",
    "        Takes in a list of words (strings) and\n",
    "        capitalizes them.\n",
    "        \n",
    "        Parameters:\n",
    "        * words - a list of words to captialize\n",
    "        \n",
    "        Returns:\n",
    "        A list of words that are capitalized.\n",
    "        \n",
    "        Examples:\n",
    "        >>> processor = StringProcessor()\n",
    "        >>> processor.capitalize_words(['hello', 'there'])\n",
    "        ['Hello', 'There']\n",
    "        >>> processor.capitalize_words(['hello', 'there', 'friends'])\n",
    "        ['Hello', 'There', 'Friends']\n",
    "        \"\"\"\n",
    "        capitalized_words = []\n",
    "        for word in words:\n",
    "            if word:\n",
    "                capitalized_words.append(\n",
    "                    word.capitalize()\n",
    "                )\n",
    "        return capitalized_words\n",
    "    \n",
    "    def to_camel_case(self, name: str) -> str:\n",
    "        \"\"\"\n",
    "        Takes a string of words, either\n",
    "        separated by \"_\" or whitespace and\n",
    "        returns a camel cased string\n",
    "        \n",
    "        Parameters:\n",
    "        * name - the string to camel case\n",
    "        \n",
    "        Returns:\n",
    "        A camel cased string, with no whitespace\n",
    "        \n",
    "        Examples:\n",
    "        >>> processor = StringProcessor()\n",
    "        >>> processor.to_camel_case(\"hello there\")\n",
    "        'HelloThere'\n",
    "        >>> processor.to_camel_case('hello_there')\n",
    "        'HelloThere'\n",
    "        \"\"\"\n",
    "        words = self.split(name)\n",
    "        words = self.capitalize_words(words)\n",
    "        return \"\".join(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't much new here, but except a new method!  Now we can call `to_camel_case` in addition to all the other functions (now called methods) we had before.  The nice thing about this, is we were able to reuse one of the functions - `remove_whitespace` from one example to the other.  This is the power of objects - we can group related functionality together.  This way the reader can better understand generally what's going on.  And which methods are likely a good idea to call on related sets of objects.  Now let's take a closer look at documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation - How and Why?\n",
    "\n",
    "The why, of a function is usually answered by documentation.  Documentation tells the story of why the code does what it does, as well as providing a high level explaination of what the code is doing, this way, those who may not be familiar with the function understand it, by looking at it.  But more importantly, this way they don't have to look at the actual code in the function to understand it.\n",
    "\n",
    "You see Python comes with a very powerful built-in function, called `help`.  Let's look at an example right now!\n",
    "\n",
    "For this example we'll be making use of numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function mean in module numpy:\n",
      "\n",
      "mean(a, axis=None, dtype=None, out=None, keepdims=<no value>)\n",
      "    Compute the arithmetic mean along the specified axis.\n",
      "    \n",
      "    Returns the average of the array elements.  The average is taken over\n",
      "    the flattened array by default, otherwise over the specified axis.\n",
      "    `float64` intermediate and return values are used for integer inputs.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : array_like\n",
      "        Array containing numbers whose mean is desired. If `a` is not an\n",
      "        array, a conversion is attempted.\n",
      "    axis : None or int or tuple of ints, optional\n",
      "        Axis or axes along which the means are computed. The default is to\n",
      "        compute the mean of the flattened array.\n",
      "    \n",
      "        .. versionadded:: 1.7.0\n",
      "    \n",
      "        If this is a tuple of ints, a mean is performed over multiple axes,\n",
      "        instead of a single axis or all the axes as before.\n",
      "    dtype : data-type, optional\n",
      "        Type to use in computing the mean.  For integer inputs, the default\n",
      "        is `float64`; for floating point inputs, it is the same as the\n",
      "        input dtype.\n",
      "    out : ndarray, optional\n",
      "        Alternate output array in which to place the result.  The default\n",
      "        is ``None``; if provided, it must have the same shape as the\n",
      "        expected output, but the type will be cast if necessary.\n",
      "        See `doc.ufuncs` for details.\n",
      "    \n",
      "    keepdims : bool, optional\n",
      "        If this is set to True, the axes which are reduced are left\n",
      "        in the result as dimensions with size one. With this option,\n",
      "        the result will broadcast correctly against the input array.\n",
      "    \n",
      "        If the default value is passed, then `keepdims` will not be\n",
      "        passed through to the `mean` method of sub-classes of\n",
      "        `ndarray`, however any non-default value will be.  If the\n",
      "        sub-class' method does not implement `keepdims` any\n",
      "        exceptions will be raised.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    m : ndarray, see dtype parameter above\n",
      "        If `out=None`, returns a new array containing the mean values,\n",
      "        otherwise a reference to the output array is returned.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    average : Weighted average\n",
      "    std, var, nanmean, nanstd, nanvar\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The arithmetic mean is the sum of the elements along the axis divided\n",
      "    by the number of elements.\n",
      "    \n",
      "    Note that for floating-point input, the mean is computed using the\n",
      "    same precision the input has.  Depending on the input data, this can\n",
      "    cause the results to be inaccurate, especially for `float32` (see\n",
      "    example below).  Specifying a higher-precision accumulator using the\n",
      "    `dtype` keyword can alleviate this issue.\n",
      "    \n",
      "    By default, `float16` results are computed using `float32` intermediates\n",
      "    for extra precision.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> a = np.array([[1, 2], [3, 4]])\n",
      "    >>> np.mean(a)\n",
      "    2.5\n",
      "    >>> np.mean(a, axis=0)\n",
      "    array([ 2.,  3.])\n",
      "    >>> np.mean(a, axis=1)\n",
      "    array([ 1.5,  3.5])\n",
      "    \n",
      "    In single precision, `mean` can be inaccurate:\n",
      "    \n",
      "    >>> a = np.zeros((2, 512*512), dtype=np.float32)\n",
      "    >>> a[0, :] = 1.0\n",
      "    >>> a[1, :] = 0.1\n",
      "    >>> np.mean(a)\n",
      "    0.54999924\n",
      "    \n",
      "    Computing the mean in float64 is more accurate:\n",
      "    \n",
      "    >>> np.mean(a, dtype=np.float64)\n",
      "    0.55000000074505806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "help(np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this documentation provides the following things:\n",
    "* what the function does\n",
    "* the expected parameters\n",
    "* what the function returns\n",
    "* an example of using the function\n",
    "* nuance around the different function parameters\n",
    "* notes and extra context for the function, motivating it's use\n",
    "\n",
    "Every function that's written should be this well documented, and this clear.  \n",
    "\n",
    "## An Introduction To Test Driven Development\n",
    "\n",
    "The core of test driven development is notionally about how to write code is testable and about writing the tests themselves.  The ultimate goal of test driven development is to make sure as few bugs as possible make it into production code.  And that as few bugs as possible are introduced into an existing code base.\n",
    "\n",
    "In theory, this process cuts down development time, because far less time should be spent fixing bugs.  Also, notionally, if bugs are found right after they are written, they are more likely to be fixable easily, because less code relies on the bugs in question.\n",
    "\n",
    "How to write easily testable code:\n",
    "\n",
    "* write many functions, because it's easy to test individual functions \n",
    "    * this informs _why_ we want our functions to do one and only one thing. \n",
    "    * also why each line should do as little as possible.  \n",
    "    * As a result it's easy to test each function, because we only need to consider a few cases.  \n",
    "    * And if there is a bug, it will be easy to find, because we can likely isolate the line that causes the issue. \n",
    "    * If there is a lot of logic per line, it can be _very_ hard to find the bug, because we might need to parse it apart into multiple logical pieces, just to figure out what's wrong.\n",
    "\n",
    "* write unit tests\n",
    "    * unit tests test the individual functions (or methods) in the code to ensure they do what's expected\n",
    "    * usually you should only need between 1-4 tests to make sure a given function works\n",
    "\n",
    "* write integration tests\n",
    "    * integration tests combine multiple functions together, to make sure they all work in concert to produce a desired result\n",
    "    * usually you only need 1-2 integration tests per set of functions. \n",
    "    * it's typically a good idea for integration tests to combine transformations across multiple files, to make sure they all work together.  \n",
    "    * practically, this means, you'd like test multiple \"main\" functions that rely on one another with some dependency.\n",
    "    * in this way, we are able to ensure that our code dependencies don't introduce bugs.\n",
    "\n",
    "### A practical look at test driven development\n",
    "\n",
    "Now that we've discussed the motivation of testing, let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string_processor import StringProcessor\n",
    "\n",
    "def test_remove_whitespace():\n",
    "    processor = StringProcessor()\n",
    "    assert 'HelloThere' == processor.remove_whitespace(\"Hello There\")\n",
    "    assert 'HelloThere' ==processor.remove_whitespace(\"HelloThere\") \n",
    "\n",
    "def test_get_upper_case_indices():\n",
    "    processor = StringProcessor()\n",
    "    assert [0, 5] == processor.get_upper_case_indices(\"HelloThere\")\n",
    "    assert [0, 5, 10] == processor.get_upper_case_indices(\"HelloThereFriends\")\n",
    "\n",
    "def test_get_lower_case_words():\n",
    "    processor = StringProcessor()\n",
    "    assert ['hello', 'there'] == processor.get_lower_case_words([0, 5], \"HelloThere\")\n",
    "    assert ['hello', 'there', 'friends'] == processor.get_lower_case_words([0, 5, 10], \"HelloThereFriends\")\n",
    "\n",
    "def test_connect_words():\n",
    "    processor = StringProcessor()\n",
    "    assert 'hello_there' == processor.connect_words(['hello', 'there'])\n",
    "    assert 'hello_there_friends' == processor.connect_words(['hello', 'there', 'friends'])\n",
    "    \n",
    "def test_to_snake_case():\n",
    "    processor = StringProcessor()\n",
    "    assert 'hello_there' == processor.to_snake_case(\"HelloThere\")\n",
    "    assert 'hello_there' == processor.to_snake_case(\"hello_there\")\n",
    "    assert 'hello_there' == processor.to_snake_case(\"Hello There\")\n",
    "\n",
    "def test_split():\n",
    "    processor = StringProcessor()\n",
    "    assert ['hello', 'there'] == processor.split(\"hello_there\")\n",
    "    assert ['hello', 'there'] == processor.split(\"hello there\")\n",
    "    assert ['hello', 'there', 'friends'] == processor.split(\"hello there friends\")\n",
    "    \n",
    "def test_capitalize_words():\n",
    "    processor = StringProcessor()\n",
    "    assert ['Hello', 'There'] == processor.capitalize_words(['hello', 'there'])\n",
    "    assert ['Hello', 'There', 'Friends'] == processor.capitalize_words(['hello', 'there', 'friends'])\n",
    "    \n",
    "def test_to_camel_case():\n",
    "    processor = StringProcessor()\n",
    "    assert 'HelloThere' == processor.to_camel_case(\"hello there\")\n",
    "    assert 'HelloThere' == processor.to_camel_case('hello_there')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the test file for the `StringProcessor` object we made earlier.  Notice that the tests mirror the examples that we created in our docstrings.  And for good measure.  A well designed test is intended to be a representative example for the code to execute.  If the examples are well chosen, the tests should encapsulate the intended functionality of the code.  Additionally, tests are good place to look for hints for how the code should run, in absence of examples in your doc strings.\n",
    "\n",
    "We can run the tests with the following command:\n",
    "\n",
    "`pytest [Test file name]`\n",
    "\n",
    "In this example it would be:\n",
    "\n",
    "`pytest test_string_processor.py`\n",
    "\n",
    "And it should tell us whether the tests pass or not.\n",
    "\n",
    "Next let's talk about `mypy` tests.  The `mypy` static type checker adds the ability for you to add static types to your functions.  This way, you can ensure that your code is being used as expected, from your tests.  Additionally, this nice annotation makes it obvious what types to pass to each of your function parameters.  This addes readability essentially for free.\n",
    "\n",
    "Let's look at an example of a function annotated with `mypy` static types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def factorial(n: int) -> int:\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return factorial(n-1)*n\n",
    "    \n",
    "factorial(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this code tells us what type of parameter to pass to the code, and what the expected return type is.  If someone is interested in the type annotations of the function we can get this easily with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': int, 'return': int}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_annotations(func):\n",
    "    return func.__annotations__\n",
    "\n",
    "get_annotations(factorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general the definition for how to annotation functions looks like the following:\n",
    "\n",
    "```\n",
    "def func(arg: arg_type, optarg: arg_type = default) -> return_type:\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even test to see if our static typing works on our test file with the following:\n",
    "\n",
    "`mypy [test file]`\n",
    "\n",
    "let's look at our specific example:\n",
    "\n",
    "`mypy test_string_processor.py`\n",
    "\n",
    "Additionally we can run mypy on _any_ file, so we can even do this on the base file:\n",
    "\n",
    "`mypy string_processor.py`\n",
    "\n",
    "However in this case, nothing will get flagged because are just defining a class, but not using it.  It's best to call `mypy` anywhere that code is executed.\n",
    "\n",
    "Type annotation reference:\n",
    "\n",
    "* https://realpython.com/python-type-checking/\n",
    "\n",
    "## The Power Of Classes\n",
    "\n",
    "The next thing to discuss is the power of classes!\n",
    "\n",
    "We've already seen classes a little bit, but what we haven't done is talk through their real power:\n",
    "\n",
    "* inheritance\n",
    "* composition\n",
    "\n",
    "_def_ inheritance := Inheritance is the ability to inherit the methods and data of a parent class into a child class.  With inheritance, we get code reuse for very little.  \n",
    "\n",
    "Caution - there is a danger with doing more than two levels of inheritance!  If you do so, you can end up it's what called callback hell, where a bug introduced in a ancestor class propagates all the way down to the descendant class.  For this reason, no more than one level of inheritance is recommended, and no more than 2 levels should _ever_ be used.\n",
    "\n",
    "_def_ composition := Composition is the notion of instantiating a class object in another class, this way you can make use of it's methods and data, without having to expose it's methods publicly or rewrite them.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "### Time to Inherit Some Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StringTokenizer(StringProcessor):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def tokenize(self, name: str, split_on: str ='') -> list:\n",
    "        \"\"\"\n",
    "        'Tokenize' the string, meaning\n",
    "        return a list of semantically viable symbols\n",
    "        or words.  \n",
    "        \n",
    "        Parameters:\n",
    "        * name - the string to tokenize\n",
    "        \n",
    "        Returns:\n",
    "        A list of tokens (usually words) with all excess\n",
    "        white space removed\n",
    "        \n",
    "        Examples:\n",
    "        >>> string_tokenizer = StringTokenizer()\n",
    "        >>> string_tokenizer.tokenize(\"Hello There My Friends\")\n",
    "        ['Hello', 'There', 'My', 'Friends']\n",
    "        >>> string_tokenizer.tokenize(\"Hello-There-My-Friends\", split_on='-')\n",
    "        ['Hello', 'There', 'My', 'Friends']\n",
    "        >>> string_tokenizer.tokenize(\"Hello  There \\nMy \\tFriends\")\n",
    "        ['Hello', 'There', 'My', 'Friends']\n",
    "        >>> string_tokenizer.tokenize(\"Hello_There_My_Friends\", split_on='_')\n",
    "        ['Hello', 'There', 'My', 'Friends']\n",
    "        \"\"\"\n",
    "        tokens = self.split(name, split_on=split_on)\n",
    "        return [self.clean_endings(token) for token in tokens]\n",
    "    \n",
    "    def clean_endings(self, word: str) -> str:\n",
    "        \"\"\"\n",
    "        Strips the endings off of words or characters\n",
    "        \n",
    "        Parameters:\n",
    "        * name - the string to clean\n",
    "        \n",
    "        Returns:\n",
    "        A string without excess space\n",
    "        \n",
    "        Examples:\n",
    "        >>> string_tokenizer = StringTokenizer()\n",
    "        >>> string_tokenizer.clean_endings(\"  Hello \")\n",
    "        'Hello'\n",
    "        >>> string_tokenizer.clean_endings(\"  \\nHello\\t \")\n",
    "        'Hello'\n",
    "        \"\"\"\n",
    "        word = word.lstrip()\n",
    "        return word.rstrip()\n",
    "        \n",
    "    def split(self, name: str, split_on: str = '') -> list:\n",
    "        \"\"\"\n",
    "        Splits a string into a list based on some typical\n",
    "        cases.\n",
    "        \n",
    "        Parameters:\n",
    "        * name - the string to split\n",
    "        \n",
    "        Returns:\n",
    "        A list of strings, where each sub string\n",
    "        is between the split character found.\n",
    "        \n",
    "        Examples:\n",
    "        >>> string_tokenizer = StringTokenizer()\n",
    "        >>> string_tokenizer.split(\"Hello there friends\")\n",
    "        ['Hello', 'there', 'friends']\n",
    "        >>> string_tokenizer.split('Hello-there-friends', split_on='-')\n",
    "        ['Hello', 'there', 'friends']\n",
    "        \"\"\"\n",
    "        if split_on != '':\n",
    "            return name.split(split_on)\n",
    "        else:\n",
    "            return name.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see at the top of the class definition we do inheritance as follows:\n",
    "\n",
    "`[Class Name]([Parent Class Name]):`\n",
    "\n",
    "For our example thats:\n",
    "\n",
    "`class StringTokenizer(StringProcessor)`\n",
    "\n",
    "Now the `StringTokenizer` has all the methods of the `StringProcessor`!  \n",
    "\n",
    "Let's talk about the goal of this class, to make our tokenizing better.  \n",
    "\n",
    "First we'll need to overwrite the `split` method.  To do so we only need to name the new method the same thing as in the parent class.  \n",
    "\n",
    "Then we define 2 _totally_ new methods:\n",
    "\n",
    "* clean_endings\n",
    "* tokenize\n",
    "\n",
    "This highlights what inheritance is good for:\n",
    "\n",
    "* establishing namespaces, so method names can be reused in different contexts.  This is because sometimes naming things is hard.  By giving the extra context of an object name, you can use the same method name multiple times!\n",
    "* allowing for smaller classes overall - so you end up writing less code.  The less code you write, the less you have to maintain.\n",
    "\n",
    "### Time To Compose Some Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "class NLPEngine:\n",
    "    def __init__(self):\n",
    "        self.spacy_nlp_sm = spacy.load(\"en_core_web_sm\")\n",
    "        self.spacy_nlp_md = None\n",
    "        \n",
    "    def get_tokens(self, doc: str) -> list:\n",
    "        \"\"\"\n",
    "        Gets the tokens in the document\n",
    "        \n",
    "        Parameters:\n",
    "        * doc - a string to process\n",
    "        \n",
    "        Returns:\n",
    "        A list of tokens parsed from the text\n",
    "        \n",
    "        Examples:\n",
    "        >>> nlp = NLPEngine()\n",
    "        >>> nlp.get_tokens('Hello there friends')\n",
    "        ['Hello', 'there', 'friends']\n",
    "        \"\"\"\n",
    "        doc = self.spacy_nlp_sm(doc)\n",
    "        return [token.text for token in doc]\n",
    "    \n",
    "    def get_pos(self, doc: str) -> list:\n",
    "        \"\"\"\n",
    "        Gets the part of speech from \n",
    "        the tokens in the document.\n",
    "        \n",
    "        Parameters:\n",
    "        * doc - a string to process\n",
    "        \n",
    "        Returns:\n",
    "        A list of tuples of the form (token, part of speech)\n",
    "        \n",
    "        Examples:\n",
    "        >>> nlp = NLPEngine()\n",
    "        >>> nlp.get_pos('My dog spot runs fast!')\n",
    "        [('My', 'DET'), ('dog', 'NOUN'), ('spot', 'NOUN'), ('runs', 'VERB'), ('fast', 'ADV'), ('!', 'PUNCT')]\n",
    "        \"\"\"\n",
    "        doc = self.spacy_nlp_sm(doc)\n",
    "        return [(token.text, token.pos_) for token in doc]\n",
    "    \n",
    "    def get_token_shape(self, doc: str) -> list:\n",
    "        \"\"\"\n",
    "        Get the shape of all tokens in the document.\n",
    "        \n",
    "        The shape is the order of upper case and lower case\n",
    "        letters per word.\n",
    "        \n",
    "        Parameters:\n",
    "        * doc - the string to process\n",
    "        \n",
    "        Returns:\n",
    "        A list of tuples of the form (token, shape)\n",
    "        \n",
    "        Examples:\n",
    "        >>> nlp = NLPEngine()\n",
    "        >>> nlp.get_token_shape('My dog spot runs fast')\n",
    "        [('My', 'Xx'), ('dog', 'xxx'), ('spot', 'xxxx'), ('runs', 'xxxx'), ('fast', 'xxxx')]\n",
    "        \"\"\"\n",
    "        doc = self.spacy_nlp_sm(doc)\n",
    "        return [(token.text, token.shape_) for token in doc]\n",
    "    \n",
    "    def get_token_tags(self, doc: str) -> list:\n",
    "        \"\"\"\n",
    "        Get the tags for each token.\n",
    "        \n",
    "        Paramters:\n",
    "        * doc - the string to process\n",
    "        \n",
    "        Returns:\n",
    "        A list of tuples of the form (token, tag)\n",
    "        \n",
    "        Examples:\n",
    "        >>> nlp = NLPEngine()\n",
    "        >>> nlp.get_token_tags('My dog spot runs fast')\n",
    "        [('My', 'PRP$'), ('dog', 'NN'), ('spot', 'NN'), ('runs', 'VBZ'), ('fast', 'RB')]\n",
    "        \n",
    "        As you can see, this returns preposition, noun, noun, verb, adverb\n",
    "        \n",
    "        For a full list of tags see:\n",
    "        https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "        \"\"\"\n",
    "        doc = self.spacy_nlp_sm(doc)\n",
    "        return [(token.text, token.tag_) for token in doc]\n",
    "    \n",
    "    def get_token_lemmas(self, doc: str) -> list:\n",
    "        \"\"\"\n",
    "        Get the lemmas for each token.\n",
    "        A lemma is normalized version of the token\n",
    "        \n",
    "        Parameters:\n",
    "        * doc - the string to process\n",
    "        \n",
    "        Returns:\n",
    "        A list of tuples of the form (token, lemma)\n",
    "        \n",
    "        Examples:\n",
    "        >>> nlp = NLPEngine()\n",
    "        >>> nlp.get_token_lemmas(\"My dog spot runs very fast.  He is the fastest doggy in the world\")\n",
    "        [('My', '-PRON-'), ('dog', 'dog'), ('spot', 'spot'), ('runs', 'run'), \n",
    "        ('very', 'very'), ('fast', 'fast'), ('.', '.'), (' ', ' '), \n",
    "        ('He', '-PRON-'), ('is', 'be'), ('the', 'the'), ('fastest', 'fast'), \n",
    "        ('doggy', 'doggy'), ('in', 'in'), ('the', 'the'), ('world', 'world')]\n",
    "        \n",
    "        What I think is important is the difference between 'fastest' and 'fast' here.  It illustrates how\n",
    "        lemmas work - a lemma is sort of like the base word, and something like fastest is the word modified\n",
    "        to make sense grammatically.  Usually nlp systems don't care much about grammar (in some cases).  \n",
    "        So we can drop it.  So we can treat lemmatization as a sort of normalization of the word that throws\n",
    "        out grammatical transforms.\n",
    "        \"\"\"\n",
    "        doc = self.spacy_nlp_sm(doc)\n",
    "        return [(token.text, token.lemma_) for token in doc]\n",
    "    \n",
    "    def get_token_ner_labels(self, doc:str) -> list:\n",
    "        \"\"\"\n",
    "        Gets the named entity recognition labels for each token\n",
    "        \n",
    "        Parameters:\n",
    "        * doc - the string to process\n",
    "        \n",
    "        Returns:\n",
    "        A list of tuples of the form (token, ner_label)\n",
    "        \n",
    "        Examples:\n",
    "        >>> nlp = NLPEngine()\n",
    "        >>> nlp.get_token_ner_labels('I predict Google is going to buy Microsoft for one dollar')\n",
    "        [('Google', 'ORG'), ('Microsoft', 'ORG'), ('one dollar', 'MONEY')]\n",
    "        \n",
    "        As you can see, the ner tagger knows that google and microsoft are organizations\n",
    "        and that one dollar is money!\n",
    "        \"\"\"\n",
    "        doc = self.spacy_nlp_sm(doc)\n",
    "        return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "    def load_medium_language_model(self):\n",
    "        \"\"\"\n",
    "        loads the medium language model into the\n",
    "        spacy_nlp_md attribute which initially set to\n",
    "        None.\n",
    "        \n",
    "        Parameters:\n",
    "        * None\n",
    "        \n",
    "        Returns:\n",
    "        Nothing\n",
    "        \"\"\"\n",
    "        if not self.spacy_nlp_md:\n",
    "            self.spacy_nlp_md = spacy.load(\"en_core_web_md\")\n",
    "        \n",
    "    def how_similar(self, word_one: str, word_two: str) -> float:\n",
    "        \"\"\"\n",
    "        A measure of similarity for two word vectors.\n",
    "        The closer to 1.0 you get, the more similar\n",
    "        the two words are.\n",
    "        \n",
    "        Similarity is calculated via the L2 norm:\n",
    "        \n",
    "        import math\n",
    "        def L2_norm(first, second):\n",
    "            differences = [second[index] - first[index]\n",
    "                           for index in range(len(second))]\n",
    "            squared_difference = [math.pow(diff, 2) \n",
    "                                  for diff in differences]\n",
    "            sum_squared_difference = sum(squared_difference)\n",
    "            return math.sqrt(sum_squared_difference)\n",
    "        \n",
    "        Note: A word vector is a compact matrix \n",
    "        representation of a word.  It encodes\n",
    "        features about a word in an R^n space.\n",
    "        \n",
    "        Parameters:\n",
    "        * word_one - a word vector\n",
    "        * word_two - another word vector\n",
    "        \n",
    "        Returns:\n",
    "        The L2 normed distance between two word vectors\n",
    "        \n",
    "        Examples:\n",
    "        >>> nlp = NLPEngine()\n",
    "        >>> nlp.how_similar('hamburger', 'hotdog')\n",
    "        \"\"\"\n",
    "        self.load_medium_language_model()\n",
    "        word_one = self.spacy_nlp_md(word_one)\n",
    "        word_two = self.spacy_nlp_md(word_two)\n",
    "        return word_one.similarity(word_two)\n",
    "    \n",
    "    def get_string_mode(self, tokens: list) -> str:\n",
    "        \"\"\"\n",
    "        Gets the most frequently occurring string,\n",
    "        known as the mode.\n",
    "        \n",
    "        Paramters:\n",
    "        * tokens - a list of strings\n",
    "        \n",
    "        Returns:\n",
    "        The most frequently occurring string in the\n",
    "        list of strings.\n",
    "        \n",
    "        Examples:\n",
    "        >>> nlp = NLPEngine()\n",
    "        >>> nlp.get_string_mode([\"en\", \"en\", \"fr\"])\n",
    "        'en'\n",
    "        \"\"\"\n",
    "        unique_tokens = set(tokens)\n",
    "        token_count = {}\n",
    "        for token in unique_tokens:\n",
    "            token_count[token] = tokens.count(token)\n",
    "        return max(token_count)\n",
    "    \n",
    "    def language_detection(self, doc: str) -> str:\n",
    "        \"\"\"\n",
    "        Returns the most likely language based on\n",
    "        the language assigned to the most tokens.\n",
    "        \n",
    "        Because some words are defined across multiple\n",
    "        languages, often times in english, french, and\n",
    "        german as well as other languages with overlap,\n",
    "        words will be true for more than one language.\n",
    "        \n",
    "        Therefore we take the most seen language per token.\n",
    "        The likelihood of a tie should be very low, unless\n",
    "        a set of text is in multiple languages.  Then using\n",
    "        this method is inappropriate.\n",
    "        \n",
    "        Paramaters:\n",
    "        * doc - the text to process\n",
    "        \n",
    "        Returns:\n",
    "        The most likely language used in the text\n",
    "        \n",
    "        Examples:\n",
    "        >>> nlp = NLPEngine()\n",
    "        >>> nlp.language_detection(['Hello there friends'])\n",
    "        'en'\n",
    "        \"\"\"\n",
    "        doc = self.spacy_nlp_sm(doc)\n",
    "        langs = [token.lang_ for token in doc]\n",
    "        return self.get_string_mode(langs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key power of composition is the ability to include other classes in your current class.  Because you can only one level deep in your dependency structure, there is a guarantee that you will avoid callback hell, which is a very useful feature.  \n",
    "\n",
    "The reason we need composition here comes for two reasons:\n",
    "\n",
    "1. We don't want to reimplement all these methods, which are super useful\n",
    "2. We need more than one language model, but only sometimes!  The medium language model is very big, and takes a while to load.  So we shouldn't load it unless we need a task that's actually going to use it, like getting the similarity between 2 words.\n",
    "\n",
    "## Debugging Your Code!\n",
    "\n",
    "If possible, you should always use an IDE like PyCharm to debug your code.  VSCode is fine too.  But if you have some code on a server somewhere, or you really don't need all the extra setup, you can go the minimal route with the following methods:\n",
    "\n",
    "* code.interact()\n",
    "* IPython.embed()\n",
    "\n",
    "These more or less do the same thing, but call different REPLs.  Let's look at a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5524ab8d1acc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnumber_one\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber_two\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mbroken_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-5524ab8d1acc>\u001b[0m in \u001b[0;36mbroken_function\u001b[0;34m(number_one, number_two)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbroken_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_two\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnumber_two\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_two\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnumber_one\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber_two\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbroken_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "def clean_number(number):\n",
    "    return 0\n",
    "\n",
    "def broken_function(number_one, number_two):\n",
    "    number_two = clean_number(number_two)\n",
    "    return number_one / number_two\n",
    "\n",
    "broken_function(7, 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the error happens on line 6.  But does that mean that's where the bug is happening?  We should debug and explore to make sure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python 3.7.3 (v3.7.3:ef4ec6ed12, Mar 25 2019, 16:52:21) \n",
      "[Clang 6.0 (clang-600.0.57)] on darwin\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      "(InteractiveConsole)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In : dir()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__builtins__', 'number_one', 'number_two']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In : number_one\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In : number_two\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import code\n",
    "\n",
    "def clean_number(number):\n",
    "    return 0\n",
    "\n",
    "def broken_function(number_one, number_two):\n",
    "    number_two = clean_number(number_two)\n",
    "    code.interact(local=locals())\n",
    "    return number_one / number_two\n",
    "\n",
    "broken_function(7, 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah ha!  We found the problem!  It looks like `clean_number` is setting `number_two` to `0`!  So all we should do is get rid of that call and then we are good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3684210526315789"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import code\n",
    "\n",
    "def clean_number(number):\n",
    "    return 0\n",
    "\n",
    "def broken_function(number_one, number_two):\n",
    "    #number_two = clean_number(number_two)\n",
    "    #code.interact(local=locals())\n",
    "    return number_one / number_two\n",
    "\n",
    "broken_function(7, 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the right result!  \n",
    "\n",
    "Yes, this was a _very_ worked example.  But most debugging honestly comes down to hitting an error and then getting the context to figure out where the error is coming from.  Then try to step back till you find the source of the bug.  Then simply update the code to do what you expect.  \n",
    "\n",
    "So `code.interact` is pretty nifty - it launches a vanilla Python REPL.  And you can specify what scope you want.\n",
    "\n",
    "Here we asked for the local scope with `code.interact(local=locals())`.  This is great for figuring out what's happening inside a function or method.  We could have also asked for `code.interact(local=globals())`, which would have given us everything in global scope.  This is useful if we are working in global scope.  \n",
    "\n",
    "In my opinion, the REPL that comes with Python is usually good enough, but sometimes you need to debug something _complex_.  In that case, I'd recommend the ipython REPL, which is really an upgrade and has a lot of extra nice to haves.  \n",
    "\n",
    "Let's look at the syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.3 (v3.7.3:ef4ec6ed12, Mar 25 2019, 16:52:21) \n",
      "Type 'copyright', 'credits' or 'license' for more information\n",
      "IPython 7.6.0 -- An enhanced Interactive Python. Type '?' for help.\n",
      "\n",
      "In [1]: dir()\n",
      "Out[1]: \n",
      "['In',\n",
      " 'Out',\n",
      " '_dh',\n",
      " '_i',\n",
      " '_i1',\n",
      " '_ih',\n",
      " '_ii',\n",
      " '_iii',\n",
      " '_oh',\n",
      " 'exit',\n",
      " 'get_ipython',\n",
      " 'local_var_one',\n",
      " 'quit']\n",
      "\n",
      "In [2]: local_var_one\n",
      "Out[2]: 'Hello there'\n",
      "\n",
      "In [3]: whatever()\n",
      "Out[3]: 'hi'\n",
      "\n",
      "In [4]: exit()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "def whatever():\n",
    "    return \"hi\"\n",
    "\n",
    "def call_ipython_repl():\n",
    "    local_var_one = \"Hello there\"\n",
    "    IPython.embed()\n",
    "    \n",
    "call_ipython_repl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a PR\n",
    "\n",
    "There are a few simple rules that I like to follow for making PRs:\n",
    "\n",
    "1. Commit as little code as possible, per PR, typically one function is all you should commit - this puts less strain on the reviewer and allows them to understand what you are doing.  It also means, the reviewer is more likely to read every line and therefore catch any bugs the first time.  If you make a reviewer review a lot of code at once, it is far more likely they will miss errors or bugs.  The goal of a PR review is to catch bugs that a linter or tests may miss.\n",
    "\n",
    "2. Use conventional commits - a conventional commit is of the following form:\n",
    "\n",
    "`git commit -m \"\n",
    "type: feature\n",
    "files added: whatever.py\n",
    "description: adding whatever.py that implements skynet.  This code will kill all humans, have fun!\"\n",
    "`\n",
    "\n",
    "This is my convention for committing and has the following properties:\n",
    "\n",
    "* type - I tend to use the following types: feature, refactor\n",
    "* files added - what files were added with this commit, it's okay if the number is none, but specifying when you added a file is extremely helpful for rebasing and reflog.\n",
    "* description - what you changed and possibly why - giving motivation for the commit makes rebasing much, much easier.  It also means the reviewer has a high level understanding of what you did, and possibly why, before reading the code.\n",
    "\n",
    "Conventional commits are extremely helpful and add in the review process, greatly.  You don't need to use my conventions, but please use a convention that describes what you are doing.  It will save everyone time, and it makes reviews easier on the reader, meaning your code will get reviewed more often.\n",
    "\n",
    "Another reason conventional commits are so powerful is for `git bisect`.  This tool helps you track down bugs in the code base by looking at commit messages.  If your commit messages are clear, straight forward and descriptive, then it can be very, very easy to revert code to a working state before a bug was introduced.  Some bugs will get through, that's just the way code works.  But with `git bisect` and conventional commits, it's easy to tell where and when it happened.\n",
    "\n",
    "Intro to git bisect: https://www.metaltoad.com/blog/beginners-guide-git-bisect-process-elimination\n",
    "\n",
    "## Continuous Integration\n",
    "\n",
    "Continuous Integration or CI runs tests against the code, to ensure no code is committed to the master (or main branch) without passing all tests.  It also makes it easier on reviewers, because usually a CI pipeline will tell you what tests failed.  If a test fails then you will know immediately that the code isn't ready for production and then the writer can go fix what's broken.  No one writes perfect code, but with a CI pipeline, you can write code with as few bugs as possible.\n",
    "\n",
    "Let's look at an example CircleCI pipeline:\n",
    "\n",
    "\n",
    "```\n",
    "# Python CircleCI 2.0 configuration file\n",
    "#\n",
    "# Check https://circleci.com/docs/2.0/language-python/ for more details\n",
    "#\n",
    "version: 2\n",
    "jobs:\n",
    "  build:\n",
    "    docker:\n",
    "      # specify the version you desire here\n",
    "      # use `-browsers` prefix for selenium tests, e.g. `3.6.1-browsers`\n",
    "      - image: circleci/python:3.6.1\n",
    "\n",
    "    working_directory: ~/repo\n",
    "\n",
    "    steps:\n",
    "      - checkout\n",
    "      - run:\n",
    "          name: install dependencies\n",
    "          command: |\n",
    "            python3 -m venv venv\n",
    "            . venv/bin/activate\n",
    "            pip install -r requirements.txt\n",
    "\n",
    "      # run tests!\n",
    "      - run:\n",
    "          name: Install python\n",
    "          command: |\n",
    "            python3 -m venv venv\n",
    "            . venv/bin/activate\n",
    "            python -m pip install pytest --user\n",
    "      - run:\n",
    "          name: run tests\n",
    "          command: |\n",
    "            . venv/bin/activate\n",
    "            python -m pytest tests\n",
    "```\n",
    "\n",
    "This is a fairly basic CI pipeline it does the following things (in order):\n",
    "\n",
    "1. creates a docker image with python 3.6.1 installed - image: circleci/python:3.6.1\n",
    "2. installs dependencies\n",
    "3. installs pytest\n",
    "4. runs the tests\n",
    "\n",
    "All you need to do is put this file in a .circleci folder at the root directory of your github repo, then sign into circleci and add the project.  Then all the tests should run!\n",
    "\n",
    "## Continuous Deployment\n",
    "\n",
    "The idea behind continuous deployment is related, but separate from continuous integration.  In CD your goal is to package up your code, and send it somewhere, perhaps running as a web service, or to execute a job.  The important notion here is, reproducability.  By having a standard process for deploying your code (whatever that means), you can ensure that the way it was run will be the same every time.  This way if an error does occur during deployment, you'll be able to figure out what may have caused the error, because the steps for deployment can be manually reproduced.\n",
    "\n",
    "## CI & CD\n",
    "\n",
    "With CI & CD your code should be fully tested, fully reviewed and automatically deployed, enabling full automation.  This allows for a less error prone software development process.  Each step informs the next:\n",
    "\n",
    "clean code informs PR review and testing, PR review and testing informs CI, CI informs CD.  All these steps work in a chain, to ensure that code can be written quickly and correctly.  Which means fewer hassels, more features and clearer notions around how to run, and hopefully why to run a piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
