{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Classification\n",
    "\n",
    "In this chapter we'll look at a few examples of applying classification.  We'll start by looking at A/B testing like we did for applying hypothesis testing.  Then we'll move onto an example with customer churn where the data is unbalanced.  We'll end the chapter with an example of credit card fraud.  \n",
    "\n",
    "## A/B Testing\n",
    "\n",
    "Recall from the Applying Statistical Tests chapter we want to send an email about a towel sale.  We will have both a control and test group - one in which something about the email changed (test) and one in which the email stays the same as in the past (control).  We will use these two samples to set up an experiment.  Did changing the email effect things?\n",
    "\n",
    "Last time we answered this question with hypothesis testing.  Now we will answer it with a classifier!\n",
    "\n",
    "### Recall Set Up\n",
    "\n",
    "In order to test this question, we can set up an experiment.  Here we will set up a randomized test group and a randomized control group.  \n",
    "\n",
    "The test group will be sent an email, with slightly different copy, or possibly with a picture.  Some specific change will be made, in any event.\n",
    "\n",
    "The control group will get the same email as last time.  This way, we can directly compare, as much as possible between the old email and the new one.  There are many things you typically need to control for, or account for in experimental design.  Some things to account for in this scenario are:\n",
    "\n",
    "1) Age\n",
    "\n",
    "2) Gender\n",
    "\n",
    "3) Location\n",
    "\n",
    "4) Time of Day\n",
    "\n",
    "5) Time of Year\n",
    "\n",
    "6) Approximate Disposable Income\n",
    "\n",
    "\n",
    "## Simulating Some Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    2822\n",
      "0    2178\n",
      "Name: converted, dtype: int64\n",
      "0    4393\n",
      "1     607\n",
      "Name: converted, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_data(df, column, choices, size):\n",
    "    \"\"\"\n",
    "    Generates categorical data given choices.\n",
    "    \n",
    "    Parameters:\n",
    "    * df - pd.DataFrame: the data to add a column to\n",
    "    * column - str: the column to generate\n",
    "    * choices - list: the list of possible choices\n",
    "    \n",
    "    Returns:\n",
    "    A dataframe with the newly generated column.\n",
    "    \"\"\"\n",
    "    df[column] = [random.choice(choices)\n",
    "                  for _ in range(size)]\n",
    "    df = pd.concat([df, pd.get_dummies(df[column])], axis=1)\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df\n",
    "\n",
    "def converted_score(x):\n",
    "    if x[\"male\"] == 1:\n",
    "        gender = 0.7\n",
    "    elif x[\"female\"] == 1:\n",
    "        gender = 1.4\n",
    "    if x[\"white\"] == 1:\n",
    "        race = 0.5\n",
    "    elif x[\"black\"] == 1:\n",
    "        race = 1.4\n",
    "    elif x[\"asian\"] == 1:\n",
    "        race = 2.8\n",
    "    elif x[\"hispanic\"] == 1:\n",
    "        race = 3.7\n",
    "    salary_alpha = gender * race\n",
    "    age_alpha = gender + race\n",
    "    return salary_alpha * x[\"salary\"] + age_alpha * x[\"age\"]\n",
    "\n",
    "def decision_boundary(result):\n",
    "    if result > 250000:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "size = 5000\n",
    "test_df = pd.DataFrame()\n",
    "control_df = pd.DataFrame()\n",
    "gender_choices = [\"male\", \"female\"]\n",
    "race_choices = [\"white\", \"black\", \"asian\", \"hispanic\"]\n",
    "test_salary_mean = 150000\n",
    "test_salary_variance = 30000 \n",
    "control_salary_mean = 55000\n",
    "control_salary_variance = 2000\n",
    "\n",
    "test_df = generate_data(test_df, \"gender\", gender_choices, size)\n",
    "test_df = generate_data(test_df, \"race\", race_choices, size)\n",
    "test_df[\"age\"] = np.random.normal(50, 25, size=len(test_df))\n",
    "test_df[\"age\"] = test_df[\"age\"].astype(int)\n",
    "test_df[\"salary\"] = np.random.normal(test_salary_mean, \n",
    "                                     test_salary_variance, \n",
    "                                     size=len(test_df))\n",
    "test_df[\"salary\"] = test_df[\"salary\"].apply(lambda x: round(x, 2))\n",
    "\n",
    "test_df[\"converted\"] = test_df.apply(converted_score, axis=1)\n",
    "test_df[\"converted\"] = test_df[\"converted\"].apply(decision_boundary)\n",
    "\n",
    "control_df = generate_data(control_df, \"gender\", gender_choices, size)\n",
    "control_df = generate_data(control_df, \"race\", race_choices, size)\n",
    "control_df[\"age\"] = np.random.normal(50, 25, size=len(control_df))\n",
    "control_df[\"age\"] = control_df[\"age\"].astype(int)\n",
    "control_df[\"salary\"] = np.random.normal(control_salary_mean, \n",
    "                                        control_salary_variance, \n",
    "                                        size=len(control_df))\n",
    "control_df[\"salary\"] = control_df[\"salary\"].apply(lambda x: round(x, 2))\n",
    "control_df[\"converted\"] = control_df.apply(converted_score, axis=1)\n",
    "control_df[\"converted\"] = control_df[\"converted\"].apply(decision_boundary)\n",
    "\n",
    "print(test_df[\"converted\"].value_counts())\n",
    "print(control_df[\"converted\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's where things get different! The next step is now to model our test and control and see if the probability of conversion is higher or lower for our test and control sets.  If they are the same or similar then our change likely had little effect.  Of course you should verify this with multiple tests as well as cross validation if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97       562\n",
      "           1       0.96      1.00      0.98       688\n",
      "\n",
      "    accuracy                           0.98      1250\n",
      "   macro avg       0.98      0.97      0.98      1250\n",
      "weighted avg       0.98      0.98      0.98      1250\n",
      "\n",
      "ROC AUC 0.9733096085409252\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "augmented_df = test_df.copy()\n",
    "augmented_y = augmented_df[\"converted\"]\n",
    "cols = augmented_df.columns.tolist()\n",
    "cols.remove(\"converted\")\n",
    "augmented_X = augmented_df[cols]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    augmented_X, augmented_y\n",
    ")\n",
    "test_clf = LogisticRegression(\n",
    "    max_iter=1000, class_weight=\"balanced\", \n",
    "    C=100, penalty=\"l2\", solver=\"liblinear\"\n",
    ")\n",
    "test_clf.fit(X_train, y_train)\n",
    "y_pred = test_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC\", roc_auc_score(y_test, y_pred))\n",
    "plt.hist(clf.predict_proba(X_test).T[1])\n",
    "plt.title('Did Convert Probability Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1108\n",
      "           1       1.00      1.00      1.00       142\n",
      "\n",
      "    accuracy                           1.00      1250\n",
      "   macro avg       1.00      1.00      1.00      1250\n",
      "weighted avg       1.00      1.00      1.00      1250\n",
      "\n",
      "ROC AUC 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "control_y = control_df[\"converted\"]\n",
    "cols = control_df.columns.tolist()\n",
    "cols.remove(\"converted\")\n",
    "control_X = control_df[cols]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    control_X, control_y\n",
    ")\n",
    "control_clf = LogisticRegression(\n",
    "    max_iter=1000, class_weight=\"balanced\", \n",
    "    C=100, penalty=\"l2\", solver=\"liblinear\"\n",
    ")\n",
    "control_clf.fit(X_train, y_train)\n",
    "y_pred = control_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC\", roc_auc_score(y_test, y_pred))\n",
    "plt.hist(clf.predict_proba(X_test).T[1])\n",
    "plt.title('Did Convert Probability Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "So we record all of the standard metrics to make sure our model explains what we are seeing well.  Then we look at the probability distributions for convert (y=1) and not convert (y=0).  If\n",
    "\n",
    "$$ P(converted | y=1) \\nsim P(converted | y=0)$$ \n",
    "\n",
    "then we can say our trial produced a meaningful result and we can say that our test data indeed changed something.\n",
    "\n",
    "Note: this is a worked example!  If this was the real world and you saw those accuracy measures, you ought to be very, very skeptical.  Especially the second set.\n",
    "\n",
    "Let's look at the specifics of our example:\n",
    "\n",
    "For test:\n",
    "\n",
    "* about 500 converted with a high probability and 450 converted with a very low probability.\n",
    "\n",
    "For control:\n",
    "\n",
    "* about 1200 people did not convert with a high probability and around 200 converted with a high probability.\n",
    "\n",
    "Clearly the test group had a different experience!  But why?  Let's dig further into the data by looking at `test_df` and `control_df`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>asian</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>white</th>\n",
       "      <th>age</th>\n",
       "      <th>salary</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.503600</td>\n",
       "      <td>0.496400</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>0.258800</td>\n",
       "      <td>0.247800</td>\n",
       "      <td>0.242600</td>\n",
       "      <td>50.339400</td>\n",
       "      <td>150040.746312</td>\n",
       "      <td>0.564400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500037</td>\n",
       "      <td>0.500037</td>\n",
       "      <td>0.433517</td>\n",
       "      <td>0.438019</td>\n",
       "      <td>0.431778</td>\n",
       "      <td>0.428698</td>\n",
       "      <td>24.645724</td>\n",
       "      <td>30019.400205</td>\n",
       "      <td>0.495885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-37.000000</td>\n",
       "      <td>41819.190000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>130438.922500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>150082.165000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>170778.290000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>256016.380000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            female         male        asian        black     hispanic  \\\n",
       "count  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000   \n",
       "mean      0.503600     0.496400     0.250800     0.258800     0.247800   \n",
       "std       0.500037     0.500037     0.433517     0.438019     0.431778   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     1.000000     1.000000     1.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "             white          age         salary    converted  \n",
       "count  5000.000000  5000.000000    5000.000000  5000.000000  \n",
       "mean      0.242600    50.339400  150040.746312     0.564400  \n",
       "std       0.428698    24.645724   30019.400205     0.495885  \n",
       "min       0.000000   -37.000000   41819.190000     0.000000  \n",
       "25%       0.000000    34.000000  130438.922500     0.000000  \n",
       "50%       0.000000    50.000000  150082.165000     1.000000  \n",
       "75%       0.000000    67.000000  170778.290000     1.000000  \n",
       "max       1.000000   143.000000  256016.380000     1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>asian</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>white</th>\n",
       "      <th>age</th>\n",
       "      <th>salary</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.504400</td>\n",
       "      <td>0.495600</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.243800</td>\n",
       "      <td>0.243200</td>\n",
       "      <td>0.248400</td>\n",
       "      <td>49.159800</td>\n",
       "      <td>54987.455054</td>\n",
       "      <td>0.121400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500031</td>\n",
       "      <td>0.500031</td>\n",
       "      <td>0.441164</td>\n",
       "      <td>0.429416</td>\n",
       "      <td>0.429058</td>\n",
       "      <td>0.432128</td>\n",
       "      <td>24.594407</td>\n",
       "      <td>1986.156938</td>\n",
       "      <td>0.326624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-40.000000</td>\n",
       "      <td>48168.270000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>53631.270000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>55009.735000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>56297.607500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>63514.950000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            female         male        asian        black     hispanic  \\\n",
       "count  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000   \n",
       "mean      0.504400     0.495600     0.264600     0.243800     0.243200   \n",
       "std       0.500031     0.500031     0.441164     0.429416     0.429058   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     1.000000     1.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "             white          age        salary    converted  \n",
       "count  5000.000000  5000.000000   5000.000000  5000.000000  \n",
       "mean      0.248400    49.159800  54987.455054     0.121400  \n",
       "std       0.432128    24.594407   1986.156938     0.326624  \n",
       "min       0.000000   -40.000000  48168.270000     0.000000  \n",
       "25%       0.000000    32.000000  53631.270000     0.000000  \n",
       "50%       0.000000    49.000000  55009.735000     0.000000  \n",
       "75%       0.000000    66.000000  56297.607500     0.000000  \n",
       "max       1.000000   145.000000  63514.950000     1.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a huge difference in salary!  Let's look at how big the difference is on average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95053.29125800001"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"salary\"].mean() - control_df[\"salary\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 95K!  That's a huge difference in standard of living.  Let's go back to our model and look at our coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female 0.9292687935837641\n",
      "male -4.103078995398313\n",
      "asian 2.6485300549834054\n",
      "black -2.6993728240235115\n",
      "hispanic 4.453509716475326\n",
      "white -7.576477149249771\n",
      "age -0.0009181427593822856\n",
      "salary 4.08325526503341e-05\n"
     ]
    }
   ],
   "source": [
    "for index, coef in enumerate(test_clf.coef_[0]):\n",
    "    print(test_df.columns[index], coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female 6.9140512773516924\n",
      "male -7.395307616549721\n",
      "asian -3.8570837201125263\n",
      "black -3.806284792620383\n",
      "hispanic 11.029417213298009\n",
      "white -3.847305039763138\n",
      "age -0.0029246918380587596\n",
      "salary -0.00018044270948364995\n"
     ]
    }
   ],
   "source": [
    "for index, coef in enumerate(control_clf.coef_[0]):\n",
    "    print(control_df.columns[index], coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big thing to pay attention to here is effect size - how the size of the variable and it's weight changes the decision or result of the model.\n",
    "\n",
    "The effect size of salary may _seem_ small based on these coefficients, but I assure you it's not.  Recall, that for logistic regression the equation is:\n",
    "\n",
    "$$ \\theta_{i}x_{i} $$\n",
    "\n",
    "That means we need to consider the _multiplication_ of these two variables.  Let's just look at the average salary for the two groups to get a sense of effect size:\n",
    "\n",
    "For test:\n",
    "\n",
    "$$ \\theta * mean(salary) = 6.126546673480162 $$\n",
    "\n",
    "That is, `4.08325526503341e-05 * 150040.746312 = 6.126546673480162`.\n",
    "\n",
    "Given that effect size is exponentiated by the base, in this case $e$, the natural number.  We have, an increase in salary by 1 dollar leading to an increase in P(Y=1) of:\n",
    "\n",
    "$$ e^{6.126546673480162} = 457.85231394605876 $$\n",
    "\n",
    "Which is _a lot_.  Especially given the effect size of the other coefficients.\n",
    "\n",
    "I'll leave as an exercise carrying this out for the other model.  But let's just say the effect size is similar.\n",
    "\n",
    "What that implies should be obvious, how much you make matters a ton!  So can we tell if the change actually meant anything?  Nope!  We have no idea.  We should construct this problem again, controlling explicitly for income in order to get a fair test.\n",
    "\n",
    "## Example Two - Customer Churn\n",
    "\n",
    "The next typical data science problem we are going to tackle is customer churn!  This is huge for product development, sales cycles and just keeping a business afloat.  If you know and can predict how much your customers are going to churn you can reliably forecast how much revenue to expect per quarter.  Which is basically essential to any and all businesses.  \n",
    "\n",
    "Since customer churn is so important, it's worth noting that there is more to churn than just what's in your model.  It can help drive decisions, but it's very important to include domain experts in churn conversations.  This means designers, UX and design folks, sales folks, executives and other stakeholders.  All of these folks matter.  For one, your model may not take into account critical variables.  For another thing, you may not be measuring enough.  A good model doesn't replace people, it helps inform a conversation and aids in decision making.\n",
    "\n",
    "Let's start with some context for what Customer Churn is and go through some possible definitions:\n",
    "\n",
    "Customer churn, loosely, is the number of customers that will stop paying for your service over a specified period of time.\n",
    "\n",
    "Definition one:\n",
    "\n",
    "$$ \\frac{customers \\space lost \\space during \\space fixed \\space period}{total \\space customers \\space at \\space the \\space start \\space of \\space the \\space fixed \\space period} $$\n",
    "\n",
    "Notice, this does not take into account the total number of customers gained.  Say for instance you started out with 100 customers, then you gained 1 million over the course of the fixed period, say a month, and then 15 thousand churned.  Your churn for the month would be:\n",
    "\n",
    "$$ \\frac{15000}{100} = 150 \\% $$\n",
    "\n",
    "If 150% of your customers churn, you might think your business is _not_ doing great.  But you gained like a million new customers over the period!  So in actuality, this is very good news, _overall_.  If you don't get a ton of new customers, this might be a good enough metric.\n",
    "\n",
    "Definition two:\n",
    "\n",
    "$$ \\frac{Churn}{Customers_{1} + Customers_{n} / 2} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "$ Customer_{1} $ := number of customers at the start of the month\n",
    "\n",
    "$ Customer_{n} $ := number of customers at the end of the month\n",
    "\n",
    "So over the same window we get:\n",
    "\n",
    "$$ \\frac{15000}{(100 + 1000000)/2} = 0.029\\% $$\n",
    "\n",
    "Which sounds much more reasonable, and accurate.\n",
    "\n",
    "Definition three:\n",
    "\n",
    "$$ \\frac{Churn}{\\frac{1}{n} \\displaystyle \\sum_{i=1}^{n} Customer_{i}} $$\n",
    "\n",
    "Here we take the average of the customer count over the period of interest, this further normalizes the churn.\n",
    "\n",
    "## Modeling Churn\n",
    "\n",
    "Once you have a good measure of Churn, that works for you, the next step is to understand your Churn number.  For this we'll create a model of the world that incorpates other data to understand when and more importantly why customers churn.\n",
    "\n",
    "For this example we will be making use of this dataset from kaggle:\n",
    "\n",
    "https://www.kaggle.com/adammaus/predicting-churn-for-bank-customers\n",
    "\n",
    "To get this part of the notebook to run, you'll need to download and unzip the data locally (a pain I know).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 12 columns):\n",
      "Surname            10000 non-null object\n",
      "CreditScore        10000 non-null int64\n",
      "Geography          10000 non-null object\n",
      "Gender             10000 non-null object\n",
      "Age                10000 non-null int64\n",
      "Tenure             10000 non-null int64\n",
      "Balance            10000 non-null float64\n",
      "NumOfProducts      10000 non-null int64\n",
      "HasCrCard          10000 non-null int64\n",
      "IsActiveMember     10000 non-null int64\n",
      "EstimatedSalary    10000 non-null float64\n",
      "Churn              10000 non-null int64\n",
      "dtypes: float64(2), int64(7), object(3)\n",
      "memory usage: 937.6+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"Churn_Modelling.csv\")\n",
    "df.drop(\"RowNumber\", inplace=True, axis=1)\n",
    "df.drop(\"CustomerId\", inplace=True, axis=1)\n",
    "df[\"Churn\"] = df[\"Exited\"] \n",
    "df.drop(\"Exited\", inplace=True, axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the `RowNumber` and the `CustomerId` since they won't be useful for understanding why our customers churn or stay.  Some of the other information may be useful.  First let's look at `Gender`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Churn %</th>\n",
       "      <th>Churn Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>0.250715</td>\n",
       "      <td>1139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>0.164559</td>\n",
       "      <td>898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Churn %  Churn Total\n",
       "Gender                       \n",
       "Female  0.250715         1139\n",
       "Male    0.164559          898"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = df.pivot_table(values=\"Churn\", index='Gender', aggfunc=np.mean)\n",
    "summary[\"Churn %\"] = summary[\"Churn\"] \n",
    "summary[\"Churn Total\"] = df.pivot_table(values=\"Churn\", index=\"Gender\", aggfunc=np.sum)\n",
    "summary.drop(\"Churn\", axis=1, inplace=True)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems pretty clear that `Gender` is going to matter, and that `Churn` is going to be higher for women than men.\n",
    "\n",
    "Now let's see if this is True for all countries under consideration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France' 'Spain' 'Germany']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Geography</th>\n",
       "      <th>France</th>\n",
       "      <th>Germany</th>\n",
       "      <th>Spain</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>0.203450</td>\n",
       "      <td>0.375524</td>\n",
       "      <td>0.212121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>0.127134</td>\n",
       "      <td>0.278116</td>\n",
       "      <td>0.131124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Geography    France   Germany     Spain\n",
       "Gender                                 \n",
       "Female     0.203450  0.375524  0.212121\n",
       "Male       0.127134  0.278116  0.131124"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df[\"Geography\"].unique())\n",
    "summary = df.pivot_table(values=\"Churn\", index=['Gender'], columns=[\"Geography\"], aggfunc=np.mean)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears churn rates are higher across the board for women over men, however churn rates do vary from country to country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France' 'Spain' 'Germany']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Geography</th>\n",
       "      <th>France</th>\n",
       "      <th>Germany</th>\n",
       "      <th>Spain</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>460</td>\n",
       "      <td>448</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>350</td>\n",
       "      <td>366</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Geography  France  Germany  Spain\n",
       "Gender                           \n",
       "Female        460      448    231\n",
       "Male          350      366    182"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df[\"Geography\"].unique())\n",
    "summary = df.pivot_table(values=\"Churn\", index=['Gender'], columns=[\"Geography\"], aggfunc=np.sum)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is probably a function of population size.  However this means explicitly controlling for `Geography` is probably important, because otherwise we may lose a confounding effect, which would lower the generalizability of our analysis.\n",
    "\n",
    "Next let's look at the effect of age on Churn.  For this we should first run the test for independence followed by a test for correlation.  Recall, we will use Kruskal-Wallis for independence and point bi serial correlation for correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KruskalResult(statistic=16030.329492796818, pvalue=0.0)\n",
      "PointbiserialrResult(correlation=0.28532303783506824, pvalue=1.2399313093495365e-186)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(stats.kruskal(df[\"Age\"], df[\"Churn\"]))\n",
    "print(stats.pointbiserialr(df[\"Age\"], df[\"Churn\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a pvalue of zero we reject the null hypothesis that the two variables are independent.  Additionally, we see a pvalue of close to zero for point bi serial correlation, therefore we reject the null hypothesis of no correlation.  So `Age` is a variable of interest.  We can also confirm this with mutual information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07710633])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import feature_selection\n",
    "feature_selection.mutual_info_regression(df[\"Age\"].values.reshape(-1, 1), df[\"Churn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't read An Introduction to Information Theory yet, the important things to note are when mutual information is 1, the variables are perfectly dependent.  When it's zero they are the same.  So there is some weak information sharing between `Age` and `Churn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually go through the rest of the variables to see which ones are likely useful for predicting churn with mutual information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Age', 0.09048326288892028),\n",
       " ('NumOfProducts', 0.07303749592710229),\n",
       " ('IsActiveMember', 0.013130240845428354),\n",
       " ('Tenure', 0.00409941408302128),\n",
       " ('EstimatedSalary', 0.0027089277988778804),\n",
       " ('Balance', 0.00161677556992057),\n",
       " ('CreditScore', 0.0006704402476174209),\n",
       " ('HasCrCard', 0.0)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [column for column in df.columns if df[column].dtype != \"object\"]\n",
    "features.remove(\"Churn\")\n",
    "X = df[features]\n",
    "y = df[\"Churn\"]\n",
    "ranks = feature_selection.mutual_info_regression(X, y)\n",
    "rankings = []\n",
    "for index, feature in enumerate(features):\n",
    "    rankings.append((feature, ranks[index]))\n",
    "sorted(rankings, key=lambda t: t[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that `NumOfProducts` is the most important and `HasCrCard` is the least important.  Or rather carries the least information about probability of churning.  This is possibly because most people have a credit card these days.\n",
    "\n",
    "Next let's kick out any variables that don't likely matter.  In this case, we can probably safely remove `HasCrCard`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"HasCrCard\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also drop `Surname`, since any information it gives us is probably spurious:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"Surname\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, let's look at how balanced our classes are.  This will inform what type of classifier we use and what kind of accuracy we can expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7963\n",
       "1    2037\n",
       "Name: Churn, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Churn\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there is some imbalance with about 80% of people not churning from the service.  So we'll need to explicitly account for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are almost ready to fit our model!  Let's first get our categorical variables ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    if df[column].dtype == \"object\":\n",
    "        df = pd.concat([df, pd.get_dummies(df[column])], axis=1)\n",
    "        df = df.drop(column, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.65      0.74      1944\n",
      "           1       0.35      0.66      0.46       556\n",
      "\n",
      "    accuracy                           0.65      2500\n",
      "   macro avg       0.61      0.66      0.60      2500\n",
      "weighted avg       0.76      0.65      0.68      2500\n",
      "\n",
      "0.6562953341030878\n",
      "11.978264824417066\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, log_loss\n",
    "\n",
    "y = df[\"Churn\"]\n",
    "cols = df.columns.tolist()\n",
    "cols.remove(\"Churn\")\n",
    "X = df[cols]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=12)\n",
    "logit = LogisticRegression(\n",
    "    C=1, penalty=\"l2\", max_iter=1000, \n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "logit.fit(X_train, y_train)\n",
    "y_pred = logit.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "print(log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One strategy to deal with class imbalance is by making that explicit.  Here we the `class_weight` hyperparameter to try and adjust for this class imbalance.  What this does is weight examples from the minority class higher than those from the majority class.  Unfortunately our model doesn't do great, so let's see if there are any obvious variables we can remove:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CreditScore -0.0031436478957918785\n",
      "Age 0.05898746159233117\n",
      "Tenure -0.08679665322930301\n",
      "Balance 3.905155260992167e-06\n",
      "NumOfProducts -0.039819154004494525\n",
      "IsActiveMember -0.07847513122067634\n",
      "EstimatedSalary -9.223377167585139e-07\n",
      "France -0.04440259570810289\n",
      "Germany 0.05012964004326264\n",
      "Spain -0.022347366299728076\n",
      "Female 0.04293418313482472\n",
      "Male -0.059554505098413556\n"
     ]
    }
   ],
   "source": [
    "for index, col in enumerate(cols):\n",
    "    print(col, logit.coef_[0][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the variables really jump out as the issue.  You might think that `EstimatedSalary` and `Balance` are the issue, but both of them have much larger values than the rest of the variables.  Let's just confirm that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>650.528800</td>\n",
       "      <td>38.921800</td>\n",
       "      <td>5.012800</td>\n",
       "      <td>76485.889288</td>\n",
       "      <td>1.530200</td>\n",
       "      <td>0.515100</td>\n",
       "      <td>100090.239881</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>96.653299</td>\n",
       "      <td>10.487806</td>\n",
       "      <td>2.892174</td>\n",
       "      <td>62397.405202</td>\n",
       "      <td>0.581654</td>\n",
       "      <td>0.499797</td>\n",
       "      <td>57510.492818</td>\n",
       "      <td>0.402769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>350.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.580000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>584.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51002.110000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>652.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>97198.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100193.915000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>718.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>127644.240000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>149388.247500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>850.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>250898.090000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>199992.480000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CreditScore           Age        Tenure        Balance  NumOfProducts  \\\n",
       "count  10000.000000  10000.000000  10000.000000   10000.000000   10000.000000   \n",
       "mean     650.528800     38.921800      5.012800   76485.889288       1.530200   \n",
       "std       96.653299     10.487806      2.892174   62397.405202       0.581654   \n",
       "min      350.000000     18.000000      0.000000       0.000000       1.000000   \n",
       "25%      584.000000     32.000000      3.000000       0.000000       1.000000   \n",
       "50%      652.000000     37.000000      5.000000   97198.540000       1.000000   \n",
       "75%      718.000000     44.000000      7.000000  127644.240000       2.000000   \n",
       "max      850.000000     92.000000     10.000000  250898.090000       4.000000   \n",
       "\n",
       "       IsActiveMember  EstimatedSalary         Churn  \n",
       "count    10000.000000     10000.000000  10000.000000  \n",
       "mean         0.515100    100090.239881      0.203700  \n",
       "std          0.499797     57510.492818      0.402769  \n",
       "min          0.000000        11.580000      0.000000  \n",
       "25%          0.000000     51002.110000      0.000000  \n",
       "50%          1.000000    100193.915000      0.000000  \n",
       "75%          1.000000    149388.247500      0.000000  \n",
       "max          1.000000    199992.480000      1.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the averages for `EstimatedSalary` and `Balance` are much much higher than the other variables.  So even though they have smaller coefficients, their effect size will be large enough to contribute to the decision boundary.  Next, let's see if the solver or penalty is the issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.72      0.80      1944\n",
      "           1       0.42      0.70      0.53       556\n",
      "\n",
      "    accuracy                           0.72      2500\n",
      "   macro avg       0.66      0.71      0.66      2500\n",
      "weighted avg       0.79      0.72      0.74      2500\n",
      "\n",
      "0.7127298161470823\n",
      "9.740107656409736\n"
     ]
    }
   ],
   "source": [
    "logit_linear = LogisticRegression(\n",
    "    solver=\"liblinear\",\n",
    "    C=1, penalty=\"l1\", max_iter=10000, \n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "logit_linear.fit(X_train, y_train)\n",
    "y_pred = logit_linear.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "print(log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CreditScore -0.0006180222871635596\n",
      "Age 0.0782462365221455\n",
      "Tenure -0.006832107433490348\n",
      "Balance 2.6100726865106033e-06\n",
      "NumOfProducts -0.125957377561036\n",
      "IsActiveMember -0.8492739668684903\n",
      "EstimatedSalary 1.2544822574956195e-07\n",
      "France -0.807417662931865\n",
      "Germany 0.0\n",
      "Spain -0.8244401538543713\n",
      "Female -0.34300710576221705\n",
      "Male -0.9037524239656831\n"
     ]
    }
   ],
   "source": [
    "for index, col in enumerate(cols):\n",
    "    print(col, logit_linear.coef_[0][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a little bit better with this model, but still not well enough.  Time to bring in another library specifically created to deal with imbalanced classes.\n",
    "\n",
    "## Dealing with imbalanced data explicitly\n",
    "\n",
    "There are a few strategies for dealing with imbalanced data.  We've already looked at class weight balancing, which can work in some cases.  But unfortunately doesn't work for our example problem.  Next let's look at under and over sampling.  Before we go through that, let's define what sampling is and why it can be effective.\n",
    "\n",
    "### Sampling\n",
    "\n",
    "In a perfect world, we would never need to sample.  We would always have enough data and that data would be our population of interest.  We would know everything about it that was relevant and we would be able to accurately come up with policy interventions or understand some phenomenon.  Unfortunately, in the real world, we have no such luck.  We rarely, if ever, have all the data about our population of interest.  Does that mean that our models are not faithful to the real world, if they are trained on limited data?  The answer is not necessarily!\n",
    "\n",
    "In chapter one we introduced the notion of the distribution and descriptive statistics.  Loosely defined, a sample is a distribution of data that comes from a large population distribution.  One way to measure similarity between the sample and the population distribution is to see if their descriptive statistics, also known as their characteristics, are similar.\n",
    "\n",
    "Let's look at an example!\n",
    "\n",
    "Suppose there exists the following population distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: -0.038410954520667805\n",
      "Standard Devation: 9.991669662907558\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "population = np.random.normal(0, 10, size=100000)\n",
    "\n",
    "bins = 100\n",
    "plt.hist(population, bins=bins)\n",
    "plt.show()\n",
    "print(\"Mean:\", np.mean(population))\n",
    "print(\"Standard Devation:\", np.std(population))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now assume we can only take samples of this population and that the financial cost of extracting the next sample increases overtime.  So let's say that the cost increases linearly with each sample extracted, by a rate of 2 cents more per sample.  Let's see how cost increases as we draw more samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_size = np.linspace(0, 10000)\n",
    "cost_for_sample_i = sample_size ** 2\n",
    "\n",
    "plt.plot(sample_size, cost_for_sample_i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the 10,000th sample costs 10,000,000 dollars in total!!!  That's a huge cost!  And that's at a rate of increase of 2 cents more per sample!  So these things can get very, very pricey.  The cost associated with obtaining a large sample or obtaining the entire population can be prohibitive.  If you think this example is an overstatement of reality, the 2010 census cost 13 billion dollars.  That's approximately 100 times more expensive than our worked example.  \n",
    "\n",
    "Hopefully I've motivated sampling for you and it's importance.  Now let's go through some techniques:  \n",
    "\n",
    "The first is just a random sample of the population distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: -0.7528969913432664\n",
      "Standard Devation: 9.8871479957865\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sample_size = 100\n",
    "sample = np.random.choice(population, size=sample_size, replace=False)\n",
    "bins = 50\n",
    "plt.hist(sample, bins=bins)\n",
    "plt.show()\n",
    "print(\"Mean:\", np.mean(sample))\n",
    "print(\"Standard Devation:\", np.std(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to call out here:\n",
    "\n",
    "1) The sample is much less bell shaped than our population.  This is to be expected, since there is a lot less data and therefore the shape of our distribution is a lot less smooth.\n",
    "\n",
    "2) The population mean and the sample mean are close.  This is exactly what we'd hope for from a representative sample, our descriptive statistics aren't that far off from one another.  Recall, statistics like the mean are sensitive to outliers, which in some cases are a negative, but here that sensitivity actually helps ensure that we are as close as possible.  Of course, you should still look out for outliers, because if your sample is made up mostly of outliers, it may be the case that your sample \"appears\" good from your descriptive statistics, when in reality, your data is not representative.  That is why it's a good idea to capture multiple measures of center and spread and ensure they all align.\n",
    "\n",
    "3) The population standard deviation and sample standard deviation are close.  The fact that the spread about the center is similar is a good sign.  Unfortunately, the standard deviation is also not great at dealing with outliers, so it's best to include other statistics as well to compensate for this.\n",
    "\n",
    "4) We chose to sample without replacement, via `replace=False`.  This means once we've drawn a sample from our population we cannot draw that sample again.  Sampling with and without replacement is a huge decision in modeling.  If you sample with replacement then it's possible your model could just memorize the data and have terrible generalization.  So it's important, if you do sample with replacement that you truly need to.\n",
    "\n",
    "Now that we have seen _a_ sampling technique in action, let's apply it to our problem.  We are going to do the following things:\n",
    "\n",
    "1) write a function that samples from our training data (not our testing data)\n",
    "2) this function must create samples that satisify the descriptive statistics of our original dataset (as close as we can get them)\n",
    "3) we will specifically be upsampling, aka sampling with replacement from the minority class.  And taking a random sample from our majority class.  By down sampling, aka sampling without replacement, from our majority class we need to upsample less from our minority class.  \n",
    "4) We'll try different levels of upsampling and downsampling such that our test set accuracy is maximized.  \n",
    "5) finally we'll look at results on a validation set of previously unseen examples to make sure everything wasn't overtuned for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-1-508cedfe1d57>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-508cedfe1d57>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import iqr\n",
    "import numpy as np\n",
    "\n",
    "def trimean(arr):\n",
    "    q1 = np.percentile(arr, 25)\n",
    "    q3 = np.percentile(arr, 75)\n",
    "    median = np.percentile(arr, 50)\n",
    "    return (q1 + 2*median + q3)/4\n",
    "\n",
    "def compare_categorical_indices(sample_index, population_index, missing_tolerance, include_outliers):\n",
    "    if include_outliers:\n",
    "        if sample_index != population_index\n",
    "            object_outside_tolerance = True\n",
    "        else:\n",
    "            object_outside_tolerance = False\n",
    "    else:\n",
    "        majority_population_indices = []\n",
    "        for index in population_index:\n",
    "            if population_value_proportions[index] > missing_tolerance:\n",
    "                majority_population_indices.append(index)\n",
    "        sample_index_list = list(sample_index)\n",
    "        sample_index_list.sort()\n",
    "        majority_population_indices.sort()\n",
    "        if sample_index_list == majority_population_indices:\n",
    "            object_outside_tolerance = True\n",
    "        else:\n",
    "            object_outside_tolerance = False\n",
    "    return object_outside_tolerance\n",
    "\n",
    "def compare_categorical_statistic(population_value_proportions, sample_value_proportions, sample_index):\n",
    "    population_proportions_in_sample = population_value_proportions[sample_index]\n",
    "    proportional_differences = population_proportions_in_sample - sample_value_proportions\n",
    "    delta = 0\n",
    "    for proportion_difference in proportional_differences:\n",
    "        if proportion_difference > tolerance:\n",
    "            object_outside_tolerance = True\n",
    "        delta += abs(proportion_difference)\n",
    "    return object_outside_tolerance, delta\n",
    "\n",
    "def compare_categorical(df, sample, column, tolerance, missing_tolerance, include_outliers):\n",
    "    population_value_proportions = df[column].value_counts()/len(df)\n",
    "    sample_value_proportions = sample[column].value_counts()/len(sample)\n",
    "    \n",
    "    sample_index = sample_value_proportions.index.sort_values()\n",
    "    population_index = population_value_proportions.index.sort_values()\n",
    "    \n",
    "    outside_tolerance_indices = compare_categorical_indices(\n",
    "        sample_index, population_index, missing_tolerance, include_outliers\n",
    "    )\n",
    "    outside_tolerance_statistic, delta = compare_categorical_statistic(\n",
    "        population_value_proportions, sample_value_proportions, sample_index\n",
    "    )\n",
    "    outside_tolerance = outside_tolerance_indices and outside_tolerance_statistic\n",
    "    return object_outside_tolerance, delta\n",
    "        \n",
    "def compare_continuous(df, sample, column, tolerance, include_outliers):\n",
    "    mean_outside_tolerance = False\n",
    "    trimean_outside_tolerance = False\n",
    "    stdev_outside_tolerance = False\n",
    "    iqr_outside_tolerance = False\n",
    "    \n",
    "    mean_deviation = abs(df[column].mean() - sample[column].mean())\n",
    "    trimean_deviation = abs(trimean(df[column]) - trimean(sample[column]))\n",
    "    stdev_deviation = abs(df[column].std() - sample[column].std())\n",
    "    iqr_deviation = abs(iqr(df[column]) - iqr(sample[column]))\n",
    "    \n",
    "    if mean_deviation > tolerance:\n",
    "        mean_outside_tolerance = True\n",
    "    if trimean_deviation > tolerance:\n",
    "        trimean_outside_tolerance = True\n",
    "    if stdev_deviation > tolerance:\n",
    "        stdev_outside_tolerance = True\n",
    "    if iqr_deviation > tolerance:\n",
    "        iqr_outside_tolerance = True\n",
    "    delta = mean_deviation + trimean_deviation + stdev_deviation + iqr_deviation\n",
    "    outside_tolerance = mean_outside_tolerance and trimean_outside_tolerance \n",
    "    outside_tolerance = outside_tolerance and stdev_outside_tolerance\n",
    "    outside_tolerance = outside_tolerance and iqr_outside_tolerance\n",
    "    return outside_tolerance, delta\n",
    "    \n",
    "def compare_columns(df, sample, columns, column_type,\n",
    "                    tolerance, outside_tolerance, \n",
    "                    missing_tolerance=0.05, include_outliers=False):\n",
    "    if column_type == \"categorical\":\n",
    "        categorical_outside_tolerance, delta = compare_categorical(\n",
    "            df, sample, column, tolerance, \n",
    "            missing_tolerance, include_outliers\n",
    "        )\n",
    "        outside_tolerance = outside_tolerance and categorical_outside_tolerance\n",
    "    if column_type == \"continuous\":\n",
    "        continuous_outside_tolerance, delta = compare_continuous(\n",
    "            df, sample, column, tolerance, include_outliers\n",
    "        )\n",
    "        outside_tolerance = outside_tolerance and continuous_outside_tolerance\n",
    "    return outside_tolerance, delta\n",
    "        \n",
    "def bootstrap(df, column_types, size=1000, tolerance=0.1, missing_tolerance=0.1, with_replacement=False):\n",
    "    outside_of_tolerance = True\n",
    "    best_sample = None\n",
    "    while outside_tolerance:\n",
    "        sample = df.sample(n=size, replace=with_replacement)\n",
    "        overall_delta = 0\n",
    "        min_overall_delta = 1e10\n",
    "        for column in df.columns:\n",
    "            column_type = column_types[column]\n",
    "            outside_tolerance, marginal_delta = compare_columns(\n",
    "                df, sample, column, column_type, \n",
    "                tolerance, missing_tolerance, outside_tolerance\n",
    "            )\n",
    "            overall_delta += marginal_delta\n",
    "        if not outside_tolerance:\n",
    "            best_sample = sample\n",
    "        elif overall_delta < min_overall_delta:\n",
    "            min_overall_delta = overall_delta\n",
    "            best_sample = sample\n",
    "    return best_sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Anamoly Detection\n",
    "\n",
    "It's time for to deal with the problem that is the bane of every data scientists existance - anamoly detection.  If you've never dealt with one of these before, don't worry, at some point you will.  These things come up _a lot_ in information extraction from text.  But they also come up generally in a bunch of places.  Normal models won't work, no matter what you try.  And worse then that, you might not even realize you are dealing with an anamoly detection problem until it's far too late.  As in, you've gone to production and the sample data you saw for training and testing was _far_ too curated.  \n",
    "\n",
    "Anamoly detection, is basically when your data falls into a class around 90-95% and about 10% or less the rest of the time.  It may be the case that this happens because, like I stated above the data you saw when you were developing the model was _not_ representative.  This can get _especially_ bad with multiclass classification where multiple classes are relatively rare in production.  You might say, these cases are easy!  Just ignore the rare classes, no point in dredging the swamp for the extra little bit of value!  And honestly, that might be a good answer, _sometimes_.  But you won't always get lucky with who you are employed by, who your boss is, or what they care about.\n",
    "\n",
    "Or in some cases, it may be by the design of the problem.  Of course, the first scenario is _far_ worse, so we will focus on the second case, where there is an anamolous class, by design.  We will also focus on the case of binary classification, because multiclass classification, is frankly speaking, just too hard.\n",
    "\n",
    "### Problem Set up - Credit Card Fraud Detection\n",
    "\n",
    "The problem of fraud is fairly straight forward - someone uses your card for a purchase that's \"weird\" and the credit card company tells you about it.  Of course, this case isn't going to happen often (we hope).  So it's the minority class by _a lot_.  We might not have many examples per customer of this occurring.  However, we may have 'enough' examples over the course of many customers.  This of course, means there will be a lot more cases of transactions where this _didn't_ happen, which can be tough, because your classifier will just learn the majority class.  This also may not be reflected in your metrics since precision, recall and f1 score will all look _great_.  However, really what this means is, they are great for the majority class, not the minority class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "* https://towardsdatascience.com/hands-on-predict-customer-churn-5c2a42806266\n",
    "* https://blog.markgrowth.com/eliminating-churn-is-growth-hacking-2-0-47a380194a06\n",
    "* https://towardsdatascience.com/churn-prediction-3a4a36c2129a\n",
    "* https://www.profitwell.com/blog/the-complete-saas-guide-to-calculating-churn-rate-and-keeping-it-simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
