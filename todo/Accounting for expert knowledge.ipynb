{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accounting for Expert Knowledge\n",
    "\n",
    "* An Introduction to Differential Equations\n",
    "    * Simple Population Models\n",
    "    * Spring Models\n",
    "    * SEIRS Models\n",
    "* A review of Statistical Learning Algorithms\n",
    "    * Newton's Method\n",
    "    * Stochastic Gradient Descent\n",
    "    * linear programming\n",
    "* Combining Physical Models with Statistical Models\n",
    "    * A review of ensembling methods\n",
    "* Statistical Models with Extra Structure\n",
    "    * Probabilistic Graphical Models\n",
    "    * Mixed Effects Models\n",
    "    * Hierarchical Bayesian Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Introduction to Differential Equations\n",
    "\n",
    "Up until this point we've assumed, at least implicitly that you need tons of data to build a model.  However, it is possible to build a model of the world with very limited data, assuming you know some things.  First we'll look at differential equations - which is used to write down mathematical models.  Specifically, with very limited data we can get a precise view of certain systems.  These systems have specific requirements though:\n",
    "\n",
    "1. We need to understand all of the \"states\" of the system\n",
    "\n",
    "Typically this means we are dealing with physical systems, where we can enumerate all the states of the model.  If the model has a hidden or unknown state, a model using differential equations is potentially ill-advised.\n",
    "\n",
    "2. We need to have realistic assumptions about our system\n",
    "\n",
    "This requirement basically states that our assumptions are realistic and by writing them down mathematically we will get a reasonable functional form for our system.\n",
    "\n",
    "3. We know all of the major variables that will interact together in our system\n",
    "\n",
    "This requirement says that we can measure everything of interest and ultimately we can capture the phenomenon we are interested through observations or assumptions.  This is another reason why differential equations is typically used with physical systems.  In this case, it's very unlikely that anything is \"hidden\" or difficult to observe.  You can know or understand everything about your system, simply by gathering observations or other similar activities.\n",
    "\n",
    "4. Our system varies in some way with time\n",
    "\n",
    "In general the procedure we are going to use to generate a differential equation could be used to write down any system that is well modeled explicitly with mathematics.  But the main difference between any old model and a model involving differential equations is dynamics.  Specifically a differential equation assumes you are working with a system that changes in some way over time.\n",
    "\n",
    "### A General Procedure For Creating A Differential Equation\n",
    "\n",
    "Once you are sure a differential equation is appropriate for your model then the following set of steps can help guide you in figuring out exactly how your differential equation or equations ought to look:\n",
    "\n",
    "Step 1. Clearly state the assumptions on which the model will be based.  These assumptions should describe the relationships among the quantities to be studied.\n",
    "\n",
    "Step 2. Completely describe the variables and parameters to be used in the model.  This is why you need a full understanding of the system.  If some of the variables are implicit or unobservable directly then a differential equation is not appropriate.\n",
    "\n",
    "Step 3. Use the assumptions formulated in Step 1 to derive equations relating to the quantities in Step 2.\n",
    "\n",
    "### A First Example\n",
    "\n",
    "Suppose you were interested in predicting the population of a certain bacteria after a number of hours.  In order to understand the population growth in general, we need a model of the growth of this population.  \n",
    "\n",
    "Step 1: Write out your assumptions:\n",
    "\n",
    "* We assume the population is unbounded.  That is, there is no maximum size that the population can grow to, so it grows forever.\n",
    "\n",
    "* The population has unlimited access to food.  That is, we never need to worry about the food supply of the bacteria disappearing because it is easy to keep adding more.  Therefore the population is also not bounded by food supply.\n",
    "\n",
    "* The population has an infinite area to grow within.  Specifically, the population won't be constrained by any area we house it in because bacteria are so small.\n",
    "\n",
    "Step 2: State your variables and parameters:\n",
    "\n",
    "* let t = time, our independent variable\n",
    "\n",
    "* let P = population, our dependent variable\n",
    "\n",
    "* let k = proportionality constant between the rate of growth of the population and the size of the population.\n",
    "\n",
    "* let $\\frac{dP}{dt}$ = the derivative of the population with respect to time.  \n",
    "\n",
    "Step 3: Write down your differential equation\n",
    "\n",
    "$$ \\frac{dP}{dt} = k*P $$\n",
    "\n",
    "What the equation says is:\n",
    "\n",
    "\"The rate of change of our population is equation to the current population times some proportionality constant\".  In other words, our population's growth rate is proportional to it's current population, at some time, t.\n",
    "\n",
    "So are we done?  Well almost.  Now comes the \"tricky\" part.  This is where most of the technical \"mathematics\" comes into play.  Now that we have a reasonable mental model for how our population grows, we need an explicit functional form, so that we can do prediction.  Specifically, we are interested in knowing our population with respect to time, in other words, we are interested in $P(t)$.  \n",
    "\n",
    "Up until now we haven't explicitly stated that our population was a function of time, but we did hint at it in several places.  First we called P a dependent variable, meaning it depends on t.  Secondly, we called t an independent variable, so there is no function that defines t.  And since P depends on t, another way of saying that is, $P(t)$.  \n",
    "\n",
    "So now that we've settled on P being a function of time, we need to figure out the exact functional form of $P(t)$.  It's worth noting, before we figure out the functional form in this case, it's sometimes impossible to write down a functional form.  That is to say, there may be no analytic solution for a given differential equation, no precise functional form.  For that, we'll turn to statistical models.  Because even if we can't capture a system due it's level of complexity, as long as we have instruments to measure it carefully, and we understand the variables of the system correctly, we can still capture it's approximate functional form statistically.  We'll see more of this later on in the capture.  Of course you could look at any of the other chapters for examples of purely statistical models if the statistical models presented here don't seem useful for the problem you are trying to solve.\n",
    "\n",
    "Alright, so back to our problem at hand:\n",
    "\n",
    "We know that:\n",
    "\n",
    "$$ \\frac{dP}{dt} = k * P(t) $$\n",
    "\n",
    "This means we need a function whose derivative is itself, times a constant.  Given the trivial nature of this function, and assuming you know calculus, the answer is obvious.  In order to figure out the general functional form, we need only do a quick pattern match.\n",
    "\n",
    "The answer is obvious, it's $e^{a*x}$.  That's because:\n",
    "\n",
    "$$ f(x) = e^{a*x} $$\n",
    "$$ f'(x) = a*e^{a*x} $$\n",
    "\n",
    "So!  If we take the derivative of $e^{a*x}$ we get back itself times a constant, in this case $a$.  Which is exactly what we wanted!!!\n",
    "\n",
    "If we want to be more specific about our prediction from here, we need only know how many bacteria we started with at time t=0 and then some later values of t.  We could then use that to figure out the exact starting value of k as well as the coefficient associated with the power of e.  That is, our exact functional form would look like this:\n",
    "\n",
    "$$ \\frac{dP}{dt} = P_{0}*e^{a*t} $$\n",
    "\n",
    "Could we have used any base to the power of x to do this?  The answer turns out to be yes!  In particular:\n",
    "\n",
    "$$ f(x) = a^{x} $$\n",
    "$$ f'(x) = ln(a) * a^{x} $$\n",
    "\n",
    "And $ln(a)$ is still a constant, so _any_ base could have worked!  Specifically the general form of our \"solution space\" is:\n",
    "\n",
    "$$ \\frac{dP}{dt} = P_{0}*b^{a*t} $$\n",
    "\n",
    "Where $b$ is some constant.\n",
    "\n",
    "This points to a more general fact - differential equations can have \"families\" of solutions.  Picking the correct constant can be the hardest part.  Since it is so difficult, we'll actually pass this part of our solution off to the computer.\n",
    "\n",
    "## Parameter estimation\n",
    "\n",
    "In general there are a few ways to do parameter estimation:\n",
    "\n",
    "1. Review the literature for agreed upon values\n",
    "\n",
    "For instance, let's say we wanted to use gravity as a constant in our equations for some physics related equation on earth.  Then we can probably just make use of the \"agreed upon\" value for the gravitational constant: `9.807 m/sÂ²`.  The scientific literature will often have the values for many useful constants that are widely used in the physical sciences.  So we can just look them up.  Of course, this method is naive.  For precise calculations we may need to actually _estimate_ these constants.  For instance, gravity on earth is not _actually_ constant.  It doesn't vary much, but it does vary.  And for some precise physical systems that minor variance matters _a lot_.  \n",
    "\n",
    "This brings us to our second method:\n",
    "\n",
    "2. Gather some data and then \"fit\" a model with those parameters.\n",
    "\n",
    "Everything we did above was about coming up with a general structural form for our mathematical description of a process.  However, since some of the constants aren't known in general, we leave them as variables.  Estimating those parameters is how we make the model useful.  Even if you are 100% sure of the mathematical form of your model, if your parameters are bad, then the model will be bad.\n",
    "\n",
    "### Fitting a Model\n",
    "\n",
    "Since literature review won't work for our example, we'll have to fit a model.  In order to do that first we typically need to collect some data.  For this first example, we'll just fake some data.  Once we've got our data, then we need to go about estimating our parameters.  For this we'll need a loss function.  The loss function tells us how far off we are in general.  We could also plot a graph of the predictions against the actual values.  Or we could plot the residuals, that is, the difference between the predictions and the actual values.  Finally, we'll need some way of guessing parameters to try.  Once we have all those three pieces, we'll be able to \"fit\" a model.  So to recap our steps are:\n",
    "\n",
    "1. Obtain some data\n",
    "2. Define a loss function\n",
    "3. Decide on a way to search for parameters\n",
    "\n",
    "### Obtain Some Data\n",
    "\n",
    "In general there are data sources for anything you might like on the internet.  If you are looking for data I can recommend looking in the following places:\n",
    "\n",
    "* https://www.kaggle.com/datasets\n",
    "* https://data.world/\n",
    "* https://github.com/awesomedata/awesome-public-datasets\n",
    "\n",
    "If you can't find what you are looking for after searching through all of that, it may not exist.  \n",
    "\n",
    "For our purposes, we are just going to generate some data from some known distributions since this is a worked first example.  Since we agreed ahead of time this should have an exponential functional form, let's start there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_data(size: int) -> np.array:\n",
    "    \"\"\"\n",
    "    A data generation process based on a random set of parameters.\n",
    "    The general functional form of our solution is exponential.\n",
    "    We also assume that our population will strictly increase with time.\n",
    "    \n",
    "    Our random parameters are:\n",
    "    * base\n",
    "    * initial population\n",
    "    * alpha\n",
    "    * error\n",
    "    \n",
    "    And our general functional form is:\n",
    "    \n",
    "    $$P_{0} * base^{alpha*time} + error$$\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    size : int - the number of time steps to generate.\n",
    "    Each time step represents 1 hour.\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    A numpy array of all the realized values for the random process\n",
    "    with the aboved general functional form.\n",
    "    \"\"\"\n",
    "    initial_population = abs(np.random.normal(200, 150))\n",
    "    base = abs(np.random.normal(2, 0.5))\n",
    "    alpha = abs(np.random.normal(2, 1.5))\n",
    "    return np.array([\n",
    "        (initial_population * (base**(alpha*time))) + np.random.normal(50, 200)\n",
    "        for time in range(size)\n",
    "    ])\n",
    "\n",
    "data = generate_data(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we are able to generate data points fairly easily.  Let's go ahead and plot our result to see what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6fa05e9550>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(100), data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected from an exponential functional form, the data \"explodes\" to a very high value as we move forward in time.  This should be unsurprising since we assume monotonic population growth.\n",
    "\n",
    "### Define a Loss Function\n",
    "\n",
    "Now that we have something to try and fit, our next step is to come up with a method for figuring out how far away our model is from our observed values.  This is crucial because, in general we don't know the mathematical formulation of the data generation process.  \n",
    "\n",
    "In general our loss function should be point statistic for a collection of distance measures.  So if we care primarily about closeness to the trend of a process, then we probably want a measure of centrality.  If we care about the maximum range of possible values, but a particular estimate only needs to be within a range, then we probably want a measure of spread.  For completeness I'll list out some distance functions, measures of centrality and measures of spread:\n",
    "\n",
    "Distance functions:\n",
    "\n",
    "* Euclidean:\n",
    "\n",
    "$$ \\sqrt{\\sum_{i=0}^{i=N} (p_{i} - q_{i})^{2}} $$\n",
    "\n",
    "* Manhattan:\n",
    "\n",
    "$$ \\sum_{i=0}^{i=N} | p_{i} - q_{i} | $$\n",
    "\n",
    "* Minkowski\n",
    "\n",
    "$$ (\\sum_{i=0}^{i=N} | p_{i} - q_{i} |^{h})^{1/h} $$\n",
    "\n",
    "Measures of Centrality:\n",
    "\n",
    "* Mean\n",
    "* Median\n",
    "* Trimean\n",
    "\n",
    "For more complete list of central tendencies I recommend checking out:\n",
    "\n",
    "* [https://en.wikipedia.org/wiki/Central_tendency](https://en.wikipedia.org/wiki/Central_tendency)\n",
    "\n",
    "Measures of Spread:\n",
    "\n",
    "* Variance\n",
    "* Standard Deviation\n",
    "* Interquartile Range\n",
    "\n",
    "For a more complete list of measures of spread I recommend checking out:\n",
    "\n",
    "* [https://en.wikipedia.org/wiki/Statistical_dispersion](https://en.wikipedia.org/wiki/Statistical_dispersion)\n",
    "\n",
    "Since we are interested in a measure of precision for this example, we'll just go with the euclidean distance and the simple mean:\n",
    "\n",
    "$$ \\sum_{i=0}^{i=N} \\frac{(p_{i} - q_{i})^{2}}{N} $$\n",
    "\n",
    "Notice how the two concepts come together:\n",
    "\n",
    "The notion of difference comes directly from our distance metric - subtraction and squaring.  And we use the formula for the simple average around that differencing.  \n",
    "\n",
    "Notice that because we don't take a square root, differences that are large will be over stated and differences that are less than 1 will be understated.  This is by design for this loss function.  If these traits were not desireable we could always take a square root after averaging, to recover a \"true\" euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completely different sequences 75.00666666666642\n",
      "similar sequences 2.0677030264267606\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_squared_error(actual: np.array, predicted: np.array) -> float:\n",
    "    \"\"\"\n",
    "    The mean squared error is a measure of \n",
    "    the average error of two sequences of data.\n",
    "    \n",
    "    It is designed by using the euclidean distance and the\n",
    "    simple mean:\n",
    "    \n",
    "    $$ \\sum_{i=0}^{i=N} \\frac{(p_{i} - q_{i})^{2}}{N} $$\n",
    "    \"\"\"\n",
    "    squared_difference = (actual - predicted) ** 2 \n",
    "    return sum(squared_difference)/len(actual)\n",
    "    \n",
    "first = np.arange(0, 15, 0.1)\n",
    "second = np.arange(15, 0, -0.1)\n",
    "\n",
    "print(\"completely different sequences\", mean_squared_error(first, second))\n",
    "\n",
    "first = np.random.normal(0, 1, size=1000)\n",
    "second = np.random.normal(0, 1, size=1000)\n",
    "\n",
    "print(\"similar sequences\", mean_squared_error(first, second))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our error measure seems to tell us something reasonable:\n",
    "\n",
    "* if the data is very different, per value in the sequence on average, the loss is large.\n",
    "* if the data is very similar, per value in the sequence on average, the loss is small.\n",
    "\n",
    "Next let's see if the length of sequence being compared matters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longer sequences have more error than short sequences 5080 out of 10000\n",
      "The average deviance was -0.005559081523059284\n"
     ]
    }
   ],
   "source": [
    "def mean(arr):\n",
    "    return sum(arr)/len(arr)\n",
    "\n",
    "count_long_larger = 0\n",
    "deviances = []\n",
    "size = 10000\n",
    "for _ in range(size):\n",
    "    long_first = np.random.normal(0, 1, size=1000)\n",
    "    long_second = np.random.normal(0, 1, size=1000)\n",
    "\n",
    "    short_first = np.random.normal(0, 1, size=100)\n",
    "    short_second = np.random.normal(0, 1, size=100)\n",
    "\n",
    "    long_error = mean_squared_error(long_first, long_second)\n",
    "    short_error = mean_squared_error(short_first, short_second)\n",
    "    \n",
    "    if long_error > short_error:\n",
    "        count_long_larger += 1\n",
    "        \n",
    "    deviances.append(long_error - short_error)\n",
    "    \n",
    "\n",
    "print(f\"Longer sequences have more error than short sequences {count_long_larger} out of {size}\")\n",
    "print(f\"The average deviance was {mean(deviances)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above \"back of the envelope calculation\" it looks like the length of the sequence alone is not enough to induce additional variance.  Therefore, this measure shouldn't be larger as a function of the size of our sequence.  Which is certainly a desirable trait.  \n",
    "\n",
    "### Decide on a way to search for parameters\n",
    "\n",
    "Now that we can tell when we are wrong and we have some data to train against, the final thing we need is a way to search for parameters.  The most naive thing to do is just search some space via iterating over a full range of values.  So let's try that first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eric/.local/lib/python3.7/site-packages/ipykernel_launcher.py:47: RuntimeWarning: overflow encountered in square\n",
      "/home/eric/.local/lib/python3.7/site-packages/ipykernel_launcher.py:76: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/eric/.local/lib/python3.7/site-packages/ipykernel_launcher.py:76: RuntimeWarning: overflow encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model with the best parameters had a loss of: 12274121.116730185\n",
      "The parameters were: (35, 1.6, 0.2)\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "def generate_data(size: int) -> np.array:\n",
    "    \"\"\"\n",
    "    A data generation process based on a random set of parameters.\n",
    "    The general functional form of our solution is exponential.\n",
    "    We also assume that our population will strictly increase with time.\n",
    "    \n",
    "    Our random parameters are:\n",
    "    * base\n",
    "    * initial population\n",
    "    * alpha\n",
    "    * error\n",
    "    \n",
    "    And our general functional form is:\n",
    "    \n",
    "    $$P_{0} * base^{alpha*time} + error$$\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    size : int - the number of time steps to generate.\n",
    "    Each time step represents 1 hour.\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    A numpy array of all the realized values for the random process\n",
    "    with the aboved general functional form.\n",
    "    \"\"\"\n",
    "    initial_population = abs(np.random.normal(200, 150))\n",
    "    base = abs(np.random.normal(2, 0.5))\n",
    "    alpha = abs(np.random.normal(2, 1.5))\n",
    "    return np.array([\n",
    "        (initial_population * (base**(alpha*time))) + np.random.normal(50, 200)\n",
    "        for time in range(size)\n",
    "    ])\n",
    "\n",
    "def mean_squared_error(actual: np.array, predicted: np.array) -> float:\n",
    "    \"\"\"\n",
    "    The mean squared error is a measure of \n",
    "    the average error of two sequences of data.\n",
    "    \n",
    "    It is designed by using the euclidean distance and the\n",
    "    simple mean:\n",
    "    \n",
    "    $$ \\sum_{i=0}^{i=N} \\frac{(p_{i} - q_{i})^{2}}{N} $$\n",
    "    \"\"\"\n",
    "    squared_difference = (actual - predicted) ** 2 \n",
    "    return sum(squared_difference)/len(actual)\n",
    "\n",
    "def search_parameters(model, parameter_ranges, data):\n",
    "    losses = []\n",
    "    parameter_enumerations = []\n",
    "    for parameter_range in parameter_ranges:\n",
    "        parameter_enumerations.append(\n",
    "            np.arange(*parameter_range)\n",
    "        )\n",
    "    params = list(\n",
    "        product(*parameter_enumerations)\n",
    "    )\n",
    "    for i in range(len(params)):\n",
    "        losses.append({\n",
    "                \"mse\": mean_squared_error(data, model(*params[i])),\n",
    "                \"params\": params[i]\n",
    "            })\n",
    "    return losses\n",
    "        \n",
    "def select_best_model(losses):\n",
    "    mses = [(i, losses[i][\"mse\"]) for i in range(len(losses))]\n",
    "    mses = sorted(mses, key=lambda t: t[1])\n",
    "    best_loss_index = mses[0][0]\n",
    "    return losses[best_loss_index]\n",
    "\n",
    "def model(initial_population, base, alpha, size=100):\n",
    "    return np.array([\n",
    "        (initial_population * (base**(alpha*time)))\n",
    "        for time in range(size)\n",
    "    ])\n",
    "\n",
    "data = generate_data(100)\n",
    "parameter_ranges = [\n",
    "    (25, 500, 10), # initial population guess range\n",
    "    (0, 5, 0.1), # base guess range\n",
    "    (0, 10, 0.1) # alpha guess range\n",
    "]\n",
    "losses = search_parameters(model, parameter_ranges, data)\n",
    "best_model = select_best_model(losses)\n",
    "print(f\"The model with the best parameters had a loss of: {best_model['mse']}\")\n",
    "print(f\"The parameters were: {best_model['params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might be tempted to draw conclusions from how poor our model does (this the best my loss got after several attempts and some fiddling with the size of the search space), but that would be ill advised.  Our hyper parameter search was _incredibly_ naive.  One lesson that is reasonable to take away from this is that exponentials are incredibly sensitive to hyper parameters.  So one should only use such a model in practice if one is sure that some other model cannot be used first.  For instance sigmoid functions, which locally approximate exponentials but trial off above or below a given value are a very good substitute.  \n",
    "\n",
    "Leaving the particulars of this example behind we sketched out for the first time a general pattern for doing modeling:\n",
    "\n",
    "1. obtain data\n",
    "2. decide on a loss function\n",
    "3. search for appropriate parameters.  \n",
    "\n",
    "This is more or less the general form for doing modeling against the solution space of differential equations.  Of course, as we've seen in other chapters, this is similar to the procedure used with purely statistical models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside - consider hidden state discovery if there are a range of parameters where the diffEQ model massively \"misbehaves\".  So if there is some range where our model is just \"wrong\", rely solely on the statistical model.\n",
    "\n",
    "Aside - consider a statistical approach to \"picking the constants\" from a family of differential equations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
