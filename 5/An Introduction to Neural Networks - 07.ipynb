{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Neural Networks\n",
    "\n",
    "\n",
    "* An Introduction to Probabilistic Graphical Models (Skip)\n",
    "    * Bayesian Networks\n",
    "    * Hidden Markov Chains\n",
    "* A review of linear regression\n",
    "    * linear algebra review\n",
    "        * row reduction of a matrix \n",
    "        * matrix multiplication\n",
    "        * invertability\n",
    "    * Implementing linear regression with linear algebra\n",
    "* Neural Networks\n",
    "    * Chain Rule over vector spaces\n",
    "    * psuedo code\n",
    "    * implementing a basic neural network in raw python\n",
    "        * Forward propagation\n",
    "        * Back propagation\n",
    "    * Issues in convergence\n",
    "        * vanishing gradient\n",
    "        * exploding gradient\n",
    "    * introduction to keras\n",
    "        * A simple example - regression\n",
    "        * A simple example - classification\n",
    "        * neural network architecture design\n",
    "            * final layer for regression\n",
    "            * final layer for classification\n",
    "            * regularization\n",
    "    * Surogate Models\n",
    "        * linear regression\n",
    "        * decision trees\n",
    "            * feature importance\n",
    "            * tre interpreter\n",
    "    * SHAP\n",
    "        * regression example\n",
    "        * classification example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Review of Linear Regression\n",
    "\n",
    "In the first part of this tutorial we saw how to implement linear regression using calculus.  In order to build comfort with matrices, which are a central core notion in the design and implementation of neural networks, we will review how to implement linear regression, except this time, using the framing of linear algebra.\n",
    "\n",
    "The other reason it is helpful to recall our notions around linear regression is neural networks draw from linear regression.  The simplest possible neural network is actually just a set of chained linear regressions.  The power of neural networks is in how one does the chaining of course.  That said, neural nets can be far more complex, interesting, powerful and difficult to understand.  \n",
    "\n",
    "Finally, it's easy to get lost in all the machinery and choices of neural networks so it's important to ground them in context - neural networks just solve an optimization problem, minimizing error like any other algorithm.  And typically, the way they do that is via Stochastic Gradient Descent.\n",
    "\n",
    "## A Review of Stochastic Gradient Descent\n",
    "\n",
    "Recall the following picture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"NewtonIteration_Ani.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<img src=\"NewtonIteration_Ani.gif\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In it, we can see broadly how gradient descent works - we start with some initial guess, then we use the gradient (which is the same as a derivative) to update that guess and get closer to an optimum.  Optimums are the minimums or maximums of your function, depending on what you are trying to do.  In the case of gradient descent, you are trying to minimize loss, that is the difference between your predicted outcomes and the outcomes you saw in your training data.\n",
    "\n",
    "## A Review of Linear Algebra\n",
    "\n",
    "In the previous implementation of Gradient Descent we made use of the derivative directly via calculus to find our optimum, minimizing error.  However, in practice most algorithms make use of the linear algebra formulation of Gradient Descent because linear algebra is extremely power and we have many computational tricks to make it fast.\n",
    "\n",
    "Before we dive into the implementation, let's review some concepts from linear algebra to get a sense of how to work with matrices and vectors, the mainstays of the subfield.\n",
    "\n",
    "### Motivating Linear Algebra\n",
    "\n",
    "One of the general goals of linear algebra is to be able to solve sets of equations simultaneously.  First, let's see how to solve a single equation using simple python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'zero' for equation one is 0\n"
     ]
    }
   ],
   "source": [
    "def equation_one(x):\n",
    "    return x\n",
    "\n",
    "def find_zero_eq_one():\n",
    "    for i in range(-100, 100):\n",
    "        if equation_one(i) == 0:\n",
    "            return i\n",
    "        \n",
    "print(\"The 'zero' for equation one is\", find_zero_eq_one())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy enough!  This is the simplest possible way we could \"solve\" an equation, just guess and check.  Now let's see if this method get's more complex when we make our equation more sophisticated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'zero' for equation two is -7\n"
     ]
    }
   ],
   "source": [
    "def equation_two(x):\n",
    "    return x + 7\n",
    "\n",
    "def find_zero_eq_two():\n",
    "    for i in range(-100, 100):\n",
    "        if equation_two(i) == 0:\n",
    "            return i\n",
    "        \n",
    "print(\"The 'zero' for equation two is\", find_zero_eq_two())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've added a constant, thus changing our equation.  Still pretty simple and we can reuse our solver!  What happens when we go to two variables and add coeficients on our variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'zero' for equation four is (-9.400000000000002, 9.999999999999963)\n"
     ]
    }
   ],
   "source": [
    "def arange(start, stop, step):\n",
    "    iterator = start\n",
    "    while iterator < stop:\n",
    "        yield iterator\n",
    "        iterator += step\n",
    "\n",
    "def equation_four(x, y):\n",
    "    return 5*x + 4*y + 7\n",
    "\n",
    "def find_zero_eq_four(episolon):\n",
    "    for i in arange(-10, 10, 0.1):\n",
    "        for j in arange(-10, 10, 0.1):\n",
    "            if abs(equation_four(i, j)) < episolon:\n",
    "                return (i, j)\n",
    "        \n",
    "print(\"The 'zero' for equation four is\", find_zero_eq_four(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, now we need two for loops and also, we can no longer make use of the built-in range function.  Instead, we need access to a far more granular set of numbers - the floating points, instead of the integers.  This also means that we can no longer have equality in our stopping condition:\n",
    "\n",
    "`if abs(equation_four(i, j)) < episolon`\n",
    "\n",
    "Now we need set a tolerance for acceptance, a them of many data science algorithms, because typically we work with floating point numbers which (almost) never converge to an integer value.\n",
    "\n",
    "Let's add yet another variable just to drive the point home:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'zero' for equation four plus is (-9.8, -9.500000000000002, 9.999999999999963)\n"
     ]
    }
   ],
   "source": [
    "def arange(start, stop, step):\n",
    "    iterator = start\n",
    "    while iterator < stop:\n",
    "        yield iterator\n",
    "        iterator += step\n",
    "        \n",
    "def equation_four_plus(x, y, z):\n",
    "    return 5*x + 4*y + 8*z + 7\n",
    "\n",
    "def find_zero_eq_four_plus(episolon):\n",
    "    for i in arange(-10, 10, 0.1):\n",
    "        for j in arange(-10, 10, 0.1):\n",
    "            for k in arange(-10, 10, 0.1):\n",
    "                if abs(equation_four_plus(i, j, k)) < episolon:\n",
    "                    return (i, j, k)\n",
    "        \n",
    "print(\"The 'zero' for equation four plus is\", find_zero_eq_four_plus(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see things get even worse!  And now we need 3 for loops!  With every variable we add to a single equation, using classical techniques, like for loops, we will run into trouble fast.  Computationally speaking, linear algebra can make our lives significantly easier, producing algorithms that can solve systems of equations, that is many equations needing to be solved together much faster than the simple for loop scheme we have defined above.\n",
    "\n",
    "This is the reason no one uses calculus to solve gradient descent, the algorithm simply does not scale the same way.  And with a neural network, you may need to solve thousands or millions of equations simultaneously (typically hundreds).  \n",
    "\n",
    "Just for completeness, let's look at how to write down how to solve a single equation with an arbitrary number of variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-7.000000000000011,)\n",
      "(-10, 1.4999999999999816)\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import itertools\n",
    "\n",
    "\n",
    "def arange(start, stop, step):\n",
    "    iterator = start\n",
    "    while iterator < stop:\n",
    "        yield iterator\n",
    "        iterator += step\n",
    "\n",
    "        \n",
    "def equation_five(coefficients, constant, variables):\n",
    "    return sum([coefficients[index]*variables[index] \n",
    "                for index in range(len(variables))]) + constant\n",
    "\n",
    "\n",
    "def find_zero_eq_five(coefficients, constant, episolon):\n",
    "    eq_five = partial(equation_five, coefficients, constant)\n",
    "    value_range = list(arange(-10, 10, 0.1))\n",
    "    values = list(itertools.permutations(value_range, len(coefficients)))\n",
    "    for value in values:        \n",
    "        if abs(eq_five(value)) < episolon:\n",
    "                return value\n",
    "\n",
    "print(find_zero_eq_five([1], 7, 0.1))\n",
    "print(find_zero_eq_five([1, 2], 7, 0.1))\n",
    "#print(find_zero_eq_five([1, 2, 3], 7, 0.1))\n",
    "#print(find_zero_eq_five([1, 2, 3, 4], 7, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason this solver looks a little more compact is not because it's solving fewer equations, it's because it's getting all the for loops upfront via the permutations function.  This little computational trick makes things a little more tractable computationally speaking, but it doesn't give us enough of a boost to make things tenable.  I could in fact break this notebook just by running the third or fourth example.\n",
    "\n",
    "## Matrices\n",
    "\n",
    "We've worked with a few algorithms a lot.  Now we'll introduce a new data structure - the matrix.  A matrix mathematically speaking is a notational convention.  More or less a matrix is just a vector of vectors.  And a vector is just the coefficients of a linear equation.  We've been implicitly using mathematical vectors since we introduced equations above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first example of matrices, Supply and Demand\n",
    "\n",
    "Let's say that we own and operate a business that sells candy bars.  Assume there is implicit demand for candy bars and many other candy bar sellers.  However, assume our costs are perhaps slightly different from other candy bar sellers, so we can choose what price to sell our candy bar at, subject to our ability to supply candy.  We can also assume we have some supply control because most people aren't patient about candy, they want it when they want it.  So if someone happens by your shop, the price may actually effect their decision to buy the candy or not.\n",
    "\n",
    "So let's say you have a supply function like:\n",
    "\n",
    "`price = 2*quantity + 3`\n",
    "\n",
    "What this equation says is as a business owner, as the price increases you'll want to supply more candy bars, because the perception is, you'll make more money.  \n",
    "\n",
    "Let's also say there is a demand function like:\n",
    "\n",
    "`price = 3 - 2*quantity`\n",
    "\n",
    "This equation says the price you can charge is subject to how many candy bars you make.  So if your shop is filled to the brim and your the only local vendor around, folks will likely be able to always satisfy their craving for candy.  And if you only make a single candy bar, folks will be willing to pay a lot to be the only one to get it.  Because candy is delicious.\n",
    "\n",
    "Given the above equations we need both the supply and demand equation to find the unique equilibrium of price and quantity such that the market for candy bars is in balance.  This balancing point, all other things held constant, will be the price that maximizes utility for the buyers of candy bars as well as the sellers of candy bars.  And since we are treating our example as idealized, we are in a capitalism regime where the intention is to maximize utility for all.  However, even with the introduction of immorality and therefore businesses or agents acting in subversive ways, all economies are still subject to the above laws generally speaking and therefore understanding this idealized situation still yields value.\n",
    "\n",
    "So!  Now that we have our supply and demand equations, how do we find the unique price, quantity that yields our equilibrium?  With matrices of course!\n",
    "\n",
    "First we'll need to turn these equations into the appropriate form:\n",
    "\n",
    "Supply:\n",
    "\n",
    "`price - 2*quantity = 3`\n",
    "\n",
    "Demand:\n",
    "\n",
    "`price + 2*quantity = 3`\n",
    "\n",
    "We need to do this so all the variables are on one side of the equation.  Now we can do this:\n",
    "\n",
    "Supply/Demand Matrix:\n",
    "\n",
    "$$\n",
    "  \\begin{bmatrix} 1 & -2 \\\\ 1 & 2 \\end{bmatrix} \n",
    "  \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now we have the two resulting mathematical objects, a matrix describing the coeficients of the two equations and a vector describing the solution space.  We'll do something called row reduction on the matrix which will produce unique solutions for the equations.  Before we carry out these operations notice that if our matrix looks like this:\n",
    "\n",
    "$$\n",
    "  \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \n",
    "  \\begin{bmatrix} a \\\\ b \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Then whatever our variables are, they are equal to a and b respectively.  Because the matrix:\n",
    "\n",
    "$$\n",
    "  \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Translates to\n",
    "\n",
    "$$\n",
    "  \\begin{bmatrix} x \\\\ y \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "With the `[a b]` vector this becomes:\n",
    "\n",
    "$$\n",
    "x = a \\\\\n",
    "y = b\n",
    "$$\n",
    "\n",
    "Which solves x and y uniquely!  So now we'll do row reduction to create a matrix which looks like the above one.  And then we'll apply the changes we used on the matrix, onto the solution vector as well.  Which will in turn solve our system of equations!\n",
    "\n",
    "\n",
    "$\\begin{bmatrix} 1 & -2 \\\\ 1 & 2 \\end{bmatrix} $\n",
    "\n",
    "```\n",
    "(R[1] = R[1] + R[2])\n",
    "------------------->\n",
    "``` \n",
    "\n",
    "$ \\begin{bmatrix} 2 & 0 \\\\ 1 & 2 \\end{bmatrix} $\n",
    "\n",
    "```\n",
    "(R[1] = R[1]/2)\n",
    "------------------->\n",
    "``` \n",
    "\n",
    "$ \\begin{bmatrix} 1 & 0 \\\\ 1 & 2 \\end{bmatrix} $\n",
    "\n",
    "```\n",
    "(R[2] = R[2] - R[1])\n",
    "------------------->\n",
    "``` \n",
    "\n",
    "$ \\begin{bmatrix} 1 & 0 \\\\ 0 & 2 \\end{bmatrix} $\n",
    "\n",
    "```\n",
    "(R[2] = R[2]/2)\n",
    "------------------->\n",
    "``` \n",
    "\n",
    "$ \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} $\n",
    "\n",
    " \n",
    "Now we'll simply apply the changes to the matrix to the vector:\n",
    "\n",
    "$ \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix} $\n",
    "\n",
    "```\n",
    "(R[1] = R[1] + R[2])\n",
    "------------------->\n",
    "``` \n",
    "\n",
    "$ \\begin{bmatrix} 6 \\\\ 3 \\end{bmatrix} $\n",
    "\n",
    "```\n",
    "(R[1] = R[1]/2)\n",
    "------------------->\n",
    "```\n",
    "\n",
    "$ \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix} $\n",
    "\n",
    "```\n",
    "(R[2] = R[2] - R[1])\n",
    "------------------->\n",
    "```\n",
    "\n",
    "$ \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix} $\n",
    "\n",
    "```\n",
    "(R[2] = R[2]/2)\n",
    "------------------->\n",
    "```\n",
    "\n",
    "$ \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix} $\n",
    "\n",
    "\n",
    "So the unique solution is:\n",
    "\n",
    "```\n",
    "price = 3\n",
    "quantity = 0\n",
    "```\n",
    "\n",
    "What this ends up telling us is it's not possible to optimally operate our candy bar shop.  Which is pretty sad.  So this means we shouldn't open our candy bar shop.  Or we need to change our supply equation by becoming more efficient at producing candy bars.  Of course, it could also be the case that this is truly optimal because candy is bad for you :P.  But that's not really an economic analysis.\n",
    "\n",
    "In any event, we are now ready to write a program which does row reduction for us, by solving a bunch of very simple equations.  Notice, what we really want is to make the numbers in the matrix that are not on the diagonal zero.  And then we want to make the numbers that are on the diagonal 1.  And then we need to store those operations and apply them to solution matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, -2.0], [0.5, 1.0]]\n",
      "1 0\n",
      "[[1.0, -2.0], [0.0, 2.0]]\n",
      "0 1\n",
      "[3.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from random import random\n",
    "from sys import setrecursionlimit\n",
    "\n",
    "def eq_solver(val, func):\n",
    "    if func(val) > 0:\n",
    "        val -= 1\n",
    "        return eq_solver(val, func)\n",
    "    elif func(val) < 0:\n",
    "        val += 1\n",
    "        return eq_solver(val, func)\n",
    "    else:\n",
    "        return round(val, 5)\n",
    "    \n",
    "    \n",
    "def eq_float_solver(val, eq, epsilon=0.0001, step_size=0.001, debug=False):\n",
    "    if abs(eq(val)) < epsilon:\n",
    "        return round(val, 5)\n",
    "    elif eq(val) > 0:\n",
    "        if debug:\n",
    "            val -= step_size\n",
    "            return eq_float_solver(val, eq)\n",
    "        else:\n",
    "            val -= step_size\n",
    "            return eq_float_solver(val, eq)\n",
    "    elif eq(val) < 0:\n",
    "        if debug:\n",
    "            val += step_size\n",
    "            return eq_float_solver(val, eq)\n",
    "        else:\n",
    "            val += step_size\n",
    "            return eq_float_solver(val, eq)\n",
    "    else:\n",
    "        return round(val, 5)\n",
    "\n",
    "\n",
    "def eq_diag_solver(val, eq, epsilon=0.0001, step_size=0.001):\n",
    "    if abs(eq(val)) - 1 < epsilon:\n",
    "        return round(val, 5)\n",
    "    elif eq(val) > 1:\n",
    "        val -= step_size\n",
    "        if val == 0:\n",
    "            val -= step_size\n",
    "        return eq_float_solver(val, eq)\n",
    "    elif eq(val) < 1:\n",
    "        val += step_size\n",
    "        if val == 0:\n",
    "            val += step_size\n",
    "        return eq_float_solver(val, eq)\n",
    "    else:\n",
    "        return round(val, 5)\n",
    "    \n",
    "\n",
    "def arange(start, stop, step):\n",
    "    cur = start\n",
    "    while start < stop:\n",
    "        yield cur\n",
    "        cur += step\n",
    "\n",
    "        \n",
    "def iterative_solver(eq, start, stop, epsilon=0.0001, step_size=0.001):\n",
    "    for val in arange(start, stop, step_size):\n",
    "        if abs(eq(val)) < epsilon:\n",
    "            return round(val, 5)\n",
    "\n",
    "\n",
    "def flatten(matrix):\n",
    "    listing = []\n",
    "    for row in matrix:\n",
    "        for elem in row:\n",
    "            listing.append(elem)\n",
    "    return listing\n",
    "\n",
    "\n",
    "def solve_matrix(matrix):\n",
    "    flattened_matrix = flatten(matrix)\n",
    "    largest_value = max(flattened_matrix)\n",
    "    num_zeros = len(str(largest_value))\n",
    "    val_range = int(\"1\" + \"0\"*num_zeros)\n",
    "    steps = []\n",
    "    cur_matrix = matrix\n",
    "    for index in range(len(matrix)):\n",
    "        col_index = index\n",
    "        row_index = index\n",
    "        cur_matrix, step = solve_diag(cur_matrix[col_index][row_index],\n",
    "                                      row_index, col_index, cur_matrix, val_range)\n",
    "        steps.append(step)\n",
    "    for row_index in range(len(matrix[0])):\n",
    "        for col_index in range(len(matrix)):\n",
    "            if col_index == row_index:\n",
    "                continue\n",
    "            print(cur_matrix)\n",
    "            step = solve_vector(cur_matrix[col_index][row_index], \n",
    "                                row_index, col_index, cur_matrix, val_range)\n",
    "            cur_matrix = linear_combination(cur_matrix, step)\n",
    "            print(col_index, row_index)\n",
    "            steps.append(step)\n",
    "    return steps, cur_matrix\n",
    "\n",
    "\n",
    "def linear_combination(matrix, step):\n",
    "    update_row = step[0]\n",
    "    other_row = step[1]\n",
    "    transformer = step[2]\n",
    "    for index in range(len(matrix[0])):\n",
    "        matrix[update_row][index] = transformer(\n",
    "            matrix[update_row][index], matrix[other_row][index])\n",
    "    return matrix\n",
    "    \n",
    "\n",
    "def solve_diag(elem, cur_elem_idx, diag_index, matrix, magnitude):\n",
    "    reciprical = 1/elem\n",
    "    matrix[diag_index] = [elem*reciprical \n",
    "                          for elem in matrix[diag_index]]\n",
    "    operation = lambda coef, elem: elem*coef\n",
    "    op = partial(operation, reciprical)\n",
    "    step = [diag_index, cur_elem_idx, op, None, reciprical, \"rescale\"]\n",
    "    return matrix, step\n",
    "\n",
    "\n",
    "def solve_vector(elem, cur_elem_idx, diag_index, matrix, magnitude):\n",
    "    for row_index, matrix_row in enumerate(matrix):        \n",
    "        if matrix_row[cur_elem_idx] != 0 and row_index != diag_index:\n",
    "            other_elem = matrix_row[cur_elem_idx]\n",
    "            other_row = row_index\n",
    "            break\n",
    "    eq_to_solve = lambda elem, other_elem, coef: elem + coef*other_elem\n",
    "    to_solve = partial(eq_to_solve, elem, other_elem)\n",
    "    start, stop = magnitude*-1, magnitude\n",
    "    coef = iterative_solver(to_solve, start=start, stop=stop)\n",
    "    operation = lambda coef, elem, other: elem + coef*other\n",
    "    op = partial(operation, coef)\n",
    "    return [diag_index, cur_elem_idx, op, other_row, coef, \"linear_combo\"]\n",
    "\n",
    "\n",
    "def apply_steps(vector, steps):\n",
    "    for step in steps:\n",
    "        if step[-1] == \"linear_combo\":\n",
    "            idx = step[0]\n",
    "            other_idx = step[3]\n",
    "            coef = step[4]\n",
    "            vector[idx] = vector[idx] + coef*vector[other_idx]\n",
    "        else:\n",
    "            idx = step[0]\n",
    "            coef = step[4]\n",
    "            vector[idx] = vector[idx]*coef\n",
    "    return vector\n",
    "\n",
    "\n",
    "#setrecursionlimit(10000)\n",
    "if __name__ == '__main__':\n",
    "    matrix = [[1, -2], [1, 2]]\n",
    "    steps, cur_matrix = solve_matrix(matrix)\n",
    "    print(apply_steps([3, 3], steps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I present the above solution for completeness, in case folks are interested in how to do this from scratch.  \n",
    "\n",
    "## Matrix Operations (Digression)\n",
    "\n",
    "Now that we've seen the power of matrices to allow us to solve systems of equations, let's briefly talk about some of their operations:\n",
    "\n",
    "* Addition\n",
    "* Multiplication\n",
    "\n",
    "Matrix Addition is as simple as you'd suspect, simply go across and add each element in the same row column position:\n",
    "\n",
    "$$\n",
    "  \\begin{bmatrix} 1 & -2 \\\\ 1 & 2 \\end{bmatrix} \n",
    "  + \n",
    "  \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}\n",
    "  =\n",
    "  \\begin{bmatrix} 2 & -1 \\\\ 2 & 3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Notice, the matrices are of the same size, and must be!  So that means you can't add two matrices of different shapes!\n",
    "\n",
    "Next let's look at multiplication:\n",
    "\n",
    "Matrix Multiplication is far more complicated, you have to go across the ith row and jth column to get the i,j entry.  This is best seen with an example:\n",
    "\n",
    "$$\n",
    "  \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \n",
    "  * \n",
    "  \\begin{bmatrix} 2 & 1 \\\\ 4 & 3 \\end{bmatrix}\n",
    "  =\n",
    "  \\begin{bmatrix} 10 & 7 \\\\ 22 & 15 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In order to recover the position at index 1, 1 (matrices are indexed starting at 1 in mathematical notation), you'd need to do:\n",
    "\n",
    "$$ 1*2 + 2*4 = 10 $$\n",
    "\n",
    "To make it clear what happens you take the first row of the first matrix: 1, 2 and multiply it by the first column of the second matrix: 2, 4 element by element and then sum the two multiplications.\n",
    "\n",
    "This means that matrix multiplication is not communitive!  Which means:\n",
    "\n",
    "$$ A*B \\neq B* A $$\n",
    "\n",
    "For matrices A and B.\n",
    "\n",
    "Additionally, matrix mulitplication need not operate over matrices of the exact same size.  Therefore we can do things like this:\n",
    "\n",
    "$$\n",
    "  \\begin{bmatrix} 1 & 2 & 3\\\\ 3 & 4 & 5\\end{bmatrix} \n",
    "  * \n",
    "  \\begin{bmatrix} 2 & 1 \\\\ 4 & 3 \\\\ 5 & 6 \\end{bmatrix}\n",
    "  =\n",
    "  \\begin{bmatrix} 25 & 25 \\\\ 47 & 45 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The only requirement is that our matrices be composable, that is, the number of columns of the first matrix is the same as the number of rows as the second matrix.  It's very important to understand how to compose matrices of different shapes because our individual layers of our neural network are in fact matrices.  If you try to compose two layers that can't be composed you will get a very annoying and confusing error.\n",
    "\n",
    "For completeness, let's see how to do this in python!  Here we'll make use of numpy because the code for implementing fast matrix mulitplication is very confusing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25 25]\n",
      " [47 45]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[1, 2, 3], [3, 4, 5]])\n",
    "B = np.array([[2, 1], [4, 3], [5, 6]])\n",
    "print(np.dot(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, here is the underlying multiplication written out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 7, 0]\n",
      "[22, 15, 0]\n",
      "[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "class Matrix:\n",
    "    def __init__(self,matrix):\n",
    "        self.matrix = matrix\n",
    "            \n",
    "    def get_size(self):\n",
    "        return len(self.matrix),len(self.matrix[0])\n",
    "\n",
    "    def pprint(self):\n",
    "        for row in self.matrix:\n",
    "            print(row)\n",
    "                \n",
    "        \n",
    "    def to_array(self):\n",
    "        return self.matrix\n",
    "        \n",
    "    def get_elem(self,row,col):\n",
    "        return self.matrix[row][col]    \n",
    "        \n",
    "    def __add__(self,other):\n",
    "        row_size,col_size = self.get_size()\n",
    "        new_matrix = []\n",
    "        for row in range(row_size):\n",
    "            new_matrix.append([elem+other.matrix[row][ind] for ind,elem in enumerate(self.matrix[row])])\n",
    "        return Matrix(new_matrix)\n",
    "    \n",
    "    def __sub__(self,other):\n",
    "        row_size,col_size = self.get_size()\n",
    "        new_matrix = []\n",
    "        for row in range(row_size):\n",
    "            new_matrix.append([elem-other.matrix[row][ind] for ind,elem in enumerate(self.matrix[row])])\n",
    "        return Matrix(new_matrix)\n",
    "\n",
    "    def simple_multiplication(self,A,B):\n",
    "        row_size,col_size = self.get_size()\n",
    "        new_matrix = [[0 for i in range(row_size)] for j in range(row_size)]\n",
    "        for i in range(row_size):\n",
    "            for k in range(row_size):\n",
    "                for j in range(row_size):\n",
    "                    new_matrix[i][j] += A[i][k] * B[k][j]\n",
    "        return Matrix(new_matrix)\n",
    "    \n",
    "    def __mul__(self,other):\n",
    "        row_size,col_size = self.get_size()\n",
    "        if row_size <= 2:\n",
    "            return self.simple_multiplication(self.matrix,other.matrix)\n",
    "        else:\n",
    "            new_size = row_size//2\n",
    "            A = [[0 for j in range(new_size)] for i in range(new_size)]\n",
    "            B = [[0 for j in range(new_size)] for i in range(new_size)]\n",
    "            C = [[0 for j in range(new_size)] for i in range(new_size)]\n",
    "            D = [[0 for j in range(new_size)] for i in range(new_size)]\n",
    "\n",
    "            E = [[0 for j in range(new_size)] for i in range(new_size)]\n",
    "            F = [[0 for j in range(new_size)] for i in range(new_size)]\n",
    "            G = [[0 for j in range(new_size)] for i in range(new_size)]\n",
    "            H = [[0 for j in range(new_size)] for i in range(new_size)]\n",
    "\n",
    "            for i in range(new_size):\n",
    "                for j in range(new_size):\n",
    "                    A[i][j] = self.matrix[i][j]\n",
    "                    B[i][j] = self.matrix[i][j+new_size]\n",
    "                    C[i][j] = self.matrix[i + new_size][j]\n",
    "                    D[i][j] = self.matrix[i + new_size][j + new_size]\n",
    "\n",
    "                    E[i][j] = other.matrix[i][j]\n",
    "                    F[i][j] = other.matrix[i][j+new_size]\n",
    "                    G[i][j] = other.matrix[i + new_size][j]\n",
    "                    H[i][j] = other.matrix[i + new_size][j + new_size]\n",
    "\n",
    "            A = Matrix(A)\n",
    "            B = Matrix(B)\n",
    "            C = Matrix(C)\n",
    "            D = Matrix(D)\n",
    "            E = Matrix(E)\n",
    "            F = Matrix(F)\n",
    "            G = Matrix(G)\n",
    "            H = Matrix(H)\n",
    "            \n",
    "            p1 = A*(F-H)\n",
    "            p2 = (A+B)*H\n",
    "            p3 = (C+D)*E\n",
    "            p4 = D*(G-E)\n",
    "            p5 = (A+D)*(E+H)\n",
    "            p6 = (B-D)*(G+H)\n",
    "            p7 = (A -C)*(E+F)\n",
    "\n",
    "            c11 = p5 + p4 - p2 + p6\n",
    "            c12 = p1 + p2\n",
    "            c21 = p3 + p4\n",
    "            c22 = p1+ p5 - p3 - p7\n",
    "\n",
    "            final = [[0 for j in range(row_size)] for i in range(row_size)]\n",
    "            for i in range(new_size):\n",
    "                for j in range(new_size):\n",
    "                    final[i][j] = c11.matrix[i][j]\n",
    "                    final[i][j+new_size] = c12.matrix[i][j]\n",
    "                    final[i + new_size][j] = c21.matrix[i][j]\n",
    "                    final[i + new_size][j + new_size] = c22.matrix[i][j]\n",
    "            return Matrix(final)\n",
    "            \n",
    "A = Matrix([[1,2,4], [3,4,7], [1,12,15]])\n",
    "B = Matrix([[2, 1, 6], [4, 3, 4], [1,1,1]])\n",
    "(A*B).pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invertability\n",
    "\n",
    "Now that we've seen how to multiply two matrices, a natural question is, can we invert a given matrix to recover the identity, if we could, then we'd have a succinct way to solve systems of equations!  It turns out, not all matrices are invertable, but some are!  And those that are make finding the solutions for a system of equations, extremely easy!  We need not go through all the row reduction stuff and we can still solve our equations.\n",
    "\n",
    "The notation for inverting a matrix is typically as follows:\n",
    "\n",
    "$$ A * A^{-1} = I $$\n",
    "\n",
    "Where A is some invertable matrix, $A^{-1}$ is it's inverse and $I$ is the identity matrix. \n",
    "\n",
    "Let's see how to obtain the inverse for a square matrix of size 2x2:\n",
    "\n",
    "$$\n",
    "  \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}^{-1}\n",
    "  =\n",
    "  \\frac{1}{ad - bc}\n",
    "  *\n",
    "  \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note, not all matrices are invertable (even square ones).  Those are called singular matrices.  You'll see this sort of error come up _a lot_ in machine learning when you are trying to fit a model to some data.\n",
    "\n",
    "## Linear Regression w/ Linear Algebra\n",
    "\n",
    "Now that we've come up with a set of tools for doing linear algebra, let's apply them to show how we can implement gradient descent with our new tools:\n",
    "\n",
    "Include http://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf specifically 1.2.1's treatment and motivation for why you can solve with matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate some data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X = 7 * np.random.rand(100,1)\n",
    "y = 9 + 4 * X+np.random.randn(100,1)\n",
    "\n",
    "plt.plot(X,y,'b.')\n",
    "plt.xlabel(\"$x$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "_ =plt.axis([0,5,6,40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5, 6, 40]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def linear_regression(X, y):\n",
    "    X_b = np.c_[np.ones((len(X),1)), X]\n",
    "    x_transpose_times_x = X_b.T.dot(X_b)\n",
    "    inverse = np.linalg.inv(x_transpose_times_x)\n",
    "    return inverse.dot(X_b.T).dot(y)\n",
    "    \n",
    "optimal_theta = linear_regression(X, y)\n",
    "X_new = np.array([[0],[5]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
    "y_predict = X_new_b.dot(optimal_theta)\n",
    "y_predict\n",
    "\n",
    "plt.plot(X_new,y_predict,'r-')\n",
    "plt.plot(X,y,'b.')\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0,5,6,40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see with the power of linear algebra our solution becomes much easier to understand!  However, this only works because of the assumptions of linear regression.  This allows for a closed for solution.  Hopefully, this shows you the power of linear regression!\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "### A naive explanation\n",
    "\n",
    "Neural networks in a sense really aren't anything new \"mathematically\".  All we are really doing is applying the chain rule over a vector space ( to do back propagation ).\n",
    "\n",
    "Let's see a simple example of chain rules:\n",
    "\n",
    "$$ f(x) = (log(x))^{2} $$\n",
    "\n",
    "To do the derivative we need to do:\n",
    "\n",
    "$$ f'(g(x)) = f'(g(x))* g'(x) $$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$ f'(x) = 2 * log(x) * 1/x $$\n",
    "\n",
    "So if our functions are over matrices then we are doing neural networks :D\n",
    "\n",
    "### Psuedo Code\n",
    "\n",
    "There are two main algorithms in neural networks:\n",
    "\n",
    "* forward propagation ( predict )\n",
    "* back propagation ( train )\n",
    "\n",
    "\n",
    "![](nn_psuedocode.png)\n",
    "\n",
    "[source](https://en.wikipedia.org/wiki/Artificial_neural_network)\n",
    "\n",
    "\n",
    "As you can see the basic idea is:\n",
    "\n",
    "* propagate data through a set of functions which are joined together via matrix multiplication\n",
    "\n",
    "* At the last layer look at how much your prediction was off by\n",
    "\n",
    "* update your weight matrix by the gradient * the error (this is a simplification)\n",
    "\n",
    "* do this until your error is lower than some tolerance\n",
    "\n",
    "\n",
    "## A Naive Implementation of a Neural Network\n",
    "\n",
    "Now that we understand the basic notions, let's go ahead and look at a simple example in Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1892145506579259\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class Dense:\n",
    "    def __init__(self, input_dim, output_dim, activation_function):\n",
    "        self.synapse = 2 * np.random.random((input_dim, output_dim)) - 1 \n",
    "        self.select_activation_function(activation_function)\n",
    "        \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def dtanh(self, y):\n",
    "        return 1 - y ** 2\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    def dsigmoid(self, y):\n",
    "        return y*(1-y)\n",
    "    \n",
    "    def select_activation_function(self, activation_function):\n",
    "        if activation_function == \"tanh\":\n",
    "            self.activation_function = self.tanh\n",
    "            self.activation_derivative = self.dtanh\n",
    "        if activation_function == \"sigmoid\":\n",
    "            self.activation_function = self.sigmoid\n",
    "            self.activation_derivative = self.dsigmoid\n",
    "\n",
    "    def forward(self, previous_layer):\n",
    "        self.output = self.activation_function(\n",
    "            previous_layer.dot(self.synapse)\n",
    "        )\n",
    "        return self.output\n",
    "                           \n",
    "    def compute_gradient(self, layer, error):\n",
    "        self.delta = error * self.activation_derivative(layer)\n",
    "        return self.delta.dot(self.synapse.T)\n",
    "\n",
    "    def prepare_for_multiplication(self, vector):\n",
    "        num_cols = len(vector)\n",
    "        num_rows = 1\n",
    "        return vector.reshape(num_rows, num_cols)\n",
    "\n",
    "    def update_weights(self, layer, learning_rate):\n",
    "        layer = self.prepare_for_multiplication(layer)\n",
    "        delta = self.prepare_for_multiplication(self.delta)\n",
    "        self.synapse += layer.T.dot(delta) * learning_rate\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, layers):\n",
    "        self.nn = layers\n",
    "        self.layers = []\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.layers = []\n",
    "        self.layers.append(X)\n",
    "        for index, synapse in enumerate(self.nn):\n",
    "            output = synapse.forward(self.layers[index])\n",
    "            self.layers.append(output)\n",
    "        return output\n",
    "\n",
    "    def backpropagate(self, error, learning_rate):\n",
    "        layers = list(reversed(self.layers))\n",
    "        nn = list(reversed(self.nn))\n",
    "        for index, layer in enumerate(layers[:-1]):\n",
    "            error = nn[index].compute_gradient(layer, error)\n",
    "\n",
    "        for index, synapse in enumerate(self.nn):\n",
    "            synapse.update_weights(self.layers[index], learning_rate)\n",
    "\n",
    "        \n",
    "class NeuralNetwork:\n",
    "    def __init__(self, learning_rate=0.1, target_mse=0.01, epochs=500):\n",
    "        self.layers = []\n",
    "        self.connections = []\n",
    "        self.network = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.target_mse = target_mse\n",
    "        self.epochs = epochs\n",
    "        self.errors = []\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def init_network(self):\n",
    "        self.network = Network(self.layers)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.init_network()\n",
    "        for epoch in range(self.epochs):\n",
    "            self.errors = []\n",
    "\n",
    "            rows, columns = X.shape\n",
    "            for index in range(rows):\n",
    "                # Forward\n",
    "                output = self.network.forward(X[index])\n",
    "                # Compute the error\n",
    "                error = y[index] - output\n",
    "                self.errors.append(error)\n",
    "\n",
    "                # Back-propagate the error\n",
    "                self.network.backpropagate(error, self.learning_rate)\n",
    "\n",
    "            mse = (np.array(self.errors) ** 2).mean()\n",
    "            if mse <= self.target_mse:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.network.forward(X)\n",
    "\n",
    "def data_generation_easy():\n",
    "    df = pd.DataFrame()\n",
    "    for _ in range(1000):\n",
    "        a = np.random.normal(0, 1)\n",
    "        b = np.random.normal(0, 3)\n",
    "        c = np.random.normal(12, 4)\n",
    "        if a + b + c > 11:\n",
    "            target = 1\n",
    "        else:\n",
    "            target = 0\n",
    "        df = df.append({\n",
    "            \"A\": a,\n",
    "            \"B\": b,\n",
    "            \"C\": c,\n",
    "            \"target\": target\n",
    "        }, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "df = data_generation_easy()\n",
    "column_names = [\"A\", \"B\", \"C\"]\n",
    "target_name = \"target\"\n",
    "X = df[column_names].values\n",
    "y = np.array([[elem] for elem in list(df[\"target\"].values)])\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "# Dense(inputs, outputs, activation)\n",
    "nn.add_layer(Dense(3, 20, \"tanh\"))\n",
    "nn.add_layer(Dense(20, 1, \"tanh\"))\n",
    "nn.fit(X, y)\n",
    "y_pred = nn.predict(X)\n",
    "print(mean_squared_error(y_pred, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our network\n",
    "\n",
    "As you can see from the above our naive neural network has three steps:\n",
    "\n",
    "* Initialization\n",
    "* forward propagation (guess)\n",
    "* back propagation (check)\n",
    "\n",
    "The initialization step sets up our neural network and the general architecture:\n",
    "\n",
    "```python\n",
    "\n",
    "class Dense:\n",
    "    def __init__(self, input_dim, output_dim, activation_function):\n",
    "        self.synapse = 2 * np.random.random((input_dim, output_dim)) - 1 \n",
    "        self.select_activation_function(activation_function)\n",
    "\n",
    "def init_network(self):\n",
    "    self.network = Network(self.layers)\n",
    "\n",
    "def add_layer(self, layer):\n",
    "    self.layers.append(layer)\n",
    " \n",
    "nn.add_layer(Dense(3, 20, \"tanh\"))\n",
    "nn.add_layer(Dense(20, 1, \"tanh\"))\n",
    "```\n",
    "\n",
    "Creating a connection really means creating a randomly initialized matrix whose values will update over the course of back propagation.  We can see this in the `__init__` function in the `Dense` object.  The `init_network` function initializes the network once all the layers have been added.\n",
    "\n",
    "As you can see, the shapes of our matrices will depend on the previous layer connecting to it - this is because we can only do matrix multiplication with composable shapes.\n",
    "\n",
    "Our so called layers are actually the weight matrices that we will use to join each layer.\n",
    "\n",
    "## Forward Propagation\n",
    "\n",
    "Here we'll look at forward propagation through the network.  Forward propagation was a major innovation in the space of neural networks.  With it, we can compose layers together, creating vast and deep networks that can learn so much from the data.  Let's explore a naive implementation:\n",
    "\n",
    "```python\n",
    "def fit(self, X, y):\n",
    "    self.init_network()\n",
    "    for epoch in range(self.epochs):\n",
    "        self.errors = []\n",
    "\n",
    "        rows, columns = X.shape\n",
    "        for index in range(rows):\n",
    "            # Forward\n",
    "            output = self.network.forward(X[index])\n",
    "            \n",
    "# in Network Object\n",
    "def forward(self, X):\n",
    "    self.layers = []\n",
    "    self.layers.append(X)\n",
    "    for index, synapse in enumerate(self.nn):\n",
    "        output = synapse.forward(self.layers[index])\n",
    "        self.layers.append(output)\n",
    "    return output\n",
    "\n",
    "# in Dense Object\n",
    "def forward(self, previous_layer):\n",
    "    self.output = self.activation_function(\n",
    "        previous_layer.dot(self.synapse)\n",
    "    )\n",
    "    return self.output\n",
    "```\n",
    "\n",
    "forward propagation is called through the fit method.  In the network object, you can see synapses - which are the weight matrices of the network, being composed with the existing layers.  Notice that the weight matrices are the important thing for training the network, we simply discard the layers at each call of forward propagation.  \n",
    "\n",
    "The layers are actually composed using the final forward method being shown above.  First, the dot product of the previous layer and the current synapse are taken and then an activation function is applied.  We can think of this operation as a non-linear transformation.  This is because a linear transformation is applied via the dot product and then a (typically) non-linear transformation is applied to that dot product.  It is possible to also use a linear transformation in as the activation function, but this means that the neural network will be unable to learn non-linear decision boundaries and therefore should be used with caution.  \n",
    "\n",
    "Stepping back a bit, we can see forward propagation as a composition of layers via a non-linear transformation, where the layers on the connections and matrix multiplication and an activation function are used to update the weights across the network.  These operations are how information and decisions flow across the network.  We can see each layer as another opportunity to learn something, to update our beliefs about the world.  Also as an opportunity to throw away knowledge - this is primarily what the non-linearity does.  In this way, we can constrain ourselves to learning only the relevant information.  \n",
    "\n",
    "As an aside, the size of a network determines how much it can learn, based on the above explaination.  As the size of the network grows it can learn more and more from the data.  This is obvious from the fact that there will be more decision makers per loop through each element of the data.  Additionally, as the size of our data grows, we will have more examples to learn from - which is why neural networks improve with more data.  So there are two tools for learning a better representation of your underlying process or distribution - build a bigger network or obtain more data.  Of course, there is a limit to how much this will improve things.  At some point, as the network grows, overfitting occurs, and the network merely memorizes the data.  \n",
    "\n",
    "## Back Propagation\n",
    "\n",
    "Back propagation is the second large innovation in neural networks we will discuss.  The way a neural network is trained, via back propagation means the interplay between the final output and error has a sophisticated and complex relationship with the weight space.  Because the weights at the end of the neural network, influence the nodes at the beginning of the neural network when error is propagated backwards. \n",
    "\n",
    "```python\n",
    "def fit(self, X, y):\n",
    "    self.init_network()\n",
    "    for epoch in range(self.epochs):\n",
    "        self.errors = []\n",
    "\n",
    "        rows, columns = X.shape\n",
    "        for index in range(rows):\n",
    "            # Forward\n",
    "            output = self.network.forward(X[index])\n",
    "            # Compute the error\n",
    "            error = y[index] - output\n",
    "            self.errors.append(error)\n",
    "\n",
    "            # Back-propagate the error\n",
    "            self.network.backpropagate(error, self.learning_rate)\n",
    "\n",
    "# in Network Object\n",
    "def backpropagate(self, error, learning_rate):\n",
    "    layers = list(reversed(self.layers))\n",
    "    nn = list(reversed(self.nn))\n",
    "    for index, layer in enumerate(layers[:-1]):\n",
    "        error = nn[index].compute_gradient(layer, error)\n",
    "\n",
    "    for index, synapse in enumerate(self.nn):\n",
    "        synapse.update_weights(self.layers[index], learning_rate)\n",
    "\n",
    "# in Dense Object\n",
    "def compute_gradient(self, layer, error):\n",
    "    self.delta = error * self.activation_derivative(layer)\n",
    "    return self.delta.dot(self.synapse.T)\n",
    "\n",
    "def prepare_for_multiplication(self, vector):\n",
    "    num_cols = len(vector)\n",
    "    num_rows = 1\n",
    "    return vector.reshape(num_rows, num_cols)\n",
    "\n",
    "def update_weights(self, layer, learning_rate):\n",
    "    layer = self.prepare_for_multiplication(layer)\n",
    "    delta = self.prepare_for_multiplication(self.delta)\n",
    "    self.synapse += layer.T.dot(delta) * learning_rate        \n",
    "```\n",
    "\n",
    "As you can see back propagation takes on two stages:\n",
    "\n",
    "* computing the gradient\n",
    "* updating the weights\n",
    "\n",
    "When computing the gradient the deltas, defined as the `error * activation_derivative(layer)`, are composed with the synapses via a dot product.  For clarity, the `activation_derivative` is the derivative of the activation function applied to the given layer.  We can think of a delta as the distance in the decision from an optimal decision.  \n",
    "\n",
    "Of course, we don't know what an optimal decision looks like, however the error in the final layer from ground truth, calculated as `error = y[index] - output` in the `fit` function, gives us some notion of optimality.  Essentially, what this tells us is, we don't know all the nuance or pieces of information that lead from the data to the target.  \n",
    "\n",
    "After the gradient has been computed, the weights are updated based on the dot product of the layer and delta.  We can think of this as trying to move the approximate amount we were off by, now in the correct direction.  The synapse or weight matrices, store the new more likely decision.  The learning rate we see there, is a discount factor, because any new information show always been taken with a grain of salt.  \n",
    "\n",
    "In general, what the neural network tries to do, is take the data its given and the final decision that's produced and try to recreate the thought process that led to the underlying decision.  And it tries to do this in mathematically certain terms.  We can think of the gradient at each layer of the network, which we can think of as a layer of nuance, as telling us directionally guiding us towards a correct decision.  \n",
    "\n",
    "One of the issues with this way of thinking about how to get closer to how a decision is made, is that the underlying curve of that gradient may be complex.  There may be lots of dead ends or wrong turns that the network takes, when trying to understand the underlying process.  This is why many examples are needed to really be sure you are moving in the right direction.  We can think of each row in our dataset as a piece of evidence and our network is like Sherlock Holmes.  It gather evidence, compares theories, and finally distills information down, into the most fitting explaination, no matter how improbable it appears at first glance.\n",
    "\n",
    "## Issues in Neural Networks\n",
    "\n",
    "There are two main types of issues with respect to neural networks:\n",
    "\n",
    "* vanishing gradients\n",
    "* exploding gradients\n",
    "\n",
    "### Vanishing Gradient\n",
    "\n",
    "The vanishing gradient problem is one of early stopping.  During back propagation the gradients can become increadibly small, due to the following simple fact:\n",
    "\n",
    "When you multiple two small numbers they get very small very fast:\n",
    "\n",
    "Example: \n",
    "\n",
    "`0.0005 * 0.0007 = 0.00000035`\n",
    "\n",
    "So essentially what happens is, if the gradients ever head to an increasingly smaller value they will get extremely close to zero very fast.  \n",
    "\n",
    "Unfortunately, the termination condition for training a neural network is similar to what happens when the gradient gets very small, so it's hard to tell if this is because the neural network has learned something or because the neural network accidentaly learned a weight early on that was small, which propagated to the whole system and led to early stopping\n",
    "\n",
    "### Exploding Gradient\n",
    "\n",
    "The exploding gradient has the opposite problem, it's when the gradients become very big very fast, because multiplying two big numbers, also creates a very big number very fast.  If that happens, it's unlikely the neural network will ever converge.\n",
    "\n",
    "Both are issues, but explodiung gradients, luckily are super obvious because there is only one reason why that would happen.  That's why we only train a neural network for a fixed number of epochs.  This way, if indeed we do have a run away gradient, we'll know it happened and won't waste time training forever.\n",
    "\n",
    "\n",
    "## Introduction to Keras\n",
    "\n",
    "Now that we've seen how to work with the machinery of neural networks, let's start making use of our first library.  For this tutorial we will make use of Keras a great high level api for neural networks.\n",
    "\n",
    "Let's start by looking at a simple regression example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.056991027049713\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=6, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation=\"linear\"))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "crashes = sns.load_dataset(\"car_crashes\")\n",
    "y = crashes[\"total\"]\n",
    "X = crashes[[\"speeding\", \"alcohol\", \"not_distracted\", \"no_previous\", \"ins_premium\", \"ins_losses\"]]\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
    "estimator.fit(X, y)\n",
    "y_pred = estimator.predict(X)\n",
    "print(metrics.mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, the following:\n",
    "\n",
    "1. we need to set the number of input dimensions to be the same as the number of columns in our X variable.\n",
    "2. we specify the loss in the compile phase as mean squared error.  With Neural networks we can even choose the loss metric!\n",
    "\n",
    "Our model does pretty good!  It's worth noting that typically you should break things out into test and train sets, but more on that in a later chapter.  \n",
    "\n",
    "Notice that the API for keras is very similar to the one we built - we build up `Dense` objects and add them to the overall model.  The big difference here is the output dimension comes first, and we need not specify an input dimension, because the keras API is smart enough to figure out your input dimension from your previous layer!!!  Which is pretty awesome.  \n",
    "\n",
    "Finally, we have to \"compile\" our model vai `model.compile` so that all of our layers work together in the final model.  Here we get to choose our optimizer and loss function.  For regression problems mean squared error is typical, although other loss functions may make sense for your use case.  This will be described in detail in a later section.\n",
    "\n",
    "Finally let's look at how we make use of our model:\n",
    "\n",
    "```python\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
    "estimator.fit(X, y)\n",
    "y_pred = estimator.predict(X)\n",
    "```\n",
    "\n",
    "This code should look pretty familar - keras comes with a scikit-learn wrapper object so we can use our keras model the same way we would use any scikit-learn model - by calling fit and predict methods.  The uniformity of this api means we can easily integrate neural networks into all our existing preprocessing, postprocessing and data visualization tools for scikit-learn, just like any other model.\n",
    "\n",
    "Let's see a simple example of classification before we move onto interpreting our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       1.00      0.96      0.98        50\n",
      "           2       0.96      1.00      0.98        50\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       150\n",
      "   macro avg       0.99      0.99      0.99       150\n",
      "weighted avg       0.99      0.99      0.99       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "data = load_iris()\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "estimator.fit(data.data, data.target)\n",
    "y_pred = estimator.predict(data.data)\n",
    "print(metrics.classification_report(data.target, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, unsurprisingly our model does quiet well!  But remember we shouldn't get too excited about this.  \n",
    "\n",
    "Some important things to note as for differences between classification and regression:\n",
    "\n",
    "1. The use of softmax in the final layer as apposed to linear for the activation function\n",
    "2. The use of categorical cross entropy as apposed to mean squared error for loss\n",
    "3. The use of the KerasClassifier object as apposed to the KerasRegressor object for object wrapping\n",
    "\n",
    "Other than that, pretty much everything is the same.\n",
    "\n",
    "Now let's move onto understanding the deeper points of building up your neural network architecture and talk through some common paradigms.  First we'll look at how to do this for regression and then classification.\n",
    "\n",
    "## Neural Network Architecture\n",
    "\n",
    "Now that we have the basics down, let's go over some of the architectural design elements of a neural network.  There are a few major components to tune:\n",
    "\n",
    "* breadth of the network - how many neurons per layer\n",
    "* the depth of the network - how many layers\n",
    "* choosing the right activation function\n",
    "* choosing how to split up the network - creating shared layers\n",
    "* choosing the right weight initialization\n",
    "* choosing the right loss function\n",
    "* choosing the right optimizer\n",
    "* choosing the right metric\n",
    "\n",
    "First let's look at a simple example network and record the metrics and then see how the model performance changes as we vary the above parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       185\n",
      "           1       0.49      1.00      0.66       180\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       365\n",
      "   macro avg       0.25      0.50      0.33       365\n",
      "weighted avg       0.24      0.49      0.33       365\n",
      "\n",
      "0.4931506849315068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=10))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "random.seed(1)\n",
    "# data found here: https://hackernoon.com/build-your-first-neural-network-to-predict-house-prices-with-keras-3fb0839680f4\n",
    "df = pd.read_csv(\"housepricedata.csv\")\n",
    "X = df.values[:,0:10]\n",
    "y = df.values[:,10]\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred = estimator.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the basic model setup for a model.  We have an input layer which has to have the same number of input dimensions as the number of columns in the independent variables, usually called.  And then we have an output layer which has to have an output dimension equal to the number of output possibilities.  For regression there should be one neuron in the final output, for classification there should be as many classes as the target variable.\n",
    "\n",
    "Our model does okay, let's see if we can do better simply by increasing the number of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      1.00      0.63       166\n",
      "           1       0.00      0.00      0.00       199\n",
      "\n",
      "   micro avg       0.45      0.45      0.45       365\n",
      "   macro avg       0.23      0.50      0.31       365\n",
      "weighted avg       0.21      0.45      0.28       365\n",
      "\n",
      "0.4547945205479452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=10))\n",
    "    model.add(Dense(10, input_dim=10))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "random.seed(1)\n",
    "# data found here: https://hackernoon.com/build-your-first-neural-network-to-predict-house-prices-with-keras-3fb0839680f4\n",
    "df = pd.read_csv(\"housepricedata.csv\")\n",
    "X = df.values[:,0:10]\n",
    "y = df.values[:,10]\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred = estimator.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we get some marginal improvement from increasing the number of layers.  Let's keep going to see if we continue to get improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       174\n",
      "           1       0.52      1.00      0.69       191\n",
      "\n",
      "   micro avg       0.52      0.52      0.52       365\n",
      "   macro avg       0.26      0.50      0.34       365\n",
      "weighted avg       0.27      0.52      0.36       365\n",
      "\n",
      "0.5232876712328767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=10))\n",
    "    model.add(Dense(10, input_dim=10))\n",
    "    model.add(Dense(10, input_dim=10))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "random.seed(1)\n",
    "# data found here: https://hackernoon.com/build-your-first-neural-network-to-predict-house-prices-with-keras-3fb0839680f4\n",
    "df = pd.read_csv(\"housepricedata.csv\")\n",
    "X = df.values[:,0:10]\n",
    "y = df.values[:,10]\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred = estimator.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some performance increase in some more categories.  Let's try adding a ton more layers to see if that continues to increase the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      1.00      0.65       176\n",
      "           1       0.00      0.00      0.00       189\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       365\n",
      "   macro avg       0.24      0.50      0.33       365\n",
      "weighted avg       0.23      0.48      0.31       365\n",
      "\n",
      "0.4821917808219178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def baseline_model(n):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=10))\n",
    "    for i in range(n):\n",
    "        model.add(Dense(10, input_dim=10))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "random.seed(1)\n",
    "# data found here: https://hackernoon.com/build-your-first-neural-network-to-predict-house-prices-with-keras-3fb0839680f4\n",
    "df = pd.read_csv(\"housepricedata.csv\")\n",
    "X = df.values[:,0:10]\n",
    "y = df.values[:,10]\n",
    "deep_model = partial(baseline_model, 20)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)\n",
    "estimator = KerasClassifier(build_fn=deep_model, epochs=100, batch_size=5, verbose=0)\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred = estimator.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our model performance has stopped increasing.  This may indicate that we have now reached a point of overfitting and therefore model performance won't continue to increase.  This speaks to a larger point - model performance won't continue to increase by just making our model deeper.  Let's take this knowledge add create a naive auto-ml function to optimize the depth of our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hidden layers 0\n",
      "Number of hidden layers 1\n",
      "Number of hidden layers 2\n",
      "Number of hidden layers 3\n",
      "Number of hidden layers 4\n",
      "Optimal number of hidden layers 2\n",
      "Optimal accuracy 0.5024657534246575\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import random\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def baseline_model(n):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=10))\n",
    "    for i in range(n):\n",
    "        model.add(Dense(10, input_dim=10))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def automl_basic(X_train, X_test, y_train, y_test, baseline, max_layers, num_runs = 3):\n",
    "    accuracy_scores = defaultdict(list)\n",
    "    for layers in range(max_layers):\n",
    "        print(\"Number of hidden layers\", layers)\n",
    "        for i in range(num_runs):\n",
    "            deep_model = partial(baseline, layers)\n",
    "            estimator = KerasClassifier(build_fn=deep_model, epochs=100, batch_size=5, verbose=0)\n",
    "            estimator.fit(X_train, y_train)\n",
    "            y_pred = estimator.predict(X_test)\n",
    "            accuracy_scores[layers].append(metrics.accuracy_score(y_test, y_pred))\n",
    "    return accuracy_scores\n",
    "\n",
    "def choose_best_model(accuracy_scores, max_layers):\n",
    "    best_acc = 0\n",
    "    best_hidden_layers = 0\n",
    "    for layers in range(max_layers):\n",
    "        cur_acc = np.mean(accuracy_scores[layers])\n",
    "        if cur_acc > best_acc:\n",
    "            best_acc = cur_acc\n",
    "            best_hidden_layers = layers\n",
    "    return best_acc, best_hidden_layers\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "df = pd.read_csv(\"housepricedata.csv\")\n",
    "X = df.values[:,0:10]\n",
    "y = df.values[:,10]\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)\n",
    "max_layers = 5\n",
    "accuracy_scores = automl_basic(X_train, X_test, y_train, y_test, baseline_model, max_layers, num_runs=5)\n",
    "best_acc, best_hidden_layers = choose_best_model(accuracy_scores, max_layers)\n",
    "print(\"Optimal number of hidden layers\", best_hidden_layers)\n",
    "print(\"Optimal accuracy\", best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the optimal number of layers is 2.  Unfortunately our naive implementation does take a while to run, so if you want to verify the consistency here, proceed with caution.\n",
    "\n",
    "Now that we've looked at increasing the depth of our network, let's look at increasing the breadth.  For this set of experiments, will simply use a single hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      1.00      0.66       178\n",
      "           1       0.00      0.00      0.00       187\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       365\n",
      "   macro avg       0.24      0.50      0.33       365\n",
      "weighted avg       0.24      0.49      0.32       365\n",
      "\n",
      "0.4876712328767123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def baseline_model(n):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n, input_dim=10))\n",
    "    model.add(Dense(n))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "random.seed(1)\n",
    "# data found here: https://hackernoon.com/build-your-first-neural-network-to-predict-house-prices-with-keras-3fb0839680f4\n",
    "df = pd.read_csv(\"housepricedata.csv\")\n",
    "X = df.values[:,0:10]\n",
    "y = df.values[:,10]\n",
    "broad_model = partial(baseline_model, 10)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)\n",
    "estimator = KerasClassifier(build_fn=broad_model, epochs=100, batch_size=5, verbose=0)\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred = estimator.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our baseline, let's see if we can improve upon that by adding more neurons in our single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       185\n",
      "           1       0.49      1.00      0.66       180\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       365\n",
      "   macro avg       0.25      0.50      0.33       365\n",
      "weighted avg       0.24      0.49      0.33       365\n",
      "\n",
      "0.4931506849315068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def baseline_model(n):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n, input_dim=10))\n",
    "    model.add(Dense(n))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "random.seed(1)\n",
    "# data found here: https://hackernoon.com/build-your-first-neural-network-to-predict-house-prices-with-keras-3fb0839680f4\n",
    "df = pd.read_csv(\"housepricedata.csv\")\n",
    "X = df.values[:,0:10]\n",
    "y = df.values[:,10]\n",
    "broad_model = partial(baseline_model, 15)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)\n",
    "estimator = KerasClassifier(build_fn=broad_model, epochs=100, batch_size=5, verbose=0)\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred = estimator.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we got a small improvement!  Let's try doing the same automl game, except this time for breadth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hidden layers 8\n",
      "Number of hidden layers 9\n",
      "Number of hidden layers 10\n",
      "Number of hidden layers 11\n",
      "Number of hidden layers 12\n",
      "Optimal number of neurons 12\n",
      "Optimal accuracy 0.5205479452054794\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import random\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def baseline_model(n):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n, input_dim=10))\n",
    "    model.add(Dense(n))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def automl_basic(X_train, X_test, y_train, y_test, baseline, min_neurons, max_neurons, num_runs = 5):\n",
    "    accuracy_scores = defaultdict(list)\n",
    "    for neurons in range(min_neurons, max_neurons):\n",
    "        print(\"Number of hidden layers\", neurons)\n",
    "        for i in range(num_runs):\n",
    "            deep_model = partial(baseline, neurons)\n",
    "            estimator = KerasClassifier(build_fn=deep_model, epochs=100, batch_size=5, verbose=0)\n",
    "            estimator.fit(X_train, y_train)\n",
    "            y_pred = estimator.predict(X_test)\n",
    "            accuracy_scores[neurons].append(metrics.accuracy_score(y_test, y_pred))\n",
    "    return accuracy_scores\n",
    "\n",
    "def choose_best_model(accuracy_scores, min_neurons, max_neurons):\n",
    "    best_acc = 0\n",
    "    best_neurons = 0\n",
    "    for neurons in range(min_neurons, max_neurons):\n",
    "        cur_acc = np.mean(accuracy_scores[neurons])\n",
    "        if cur_acc > best_acc:\n",
    "            best_acc = cur_acc\n",
    "            best_neurons = neurons\n",
    "    return best_acc, best_neurons\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "df = pd.read_csv(\"housepricedata.csv\")\n",
    "X = df.values[:,0:10]\n",
    "y = df.values[:,10]\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)\n",
    "min_neurons = 8\n",
    "max_neurons = 13 \n",
    "accuracy_scores = automl_basic(X_train, X_test, y_train, y_test, baseline_model, min_neurons, max_neurons, num_runs=2)\n",
    "best_acc, best_neurons = choose_best_model(accuracy_scores, min_neurons, max_neurons)\n",
    "print(\"Optimal number of neurons\", best_neurons)\n",
    "print(\"Optimal accuracy\", best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like 12 does the best!  Finally, let's see what happens when we combine the two approaches together.  Using this will get a sense of the optimal capacity of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import random\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "def baseline_model(neurons, layers):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=10))\n",
    "    for _ in range(layers):\n",
    "        model.add(Dense(neurons))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def automl_basic(X_train, X_test, y_train, y_test, baseline, min_neurons, max_neurons, max_layers, num_runs = 3):\n",
    "    accuracy_scores = defaultdict(list)\n",
    "    for layers_neurons in itertools.product(range(max_layers), range(min_neurons, max_neurons)):\n",
    "        layers = layers_neurons[0]\n",
    "        neurons = layers_neurons[1]\n",
    "        print(\"Number of hidden layers\", layers)\n",
    "        for i in range(num_runs):\n",
    "            deep_broad_model = partial(baseline, neurons, layers)\n",
    "            estimator = KerasClassifier(build_fn=deep_broad_model, epochs=100, batch_size=5, verbose=0)\n",
    "            estimator.fit(X_train, y_train)\n",
    "            y_pred = estimator.predict(X_test)\n",
    "            accuracy_scores[layers_neurons].append(metrics.accuracy_score(y_test, y_pred))\n",
    "    return accuracy_scores\n",
    "\n",
    "def choose_best_model(accuracy_scores, min_neurons, max_neurons, max_layers):\n",
    "    best_acc = 0\n",
    "    best_layers = 0\n",
    "    best_neurons = 0\n",
    "    for layers_neurons in itertools.product(range(max_layers), range(min_neurons, max_neurons)):\n",
    "        cur_acc = np.mean(accuracy_scores[layers_neurons])\n",
    "        if cur_acc > best_acc:\n",
    "            best_acc = cur_acc\n",
    "            best_layers = layers_neurons[0]\n",
    "            best_neurons = layers_neurons[1]\n",
    "    return best_acc, best_layers, best_neurons\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "df = pd.read_csv(\"housepricedata.csv\")\n",
    "X = df.values[:,0:10]\n",
    "y = df.values[:,10]\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)\n",
    "min_neurons = 8\n",
    "max_neurons = 13\n",
    "max_layers = 10\n",
    "accuracy_scores = automl_basic(X_train, X_test, y_train, y_test, baseline_model, min_neurons, max_neurons, max_layers, num_runs=2)\n",
    "best_acc, best_layers, best_neurons = choose_best_model(accuracy_scores, min_neurons, max_neurons, max_layers)\n",
    "print(\"Optimal number of hidden layers\", best_layers)\n",
    "print(\"Optimal number of neurons per layer\", best_neurons)\n",
    "print(\"Optimal accuracy\", best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the code is pretty slow on a single computer I had to run it on a cluster, you can see the results below:\n",
    "```\n",
    "Optimal number of hidden layers 0\n",
    "Optimal number of neurons per layer 11\n",
    "Optimal accuracy 0.5041095890410959\n",
    "```\n",
    "## Exploring Different Models\n",
    "\n",
    "So far we've looked at models where the number of neurons per layer have been constant.  Now we'll look at a few ideas for changing the number of neurons per layer.  First we'll look at a \"pyramid\" network where the number of neurons is large at the start of the network and the number of neurons gets small towards the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       163\n",
      "           1       0.55      1.00      0.71       202\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       365\n",
      "   macro avg       0.28      0.50      0.36       365\n",
      "weighted avg       0.31      0.55      0.39       365\n",
      "\n",
      "0.5534246575342465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import random\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=10))\n",
    "    model.add(Dense(5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "random.seed(1)\n",
    "# data found here: https://hackernoon.com/build-your-first-neural-network-to-predict-house-prices-with-keras-3fb0839680f4\n",
    "df = pd.read_csv(\"housepricedata.csv\")\n",
    "X = df.values[:,0:10]\n",
    "y = df.values[:,10]\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred = estimator.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we get a performance boost just by changing the structure of the neurons.  Let's see if we can do better by making the network deeper, by keeping the same general structure: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67       184\n",
      "           1       0.00      0.00      0.00       181\n",
      "\n",
      "   micro avg       0.50      0.50      0.50       365\n",
      "   macro avg       0.25      0.50      0.34       365\n",
      "weighted avg       0.25      0.50      0.34       365\n",
      "\n",
      "0.5041095890410959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import random\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=10))\n",
    "    model.add(Dense(8))\n",
    "    model.add(Dense(5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "random.seed(1)\n",
    "# data found here: https://hackernoon.com/build-your-first-neural-network-to-predict-house-prices-with-keras-3fb0839680f4\n",
    "df = pd.read_csv(\"housepricedata.csv\")\n",
    "X = df.values[:,0:10]\n",
    "y = df.values[:,10]\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred = estimator.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this has diminishing returns.  Next we will look at a \"christmas tree\" network that starts out thin and then gets fat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       195\n",
      "           1       0.47      1.00      0.64       170\n",
      "\n",
      "   micro avg       0.47      0.47      0.47       365\n",
      "   macro avg       0.23      0.50      0.32       365\n",
      "weighted avg       0.22      0.47      0.30       365\n",
      "\n",
      "0.4657534246575342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import random\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=10))\n",
    "    model.add(Dense(3))\n",
    "    model.add(Dense(5))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "random.seed(1)\n",
    "# data found here: https://hackernoon.com/build-your-first-neural-network-to-predict-house-prices-with-keras-3fb0839680f4\n",
    "df = pd.read_csv(\"housepricedata.csv\")\n",
    "X = df.values[:,0:10]\n",
    "y = df.values[:,10]\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred = estimator.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the christmas tree model does somewhat worse.  Next we'll look at increasing and decreasing the number of nodes over the layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.02       175\n",
      "           1       0.52      1.00      0.69       190\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       365\n",
      "   macro avg       0.76      0.51      0.35       365\n",
      "weighted avg       0.75      0.53      0.37       365\n",
      "\n",
      "0.5260273972602739\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import random\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=10))\n",
    "    model.add(Dense(5))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Dense(5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "random.seed(1)\n",
    "# data found here: https://hackernoon.com/build-your-first-neural-network-to-predict-house-prices-with-keras-3fb0839680f4\n",
    "df = pd.read_csv(\"housepricedata.csv\")\n",
    "X = df.values[:,0:10]\n",
    "y = df.values[:,10]\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred = estimator.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this increasing decreasing network does really well on the data!  Let's see if increasing the depth can be optmized.  For this we'll make use of our automl toolset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hidden layers 0\n",
      "Number of hidden layers 1\n",
      "Number of hidden layers 2\n",
      "Number of hidden layers 3\n",
      "Number of hidden layers 4\n",
      "Number of hidden layers 5\n",
      "Number of hidden layers 6\n",
      "Optimal number of hidden layers 1\n",
      "Optimal accuracy 0.5024657534246575\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import random\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def baseline_model(n):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=10))\n",
    "    model.add(Dense(5))\n",
    "    for i in range(n):\n",
    "        model.add(Dense(10))\n",
    "        model.add(Dense(5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def automl_basic(X_train, X_test, y_train, y_test, baseline, max_layers, num_runs = 3):\n",
    "    accuracy_scores = defaultdict(list)\n",
    "    for layers in range(max_layers):\n",
    "        print(\"Number of hidden layers\", layers)\n",
    "        for i in range(num_runs):\n",
    "            deep_model = partial(baseline, layers)\n",
    "            estimator = KerasClassifier(build_fn=deep_model, epochs=100, batch_size=5, verbose=0)\n",
    "            estimator.fit(X_train, y_train)\n",
    "            y_pred = estimator.predict(X_test)\n",
    "            accuracy_scores[layers].append(metrics.accuracy_score(y_test, y_pred))\n",
    "    return accuracy_scores\n",
    "\n",
    "def choose_best_model(accuracy_scores, max_layers):\n",
    "    best_acc = 0\n",
    "    best_hidden_layers = 0\n",
    "    for layers in range(max_layers):\n",
    "        cur_acc = np.mean(accuracy_scores[layers])\n",
    "        if cur_acc > best_acc:\n",
    "            best_acc = cur_acc\n",
    "            best_hidden_layers = layers\n",
    "    return best_acc, best_hidden_layers\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "df = pd.read_csv(\"housepricedata.csv\")\n",
    "X = df.values[:,0:10]\n",
    "y = df.values[:,10]\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)\n",
    "max_layers = 7\n",
    "accuracy_scores = automl_basic(X_train, X_test, y_train, y_test, baseline_model, max_layers, num_runs=5)\n",
    "best_acc, best_hidden_layers = choose_best_model(accuracy_scores, max_layers)\n",
    "print(\"Optimal number of hidden layers\", best_hidden_layers)\n",
    "print(\"Optimal accuracy\", best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see as we continue to add layers the model performance does not improve.  So we can see there are diminishing returns to this pattern.  Next let's look at another pattern where the network starts off broad, goes narrow and then goes broad again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      1.00      0.68       186\n",
      "           1       0.00      0.00      0.00       179\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       365\n",
      "   macro avg       0.25      0.50      0.34       365\n",
      "weighted avg       0.26      0.51      0.34       365\n",
      "\n",
      "0.5095890410958904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import random\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=10))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Dense(100))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "random.seed(1)\n",
    "# data found here: https://hackernoon.com/build-your-first-neural-network-to-predict-house-prices-with-keras-3fb0839680f4\n",
    "df = pd.read_csv(\"housepricedata.csv\")\n",
    "X = df.values[:,0:10]\n",
    "y = df.values[:,10]\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred = estimator.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, by constraining the network in another way we get to 100% recall for the zero label.  Now let's see if we can combine two networks to increase performance.  In order to do this we'll make use of the keras functional API and make use of shared layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      1.00      0.65       176\n",
      "           1       0.00      0.00      0.00       189\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       365\n",
      "   macro avg       0.24      0.50      0.33       365\n",
      "weighted avg       0.23      0.48      0.31       365\n",
      "\n",
      "0.4821917808219178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "def baseline_model():\n",
    "    # first input model\n",
    "    visible = Input(shape=(10,))\n",
    "    dense11 = Dense(10)(visible)\n",
    "    dense12 = Dense(5)(dense11)\n",
    "    dense13 = Dense(10)(dense12)\n",
    "    dense14 = Dense(5)(dense13)\n",
    "\n",
    "    # second input model\n",
    "    dense21 = Dense(100)(visible)\n",
    "    dense22 = Dense(1)(dense21)\n",
    "    dense23 = Dense(1)(dense22)\n",
    "    dense24 = Dense(100)(dense23)\n",
    "    # merge input models\n",
    "    merge = concatenate([dense14, dense24])\n",
    "    output = Dense(2, activation='sigmoid')(merge)\n",
    "\n",
    "    model = Model(inputs=visible, \n",
    "                  outputs=output) \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "random.seed(1)\n",
    "# data found here: https://hackernoon.com/build-your-first-neural-network-to-predict-house-prices-with-keras-3fb0839680f4\n",
    "df = pd.read_csv(\"housepricedata.csv\")\n",
    "X = df.values[:,0:10]\n",
    "y = df.values[:,10]\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred = estimator.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the functional API doesn't yield results that are any better.  Now that we've explored a number of architectures, let's next look at activation functions which can help constrain our data to make the decision boundary clearer.\n",
    "\n",
    "## Activation Functions\n",
    "\n",
    "As we saw in our naive implementation these activation functions are fairly simple.  Let's look at some naive implementations of each of these activations to look for common ground.  Then we'll make use of activation functions in a myriad of ways to attempt to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, constant=3):\n",
    "    return x*constant\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / sum(np.exp(x))\n",
    "\n",
    "def leaky_relu(x, alpha):\n",
    "    return [alpha*elem if elem < 0 else elem for elem in x]\n",
    "\n",
    "def elu(x, alpha):\n",
    "    return [alpha * np.exp(elem) - 1 \n",
    "            if elem <= 0 else elem \n",
    "            for elem in x]\n",
    "\n",
    "def relu(x):\n",
    "    return [0 if elem <= 0 else elem \n",
    "            for elem in x]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the standard activation functions folks tend to use\n",
    "\n",
    "Are there are any common features of the functions we've defined?\n",
    "\n",
    "The Leaky RELU, the ELU, and the RELU all have a ternary operator.\n",
    "\n",
    "Read these:\n",
    "* https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\n",
    "* https://en.wikipedia.org/wiki/Activation_function\n",
    "* https://en.wikipedia.org/wiki/Perceptron\n",
    "* https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization\n",
    "\n",
    "Read these:\n",
    "* https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78\n",
    "* https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94\n",
    "* https://stats.stackexchange.com/questions/47590/what-are-good-initial-weights-in-a-neural-network\n",
    "* https://isaacchanghau.github.io/post/weight_initialization/\n",
    "* http://cs231n.github.io/neural-networks-2/\n",
    "https://arxiv.org/abs/1704.08863"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surogate Models\n",
    "\n",
    "## SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "* https://datascience.stackexchange.com/questions/10000/what-is-the-difference-between-a-dynamic-bayes-network-and-a-hmm\n",
    "* https://machinelearningmastery.com/keras-functional-api-deep-learning/\n",
    "* data found here: https://hackernoon.com/build-your-first-neural-network-to-predict-house-prices-with-keras-3fb0839680f4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
