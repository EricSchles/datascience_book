{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83cc6741",
   "metadata": {},
   "source": [
    "# Low Level Neural Network Libraries\n",
    "\n",
    "## Table Of Contents\n",
    "\n",
    "* Tensorflow Low Level Api\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Now that we've gone through how to build a neural network from scratch, it's time to learn how to use frameworks to program neural networks, sort of from scratch, but using some of the helpful pieces to make our lives a little easier, while still allowing for the flexibility of going deep into the code.\n",
    "\n",
    "We are going to start with Tensorflow since it was one of the first successful neural network frameworks out there.  We'll see how to implement layers and write our first working network.  \n",
    "\n",
    "There are two major components to a neural network in tensorflow:\n",
    "\n",
    "* The model high level architecture\n",
    "* The individual layers\n",
    "\n",
    "This should seem familiar from the last section, this is because it's simply the right way to think about neural network design.  The major difference is now we'll need to implement less.  Let's look at an example of linear regression using tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab62cfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 15:34:44.418249: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-06-06 15:34:44.418294: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-06-06 15:34:45.012261: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x12ffa55c0\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "could not find registered platform with id: 0x12ffa55c0 [Op:__inference__update_step_xla_136]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps):\n\u001b[0;32m---> 62\u001b[0m     \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     pred \u001b[38;5;241m=\u001b[39m nn(X_test)\n\u001b[1;32m     64\u001b[0m     loss \u001b[38;5;241m=\u001b[39m mse(pred, y_test)\n",
      "Cell \u001b[0;32mIn [1], line 45\u001b[0m, in \u001b[0;36mNeuralNet.step\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     43\u001b[0m     loss \u001b[38;5;241m=\u001b[39m mse(pred, y)\n\u001b[1;32m     44\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1140\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_gradients_aggregation \u001b[38;5;129;01mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[1;32m   1139\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_gradients(grads_and_vars)\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:634\u001b[0m, in \u001b[0;36m_BaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    633\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables))\n\u001b[0;32m--> 634\u001b[0m iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1166\u001b[0m, in \u001b[0;36mOptimizer._internal_apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_internal_apply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[0;32m-> 1166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_merge_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_apply_gradients_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribution_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/distribute/merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[0;32m---> 51\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m distribution_strategy_context\u001b[38;5;241m.\u001b[39mget_replica_context()\u001b[38;5;241m.\u001b[39mmerge_call(\n\u001b[1;32m     54\u001b[0m       fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1216\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn\u001b[0;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step(grad, var)\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[0;32m-> 1216\u001b[0m     \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_grad_to_update_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[1;32m   1221\u001b[0m     _, var_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:2637\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2634\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   2635\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   2636\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 2637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2638\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2639\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[1;32m   2640\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:3710\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[1;32m   3708\u001b[0m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[1;32m   3709\u001b[0m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[0;32m-> 3710\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_non_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:3716\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   3712\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[1;32m   3713\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[1;32m   3714\u001b[0m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[1;32m   3715\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[0;32m-> 3716\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[1;32m   3718\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:595\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    594\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1211\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_grad_to_update_var\u001b[39m(var, grad):\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit_compile:\n\u001b[0;32m-> 1211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_step_xla\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_var_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step(grad, var)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mNotFoundError\u001b[0m: could not find registered platform with id: 0x12ffa55c0 [Op:__inference__update_step_xla_136]"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Dense(tf.Module):\n",
    "    def __init__(self, input_dim, output_size, name=None, final=False):\n",
    "        super().__init__(name=name)\n",
    "        self.final = final\n",
    "        init_weights = tf.random.normal([input_dim, output_size])\n",
    "        self.w = tf.Variable(\n",
    "            init_weights, name='w'\n",
    "        )\n",
    "        self.b = tf.Variable(\n",
    "            tf.zeros([output_size]), name='b'\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        result = tf.matmul(x, self.w) + self.b\n",
    "        if self.final:\n",
    "            return result[:,0]\n",
    "        return result\n",
    "\n",
    "        \n",
    "class NeuralNet(Model):\n",
    "    def __init__(self, X_in, X_out, optimizer):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer = Dense(X_in, X_out, final=True)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "    def step(self, x, y):\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        y = tf.cast(y, tf.float32)\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self.call(x)\n",
    "            loss = mse(pred, y)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "def mse(prediction, truth):\n",
    "    prediction = tf.cast(prediction, tf.float64)\n",
    "    truth = tf.cast(truth, tf.float64)\n",
    "    return tf.metrics.MSE(prediction, truth)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X, y = make_regression(n_samples=1000, n_features=100)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    learning_rate = 0.9\n",
    "    optimizer = tf.optimizers.Adam(learning_rate)\n",
    "    nn = NeuralNet(X_train.shape[1], 1, optimizer)\n",
    "    num_steps = 110\n",
    "    losses = []\n",
    "    for step in range(num_steps):\n",
    "        nn.step(X_train, y_train)\n",
    "        pred = nn(X_test)\n",
    "        loss = mse(pred, y_test)\n",
    "        losses.append(loss)\n",
    "    plt.plot(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5372b7ae",
   "metadata": {},
   "source": [
    "As this simple example shows, we are using the neural network framework to do linear regression.  If it's unclear where that's happening - recall that each of the individual layers are models themselves.  Here the linear regression 'model' is implemented here:\n",
    "\n",
    "```python\n",
    "def __call__(self, x):\n",
    "    result = tf.matmul(x, self.w) + self.b\n",
    "    if self.final:\n",
    "        return result[:,0]\n",
    "    return result\n",
    "```\n",
    "\n",
    "As you can tell we are doing the standard thing, as seen from previous chapters.  The only real difference is we are using tenorflow's matrix multiplication which is the same (more or less) as numpy.dot.  This is the first advantage of using a library like tensorflow, a lot of the heavy lifting is already done for us.  Not only do we get matrix multiplication for free, we also get automatic differentiation, so we don't need to calculate any derivatives.  \n",
    "\n",
    "As a result of doing our linear regression as well as optimizing subject to our loss, we actually get something 'stronger' than straight linear regression.  Specifically, we do our linear regression at each forward pass and then we update our weights subject to the gradient of our loss, meaning we are more or less fine-tuning our weights on every back pass and then doing linear regression on our forward pass.  By constantly fine tuning our weights and then carrying out linear regression, we end up with an optimized form of linear regression.  Which in a sense is the power of neural networks in the first place.\n",
    "\n",
    "There is actually a second optimization that we get for 'free' with tensorflow - that is the ADAM optimizer, which is an adaptive version of the standard stochastic gradient descent where the learning rate is adjusted throughout the learning process subject to momentum - when the loss is large, the learning rate adjusts alot, and when the loss is small, the learning rate adjusts a little.  In this way, we are optimizing the learning rate as we train.\n",
    "\n",
    "These two innovations - optimizing the weight initialization and optimizing the learning rate lead to an efficient optimization process, which is at the heart of why neural networks perform so well - they are adaptive to their own initial bad guesses and correct for them quickly where they matter - with the weight update and the loss function.  These two structural pieces of training any model contribute the most to how fast they will converge.  Or whether convergence happens at all.  And we get them, more or less for free with any decent neural network framework.\n",
    "\n",
    "Next let's look at the step function:\n",
    "\n",
    "```python\n",
    "def step(self, x, y):\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = self.call(x)\n",
    "        loss = mse(pred, y)\n",
    "    gradients = tape.gradient(loss, self.trainable_variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "```\n",
    "\n",
    "This one will look essentially the same for most neural networks.  It tells tensorflow at each step of training.  As long as we are using a standard optimizer, this will essentially be the same.  The only thing we might play with here is the types of our dependent and independent variables.  The gradient tape, as shown above is how we track our gradient across our layers.  As you can see, tape is how we calculate the gradient for each activation function.  \n",
    "\n",
    "Then those gradients get 'applied' to our trainable variables, which are exposed through each of our layers.  \n",
    "\n",
    "Now let's create a new example.  This time we'll make use of a non-linear activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30dfe56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float64, numpy=0.0015415136807632708>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.0027454654557903043>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.0048625763180615905>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.008476800506468778>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.014260227863769513>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYD0lEQVR4nO3de3Bc93ne8e+D+x0gCPAikhJJiZYiy9WNI9GVp1WihqI0jcWmHo01icVRNGFmIo+dTmZaOf2Dqd3MJJ0mrpXaatSYtpQmUhzHrmiNEpalVTmNRxfQUiXqSpi6EBQv4P0KkADe/rE/iCsIIAhggQPueT4zO9h99+zue3A4fPD7ncsqIjAzs3yryLoBMzPLnsPAzMwcBmZm5jAwMzMcBmZmBlRl3cBkdXR0xNKlS7Nuw8zsorJt27YDEdE5sn7RhsHSpUvp6urKug0zs4uKpPdGq3uayMzMHAZmZuYwMDMzHAZmZobDwMzMcBiYmRkOAzMzI4dh8OhP3+VH/++DrNswM5tVchcGj7/wvsPAzGyE3IVBc10VJ/oHsm7DzGxWyV0YNNVWcbzPYWBmVix3YdBcV+2RgZnZCLkLg6a6Ko73nc26DTOzWSV3YdDsaSIzs4/JXxjUVdE/MMSZgaGsWzEzmzVyFwZNtYWvcPB+AzOzc3IXBs111QDeb2BmViR3YdBUVxgZeL+Bmdk5uQuDZoeBmdnH5C4MWusL00RHT5/JuBMzs9kjd2HQ0VQLwMGTDgMzs2G5C4M5DTUAHDzhMDAzG5a7MKipqqClroqDJ/qzbsXMbNbIXRhAYarI00RmZufkMgzmNtV4msjMrEguw6C9sYaDJz1NZGY2bNwwkLRE0jOSXpf0mqQvp3q7pC2SdqSfc1Jdkh6S1C3pFUk3FL3XurT8Dknriuo3Sno1veYhSZqOlR02t6mWQ54mMjP70IWMDAaA342Iq4FVwAOSrgYeBLZGxApga3oMcAewIt3WAw9DITyADcDNwE3AhuEAScv8ZtHr1kx91cbW0VjDoZNnGByK6fwYM7OLxrhhEBF7IuJn6f5x4A1gEXAX8Gha7FFgbbp/F/BYFDwHtElaCNwObImIQxFxGNgCrEnPtUTEcxERwGNF7zUt5jbVMhRw5JRHB2ZmMMF9BpKWAtcDzwPzI2JPemovMD/dXwTsKnpZT6qdr94zSn20z18vqUtSV29v70Ra/4j2xnSugaeKzMyACYSBpCbgb4HfiYhjxc+lv+infc4lIh6JiJURsbKzs3PS79PZXDgLufe4dyKbmcEFhoGkagpB8JcR8YNU3pemeEg/96f6bmBJ0csXp9r56otHqU+b4TDYf7xvOj/GzOyicSFHEwn4NvBGRPxJ0VObgOEjgtYBTxbV701HFa0CjqbppM3Aaklz0o7j1cDm9NwxSavSZ91b9F7TYp5HBmZmH1F1AcvcAnwBeFXSy6n2e8AfAt+TdD/wHnB3eu5p4E6gGzgF3AcQEYckfQ14MS331Yg4lO7/NvBdoB74u3SbNk21VdRXV7L/mMPAzAwuIAwi4v8CYx33f9soywfwwBjvtRHYOEq9C7hmvF5KRRLzWmrZ75GBmRmQ0zOQoTBV5H0GZmYFOQ6DOo8MzMyS3IZBZ3Mtvd5nYGYG5DwMjvcPcPrMYNatmJllLrdh4MNLzczOyW8YtNQBPvHMzAzyHAYfnoXskYGZmcPgmEcGZma5DYM5DTVUVcgjAzMzchwGFRWio6nWO5DNzMhxGAC+JIWZWZLrMGhrqOHI6bNZt2Fmlrlch8Gchmp/9aWZGTkPg7b6ag77qy/NzHIeBg01HOsbYHBo2r+x08xsVst5GFQDcNT7Dcws53IdBnMaagC838DMci/XYdCaRgaHT3lkYGb5luswaKsfnibyyMDM8i3XYTA8TXT4pEcGZpZvuQ6D4R3IPvHMzPIu12HQUleN5B3IZma5DoOKCtFaX81hh4GZ5VyuwwAKO5GP+GgiM8u53IdBa321Tzozs9xzGDTUcMxhYGY55zDwyMDMzGHQWl/lMDCz3HMYpJHBkK9camY55jCor2Yo4MSZgaxbMTPLTO7DoK2+cEmKoz681MxyLPdh0FLv7zQwM8t9GLSmMPDhpWaWZw6Del+szszMYeCvvjQzGz8MJG2UtF/S9qLa70vaLenldLuz6LmvSOqW9Jak24vqa1KtW9KDRfVlkp5P9b+WVFPKFRxPq/cZmJld0Mjgu8CaUepfj4jr0u1pAElXA58HPple8y1JlZIqgW8CdwBXA/ekZQH+KL3XFcBh4P6prNBENdZUUlUhh4GZ5dq4YRARPwEOXeD73QU8ERH9EfEO0A3clG7dEbEzIs4ATwB3SRLwS8D30+sfBdZObBWmRpIvSWFmuTeVfQZflPRKmkaak2qLgF1Fy/Sk2lj1ucCRiBgYUR+VpPWSuiR19fb2TqH1j3IYmFneTTYMHgYuB64D9gB/XKqGziciHomIlRGxsrOzs2Tv21Jf7ZPOzCzXJhUGEbEvIgYjYgj47xSmgQB2A0uKFl2camPVDwJtkqpG1GeURwZmlneTCgNJC4se/itg+EijTcDnJdVKWgasAF4AXgRWpCOHaijsZN4UEQE8A3wuvX4d8ORkepqKtgaHgZnlW9V4C0h6HLgV6JDUA2wAbpV0HRDAu8BvAUTEa5K+B7wODAAPRMRgep8vApuBSmBjRLyWPuLfAU9I+o/AS8C3S7VyF8ojAzPLu3HDICLuGaU85n/YEfEHwB+MUn8aeHqU+k7OTTNlorW+mmN9hctYV1Qoy1bMzDKR+zOQoRAGEXC8z5exNrN8chjgK5eamTkMgDaHgZnlnMMAX5/IzMxhgK9cambmMMAjAzMzhwHFX3BzJuNOzMyy4TAA6qsrqams8MjAzHLLYUDhMtYt9dX+HmQzyy2HQdJaX+WRgZnllsMg8fWJzCzPHAZJa301R/ydBmaWUw6DpK2hxiMDM8sth0HiaSIzyzOHQdJSX83xvgEGhyLrVszMZpzDIBk+8ex4n0cHZpY/DoPEl6QwszxzGCTDl7H2EUVmlkcOg8RXLjWzPHMYJJ4mMrM8cxgkDgMzyzOHQeIwMLM8cxgkddWV1Fb5MtZmlk8OgyKt9dUc9dFEZpZDDoMiviSFmeWVw6CIw8DM8sphUMRhYGZ55TAo0trgMDCzfHIYFPHIwMzyymFQpLW+mhP9AwwMDmXdipnZjHIYFBk+8exY30DGnZiZzSyHQRGfhWxmeeUwKNLmK5eaWU45DIoMjwwOnzqTcSdmZjPLYVCkvbEWgMMnHQZmli/jhoGkjZL2S9peVGuXtEXSjvRzTqpL0kOSuiW9IumGotesS8vvkLSuqH6jpFfTax6SpFKv5IWa21QDwMETDgMzy5cLGRl8F1gzovYgsDUiVgBb02OAO4AV6bYeeBgK4QFsAG4GbgI2DAdIWuY3i1438rNmTHNtFTWVFRw42Z9VC2ZmmRg3DCLiJ8ChEeW7gEfT/UeBtUX1x6LgOaBN0kLgdmBLRByKiMPAFmBNeq4lIp6LiAAeK3qvGSeJuU01HhmYWe5Mdp/B/IjYk+7vBean+4uAXUXL9aTa+eo9o9QzUwgDjwzMLF+mvAM5/UUfJehlXJLWS+qS1NXb2zstnzG3sZaD3oFsZjkz2TDYl6Z4SD/3p/puYEnRcotT7Xz1xaPURxURj0TEyohY2dnZOcnWz8/TRGaWR5MNg03A8BFB64Ani+r3pqOKVgFH03TSZmC1pDlpx/FqYHN67pikVekoonuL3isTHU21HDzZT2HAY2aWD1XjLSDpceBWoENSD4Wjgv4Q+J6k+4H3gLvT4k8DdwLdwCngPoCIOCTpa8CLabmvRsTwTunfpnDEUj3wd+mWmbmNNfSdHeLUmUEaa8f99ZiZlYVx/7eLiHvGeOq2UZYN4IEx3mcjsHGUehdwzXh9zJS5TYUTzw6eOOMwMLPc8BnIIwyfeOZzDcwsTxwGI3Q0nhsZmJnlhcNghHOXpPDIwMzyw2EwQntjmiZyGJhZjjgMRqirrqSlror9xx0GZpYfDoNRLGytZ8/RvqzbMDObMQ6DUcxvrWPfMYeBmeWHw2AUC1vqPDIws1xxGIxifmsdB070c3ZwKOtWzMxmhMNgFAtb64jAO5HNLDccBqNY0FIHwF5PFZlZTjgMRrGg1WFgZvniMBjF8Mhgz9HTGXdiZjYzHAajaGuopraqwoeXmlluOAxGIYkFrT681Mzyw2Ewhkta69l9xNNEZpYPDoMxLGmvZ9chh4GZ5YPDYAyXtjdw4EQ/p88MZt2Kmdm0cxiMYUl7AwC7Dp/KuBMzs+nnMBjDpSkM3j/oMDCz8ucwGMOHYXDIYWBm5c9hMIb2xhoaayodBmaWCw6DMUhiSXsDPd5nYGY54DA4j0vbGzwyMLNccBicx6XtDbx38BRDQ5F1K2Zm08phcB7LOhvpHxjiA1+wzszKnMPgPJZ3NAGws/dkxp2YmU0vh8F5XN7ZCMDO3hMZd2JmNr0cBufR2VxLU20VOw94ZGBm5c1hcB6SuLyz0dNEZlb2HAbjWN7Z5GkiMyt7DoNxLO9o5IOjfZw6M5B1K2Zm08ZhMI7lnYUjit7xfgMzK2MOg3Es//CIIoeBmZUvh8E4lnU0IjkMzKy8OQzGUVddySWt9ew84J3IZla+phQGkt6V9KqklyV1pVq7pC2SdqSfc1Jdkh6S1C3pFUk3FL3PurT8DknrprZKpXf5vCa69zsMzKx8lWJk8IsRcV1ErEyPHwS2RsQKYGt6DHAHsCLd1gMPQyE8gA3AzcBNwIbhAJktrpxfCINBX7DOzMrUdEwT3QU8mu4/Cqwtqj8WBc8BbZIWArcDWyLiUEQcBrYAa6ahr0n7xPxm+geGePeg9xuYWXmaahgE8L8kbZO0PtXmR8SedH8vMD/dXwTsKnptT6qNVf8YSesldUnq6u3tnWLrF+6qBS0AvL33+Ix9ppnZTJpqGHwmIm6gMAX0gKR/VvxkRASFwCiJiHgkIlZGxMrOzs5Sve24rpjXhARvOgzMrExNKQwiYnf6uR/4IYU5/31p+of0c39afDewpOjli1NtrPqsUV9TydK5jbzlMDCzMjXpMJDUKKl5+D6wGtgObAKGjwhaBzyZ7m8C7k1HFa0CjqbppM3Aaklz0o7j1ak2q3xifhNv73MYmFl5qprCa+cDP5Q0/D5/FRF/L+lF4HuS7gfeA+5Oyz8N3Al0A6eA+wAi4pCkrwEvpuW+GhGHptDXtLhyQQtbXt9H39lB6qors27HzKykJh0GEbETuHaU+kHgtlHqATwwxnttBDZOtpeZcNWCZoYCduw7wacWt2bdjplZSfkM5At01YJmAN7YcyzjTszMSs9hcIGWzm2kqbaKV3cfzboVM7OScxhcoIoKcfUlLQ4DMytLDoMJ+NSiVt7Yc4yBwaGsWzEzKymHwQR8alEr/QND7PBF68yszDgMJuCaRYWjiDxVZGblxmEwAcs7GmmsqWS7w8DMyozDYAIqKsQnL2nllR6HgZmVF4fBBF1/WRuvfXCUvrODWbdiZlYyDoMJunlZO2cHg5feP5J1K2ZmJeMwmKAbL2tHghfemXWXTzIzmzSHwQS11ldz1YIWXnj3YNatmJmVjMNgEm5aOoefvXeEMwM++czMyoPDYBL+6RUdnD47SNd7nioys/LgMJiEW67ooLpSPPvWzH0Ps5nZdHIYTEJTbRUrL2vn2bcdBmZWHhwGk3TrlZ28ufc4e46ezroVM7MpcxhM0m2/MB+Azdv3ZtyJmdnUOQwm6Yp5TVy1oJkfvbIn61bMzKbMYTAFv3LtJWx77zA9h09l3YqZ2ZQ4DKbgs9deAsCTL3+QcSdmZlPjMJiCJe0NfHr5XB5/4X0GhyLrdszMJs1hMEW/vuoyeg6f5tm392fdipnZpDkMpmj1J+czr7mW7/zju1m3YmY2aQ6DKaqurOA3PrOMf9hxgJfeP5x1O2Zmk+IwKIEvrLqMOQ3VfGPrjqxbMTObFIdBCTTWVvFb//xy/s9bvfzEl6gws4uQw6BE7rtlKUvnNvD7P3rNl7Y2s4uOw6BEaqsq2fDZT7Kz9yTffKY763bMzCbEYVBCv3jlPH71+kX86Y938NxOfxOamV08HAYl9tW113DZ3Ea+9PhL7Drky1SY2cXBYVBiTbVV/Ldfv5G+s4N84dvPs/94X9YtmZmNy2EwDa5c0Mx37ruJfcf6+dVv/ZQd+45n3ZKZ2Xk5DKbJjZfN4Yn1q+gfGGLtN/+Rv3r+fSJ8/SIzm50cBtPo2iVtPPnALVy7pI3f++GrrP3WT3n27V6HgpnNOrMmDCStkfSWpG5JD2bdT6lc0lbP/7j/Zv7Tv/4n9B7rY93GF7jtj5/lv/54B9t3H2XIVzs1s1lAs+GvVEmVwNvALwM9wIvAPRHx+livWblyZXR1dc1Qh6XRPzDIky9/wPe39fDCO4cAaK2v5hcWNvOJ+c1c3tnEvOZaOptr6WiqpamuivrqSuqqK6msUMbdm1k5kLQtIlaOrFdl0cwobgK6I2IngKQngLuAMcPgYlRbVcndK5dw98ol7D/Wx09/fpDndh7kzb3H+cHPdnOif2DM19ZUVVBfXUlVhZBEhaCyQlRIaMT9mYoNafo/yRFo9nFPfekz1FZVlvQ9Z0sYLAJ2FT3uAW4euZCk9cB6gEsvvXRmOpsm81rqWHv9ItZevwiAiKD3RD8Hjp+h90Q/vcf7OXVmgNNnBjl9tnDrOzPIYARDAUNDwVAEg0OF1w5FMBgwNFMjvRn4mJiJDzG7CGka/kyaLWFwQSLiEeARKEwTZdxOSUliXnMd85rrsm7FzHJotuxA3g0sKXq8ONXMzGwGzJYweBFYIWmZpBrg88CmjHsyM8uNWTFNFBEDkr4IbAYqgY0R8VrGbZmZ5casCAOAiHgaeDrrPszM8mi2TBOZmVmGHAZmZuYwMDMzh4GZmTFLrk00GZJ6gfcm+fIO4EAJ27kYeJ3zweucD1NZ58sionNk8aINg6mQ1DXahZrKmdc5H7zO+TAd6+xpIjMzcxiYmVl+w+CRrBvIgNc5H7zO+VDydc7lPgMzM/uovI4MzMysiMPAzMzyFQaS1kh6S1K3pAez7qdUJC2R9Iyk1yW9JunLqd4uaYukHennnFSXpIfS7+EVSTdkuwaTJ6lS0kuSnkqPl0l6Pq3bX6dLoiOpNj3uTs8vzbTxSZLUJun7kt6U9IakT5f7dpb0b9K/6+2SHpdUV27bWdJGSfslbS+qTXi7SlqXlt8had1EeshNGEiqBL4J3AFcDdwj6epsuyqZAeB3I+JqYBXwQFq3B4GtEbEC2JoeQ+F3sCLd1gMPz3zLJfNl4I2ix38EfD0irgAOA/en+v3A4VT/elruYvQN4O8j4irgWgrrXrbbWdIi4EvAyoi4hsIl7j9P+W3n7wJrRtQmtF0ltQMbKHxl8E3AhuEAuSARkYsb8Glgc9HjrwBfybqvaVrXJ4FfBt4CFqbaQuCtdP/PgHuKlv9wuYvpRuEb8bYCvwQ8BYjCWZlVI7c5he/K+HS6X5WWU9brMMH1bQXeGdl3OW9nzn0/envabk8Bt5fjdgaWAtsnu12Be4A/K6p/ZLnxbrkZGXDuH9WwnlQrK2lYfD3wPDA/Ivakp/YC89P9cvld/Bfg3wJD6fFc4EhEDKTHxev14Tqn54+m5S8my4Be4DtpauzPJTVSxts5InYD/xl4H9hDYbtto7y387CJbtcpbe88hUHZk9QE/C3wOxFxrPi5KPypUDbHEUv6l8D+iNiWdS8zqAq4AXg4Iq4HTnJu6gAoy+08B7iLQhBeAjTy8emUsjcT2zVPYbAbWFL0eHGqlQVJ1RSC4C8j4gepvE/SwvT8QmB/qpfD7+IW4LOS3gWeoDBV9A2gTdLwN/gVr9eH65yebwUOzmTDJdAD9ETE8+nx9ymEQzlv538BvBMRvRFxFvgBhW1fztt52ES365S2d57C4EVgRToKoYbCTqhNGfdUEpIEfBt4IyL+pOipTcDwEQXrKOxLGK7fm45KWAUcLRqOXhQi4isRsTgillLYlj+OiF8DngE+lxYbuc7Dv4vPpeUvqr+gI2IvsEvSlal0G/A6ZbydKUwPrZLUkP6dD69z2W7nIhPdrpuB1ZLmpBHV6lS7MFnvNJnhHTR3Am8DPwf+fdb9lHC9PkNhCPkK8HK63UlhrnQrsAP430B7Wl4Ujqz6OfAqhSM1Ml+PKaz/rcBT6f5y4AWgG/gboDbV69Lj7vT88qz7nuS6Xgd0pW39P4E55b6dgf8AvAlsB/4CqC237Qw8TmGfyFkKI8D7J7Ndgd9I694N3DeRHnw5CjMzy9U0kZmZjcFhYGZmDgMzM3MYmJkZDgMzM8NhYGZmOAzMzAz4/8/uEnUNcvCvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Dense(tf.Module):\n",
    "    def __init__(self, input_dim, output_size, name=None, final=False):\n",
    "        super().__init__(name=name)\n",
    "        self.final = final\n",
    "        init_weights = tf.random.normal([input_dim, output_size])\n",
    "        self.w = tf.Variable(\n",
    "            init_weights, name='w'\n",
    "        )\n",
    "        self.b = tf.Variable(\n",
    "            tf.zeros([output_size]), name='b'\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        result = tf.matmul(x, self.w) + self.b\n",
    "        if self.final:\n",
    "            return result[:,0]\n",
    "        return tf.nn.tanh(result)\n",
    "\n",
    "        \n",
    "class NeuralNet(Model):\n",
    "    def __init__(self, X_in, X_out, optimizer):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer = Dense(X_in, X_out, final=True)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "    def step(self, x, y):\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        y = tf.cast(y, tf.float32)\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self.call(x)\n",
    "            loss = mse(pred, y)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "def mse(prediction, truth):\n",
    "    prediction = tf.cast(prediction, tf.float64)\n",
    "    truth = tf.cast(truth, tf.float64)\n",
    "    return tf.metrics.MSE(prediction, truth)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X, y = make_regression(n_samples=1000, n_features=100)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    learning_rate = 0.9\n",
    "    optimizer = tf.optimizers.Adam(learning_rate)\n",
    "    nn = NeuralNet(X_train.shape[1], 1, optimizer)\n",
    "    num_steps = 1000\n",
    "    losses = []\n",
    "    for step in range(num_steps):\n",
    "        nn.step(X_train, y_train)\n",
    "        pred = nn(X_test)\n",
    "        loss = mse(pred, y_test)\n",
    "        losses.append(loss)\n",
    "    plt.plot(losses)\n",
    "\n",
    "losses[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca7f17",
   "metadata": {},
   "source": [
    "As you can see, not much has changed.  The tanh function bounds our weights between -1 to 1 meaning we are less likely to have a vanishing our exploding gradient.  You may be wondering, how to deal with different types of problems - for instance, if you were interested in a classification problem, what might you change?  That comes down to the loss function.\n",
    "\n",
    "Of course, different activation functions are better for different problems.  Some people try to prescribe rules of thumb here, but it's best to look at your data, your layers, your activations and your loss holistically to decide what's the right set up for your problem.\n",
    "\n",
    "Next, let's add some more layers, to see how that works and if that increases the speed of convergence at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a946865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float64, numpy=1507.5564728675026>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=1516.2875956464147>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=1534.6091275769984>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=1533.1236966322867>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=1503.8588292246054>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Dense(tf.Module):\n",
    "    def __init__(self, input_dim, output_size, name=None, final=False):\n",
    "        super().__init__(name=name)\n",
    "        self.final = final\n",
    "        init_weights = tf.random.normal([input_dim, output_size])\n",
    "        self.w = tf.Variable(\n",
    "            init_weights, name='w'\n",
    "        )\n",
    "        self.b = tf.Variable(\n",
    "            tf.zeros([output_size]), name='b'\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        result = tf.matmul(x, self.w) + self.b\n",
    "        if self.final:\n",
    "            return result[:,0]\n",
    "        return tf.nn.tanh(result)\n",
    "\n",
    "        \n",
    "class NeuralNet(Model):\n",
    "    def __init__(self, X_in, X_out, optimizer):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.hidden_layer = Dense(X_in, X_in)\n",
    "        self.final_layer = Dense(X_in, X_out, final=True)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def call(self, x):\n",
    "        res = self.hidden_layer(x)\n",
    "        return self.final_layer(res)\n",
    "\n",
    "    def step(self, x, y):\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        y = tf.cast(y, tf.float32)\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self.call(x)\n",
    "            loss = mse(pred, y)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "def mse(prediction, truth):\n",
    "    prediction = tf.cast(prediction, tf.float64)\n",
    "    truth = tf.cast(truth, tf.float64)\n",
    "    return tf.metrics.MSE(prediction, truth)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X, y = make_regression(n_samples=1000, n_features=100)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    learning_rate = 0.9\n",
    "    optimizer = tf.optimizers.Adam(learning_rate)\n",
    "    nn = NeuralNet(X_train.shape[1], 1, optimizer)\n",
    "    num_steps = 1000\n",
    "    losses = []\n",
    "    for step in range(num_steps):\n",
    "        nn.step(X_train, y_train)\n",
    "        pred = nn(X_test)\n",
    "        loss = mse(pred, y_test)\n",
    "        losses.append(loss)\n",
    "    plt.plot(losses)\n",
    "\n",
    "losses[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9fbcc7",
   "metadata": {},
   "source": [
    "As you can see, now that we've doubled the number of layers, the amount of time that we need to get good performance goes down quiet a bit.  This is the standard phenomenon in neural networks - increasing the number of layers increases the rate of convergence.  Interestingly though, increasing the number of layers does not get us as small a loss.  We appear to be overfitting in this case.  Let's try decreasing the capacity of our network and removing the non-linearity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4b2c377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float64, numpy=3.947685570905593e-10>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=4.03295296455105e-10>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=3.530168021987341e-10>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=3.3769778791998974e-10>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=3.2424080677943773e-10>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZ50lEQVR4nO3dfXBV953f8fcHxIOfAVtLCCIVqdlksbtZ2wrGTaaThgSE6w2ejuOB2Qmql4RpQ7pJNh0v7LaltddtMtlZEmYcxtSwhozHDyVpTV0cymIybmaCg/yw2AazyHhtRLGRAeMHYkDo2z/uT9Lh6h6EdKV7hfR5zdy553zP75zzOxzmfnQe7rmKCMzMzEoZVe0OmJnZ0OWQMDOzXA4JMzPL5ZAwM7NcDgkzM8tVU+0ODLRrrrkm6uvrq90NM7OLynPPPfdORNQW14ddSNTX19Pc3FztbpiZXVQkvVGq7tNNZmaWyyFhZma5HBJmZpbLIWFmZrkcEmZmlsshYWZmuRwSZmaWq9eQkLRe0hFJL5eY9j1JIemaNC5JqyW1SNot6cZM2yZJ+9OrKVO/SdJLaZ7VkpTqkyRtS+23SZo4MJtc2pmzHTzefJCODj863cys04UcSTwENBYXJU0D5gJvZsrzgRnptRRYk9pOAlYCNwOzgJWZD/01wDcy83WuazmwPSJmANvT+KBZ+8wB7t60m5893zqYqzEzu6j0GhIR8QxwrMSkVcDdQPZP7wXAxijYCUyQNAWYB2yLiGMRcRzYBjSmaVdGxM4o/PrRRuD2zLI2pOENmfqgeOeDUwCc+O2ZwVyNmdlFpV/XJCQtAA5FxN8VTZoKHMyMt6ba+eqtJeoAkyPicBp+C5h8nv4sldQsqbmtra2vm2NmZjn6HBKSLgX+HPiPA9+d0tJRRu7FgohYGxENEdFQW9vj+VRmZtZP/TmS+MfAdODvJP0DUAc8L+ljwCFgWqZtXaqdr15Xog7wdjodRXo/0o++9lm6bm5mZvQjJCLipYj4nYioj4h6CqeIboyIt4DNwOJ0l9Ns4EQ6ZbQVmCtpYrpgPRfYmqa9J2l2uqtpMfBEWtVmoPMuqKZM3czMKuRCboF9BPg18ClJrZKWnKf5FuAA0AL8N+CbABFxDLgX2JVe96Qaqc2DaZ7XgKdS/fvAlyXtB76Uxs3MrIJ6/T2JiFjUy/T6zHAAy3LarQfWl6g3A9eXqB8F5vTWPzMzGzz+xnUS/g6dmVkPDgkzM8vlkDAzs1wOiSK+AdbMrJtDwszMcjkkzMwsl0PCzMxyOSTMzCyXQ8LMzHI5JMzMLJdDoogfAmtm1s0hYWZmuRwSZmaWyyFhZma5HBJJ+DGwZmY9OCSK+Lq1mVk3h4SZmeVySJiZWS6HhJmZ5eo1JCStl3RE0suZ2g8lvSppt6T/IWlCZtoKSS2S9kmal6k3plqLpOWZ+nRJz6b6Y5LGpvq4NN6SptcP1EaX4svWZmY9XciRxENAY1FtG3B9RPw+8PfACgBJM4GFwHVpnp9IGi1pNHA/MB+YCSxKbQF+AKyKiGuB48CSVF8CHE/1VamdmZlVUK8hERHPAMeKav8nItrT6E6gLg0vAB6NiFMR8TrQAsxKr5aIOBARp4FHgQWSBHwR2JTm3wDcnlnWhjS8CZiT2puZWYUMxDWJPwaeSsNTgYOZaa2plle/Gng3Ezid9XOWlaafSO17kLRUUrOk5ra2trI2xjlkZtatrJCQ9BdAO/DwwHSnfyJibUQ0RERDbW1tNbtiZjas1PR3Rkn/CrgNmBPdX1c+BEzLNKtLNXLqR4EJkmrS0UK2feeyWiXVAFel9mZmViH9OpKQ1AjcDXwlIk5mJm0GFqY7k6YDM4DfALuAGelOprEULm5vTuGyA7gjzd8EPJFZVlMavgN4OvzsDDOziur1SELSI8AXgGsktQIrKdzNNA7Yls7h74yIfx0Rr0h6HNhD4TTUsog4m5bzLWArMBpYHxGvpFX8GfCopL8EXgDWpfo64KeSWihcOF84ANuby/FjZtZTryEREYtKlNeVqHW2vw+4r0R9C7ClRP0AhbufiusfAV/trX9mZjZ4/I1rMzPL5ZAo4jtgzcy6OSTMzCyXQ8LMzHI5JMzMLJdDIgk/B9bMrAeHRBFftzYz6+aQMDOzXA4JMzPL5ZAwM7NcDonEz24yM+vJIWFmZrkcEmZmlsshUcwPbzIz6+KQMDOzXA4JMzPL5ZAwM7NcDonEd8CamfXkkDAzs1y9hoSk9ZKOSHo5U5skaZuk/el9YqpL0mpJLZJ2S7oxM09Tar9fUlOmfpOkl9I8q6XC7UV56zAzs8q5kCOJh4DGotpyYHtEzAC2p3GA+cCM9FoKrIHCBz6wErgZmAWszHzorwG+kZmvsZd1DCrfAGtm1q3XkIiIZ4BjReUFwIY0vAG4PVPfGAU7gQmSpgDzgG0RcSwijgPbgMY07cqI2BkRAWwsWlapdZiZWYX095rE5Ig4nIbfAian4anAwUy71lQ7X721RP186+hB0lJJzZKa29ra+rE5ZmZWStkXrtMRwKDeHNTbOiJibUQ0RERDbW3tYHbFzGxE6W9IvJ1OFZHej6T6IWBapl1dqp2vXleifr51DAo/BdbMrKf+hsRmoPMOpSbgiUx9cbrLaTZwIp0y2grMlTQxXbCeC2xN096TNDvd1bS4aFml1jGo/OgmM7NuNb01kPQI8AXgGkmtFO5S+j7wuKQlwBvAnan5FuBWoAU4CdwFEBHHJN0L7Ert7omIzovh36RwB9UlwFPpxXnWYWZmFdJrSETEopxJc0q0DWBZznLWA+tL1JuB60vUj5Zah5mZVY6/cW1mZrkcEl185drMrJhDwszMcjkkzMwsl0OiiPz0JjOzLg4JMzPL5ZAwM7NcDgkzM8vlkEj87CYzs54cEkX87CYzs24OCTMzy+WQMDOzXA4JMzPL5ZAwM7NcDgkzM8vlkEh8C6yZWU8OiSK+A9bMrJtDwszMcjkkzMwsV1khIem7kl6R9LKkRySNlzRd0rOSWiQ9Jmlsajsujbek6fWZ5axI9X2S5mXqjanWIml5OX01M7O+63dISJoK/AnQEBHXA6OBhcAPgFURcS1wHFiSZlkCHE/1Vakdkmam+a4DGoGfSBotaTRwPzAfmAksSm3NzKxCyj3dVANcIqkGuBQ4DHwR2JSmbwBuT8ML0jhp+hxJSvVHI+JURLwOtACz0qslIg5ExGng0dTWzMwqpN8hERGHgL8C3qQQDieA54B3I6I9NWsFpqbhqcDBNG97an91tl40T169B0lLJTVLam5ra+vf9uB7YM3MipVzumkihb/spwMfBy6jcLqo4iJibUQ0RERDbW1tWcvyU2DNzLqVc7rpS8DrEdEWEWeAnwOfAyak008AdcChNHwImAaQpl8FHM3Wi+bJq5uZWYWUExJvArMlXZquLcwB9gA7gDtSmybgiTS8OY2Tpj8dEZHqC9PdT9OBGcBvgF3AjHS31FgKF7c3l9FfMzPro5rem5QWEc9K2gQ8D7QDLwBrgf8NPCrpL1NtXZplHfBTSS3AMQof+kTEK5IepxAw7cCyiDgLIOlbwFYKd06tj4hX+ttfMzPru36HBEBErARWFpUPULgzqbjtR8BXc5ZzH3BfifoWYEs5fbxQfnaTmVlP/sZ1EfnpTWZmXRwSZmaWyyFhZma5HBJmZpbLIWFmZrkcEmZmlsshkfgOWDOznhwSZmaWyyFRxE+DNTPr5pAwM7NcDokifjyHmVk3h4SZmeVySBTxgYSZWTeHROLTTGZmPTkkzMwsl0OiiI8ozMy6OSTMzCyXQ6KIv0xnZtbNIZE4HMzMeiorJCRNkLRJ0quS9kq6RdIkSdsk7U/vE1NbSVotqUXSbkk3ZpbTlNrvl9SUqd8k6aU0z2pJ/m1RM7MKKvdI4sfALyLi08BngL3AcmB7RMwAtqdxgPnAjPRaCqwBkDQJWAncDMwCVnYGS2rzjcx8jWX2t1e+cG1m1q3fISHpKuCfAesAIuJ0RLwLLAA2pGYbgNvT8AJgYxTsBCZImgLMA7ZFxLGIOA5sAxrTtCsjYmdEBLAxsywzM6uAco4kpgNtwN9IekHSg5IuAyZHxOHU5i1gchqeChzMzN+aauert5ao9yBpqaRmSc1tbW1lbJK/cW1mllVOSNQANwJrIuIG4EO6Ty0BkI4ABv1zNyLWRkRDRDTU1tYO9urMzEaMckKiFWiNiGfT+CYKofF2OlVEej+Sph8CpmXmr0u189XrStQHly9KmJl16XdIRMRbwEFJn0qlOcAeYDPQeYdSE/BEGt4MLE53Oc0GTqTTUluBuZImpgvWc4Gtadp7kmanu5oWZ5Y18JwNZmY91JQ5/78FHpY0FjgA3EUheB6XtAR4A7gztd0C3Aq0ACdTWyLimKR7gV2p3T0RcSwNfxN4CLgEeCq9zMysQsoKiYh4EWgoMWlOibYBLMtZznpgfYl6M3B9OX3sKx9QmJl18zeuzcwsl0OiiK9bm5l1c0iYmVkuh0TSeQARPpQwM+vikDAzs1wOCTMzy+WQKOKTTWZm3RwSZmaWyyFRxNetzcy6OSQS39VkZtaTQ8LMzHI5JIr4eMLMrJtDwszMcjkkivjahJlZN4eEmZnlckgky/75tdXugpnZkOOQSD521fhqd8HMbMhxSJiZWS6HRBFftzYz61Z2SEgaLekFSU+m8emSnpXUIukxSWNTfVwab0nT6zPLWJHq+yTNy9QbU61F0vJy+9rLdgzm4s3MLkoDcSTxbWBvZvwHwKqIuBY4DixJ9SXA8VRfldohaSawELgOaAR+koJnNHA/MB+YCSxKbQdV+Ot0ZmZdygoJSXXAvwAeTOMCvghsSk02ALen4QVpnDR9Tmq/AHg0Ik5FxOtACzArvVoi4kBEnAYeTW0HhY8jzMx6KvdI4kfA3UBHGr8aeDci2tN4KzA1DU8FDgKk6SdS+6560Tx59R4kLZXULKm5ra2tzE0yM7NO/Q4JSbcBRyLiuQHsT79ExNqIaIiIhtra2jKXNUCdMjMbBmrKmPdzwFck3QqMB64EfgxMkFSTjhbqgEOp/SFgGtAqqQa4CjiaqXfKzpNXH3C+bm1m1lO/jyQiYkVE1EVEPYULz09HxB8BO4A7UrMm4Ik0vDmNk6Y/HYUHJW0GFqa7n6YDM4DfALuAGeluqbFpHZv7298L3q7BXoGZ2UVkML4n8WfAn0pqoXDNYV2qrwOuTvU/BZYDRMQrwOPAHuAXwLKIOJuORL4FbKVw99Tjqe2gULp0/f2nXh2sVZiZXXTKOd3UJSJ+CfwyDR+gcGdScZuPgK/mzH8fcF+J+hZgy0D00czM+s7fuE58TcLMrCeHhJmZ5XJIlHDmbEfvjczMRgCHRAknT52tdhfMzIYEh0QJH5xu772RmdkI4JBIsheuz7T7dJOZGTgkSurwsznMzACHRBdlngPb4YwwMwMcEiX5SMLMrMAhUYJDwsyswCGRZC9cd/i6tZkZ4JAoyUcSZmYFDokk++gmh4SZWYFDogTf3WRmVuCQKOG0v0xnZgY4JLooc+X6zgd+zQen/GgOMzOHRI43jn5Y7S6YmVWdQyIp/s2hI++fqko/zMyGEodEjnv+155qd8HMrOr6HRKSpknaIWmPpFckfTvVJ0naJml/ep+Y6pK0WlKLpN2Sbswsqym13y+pKVO/SdJLaZ7V0uD9yGjxkl9/50P/+JCZjXjlHEm0A9+LiJnAbGCZpJnAcmB7RMwAtqdxgPnAjPRaCqyBQqgAK4GbgVnAys5gSW2+kZmvsYz+9tnRD05XcnVmZkNOv0MiIg5HxPNp+H1gLzAVWABsSM02ALen4QXAxijYCUyQNAWYB2yLiGMRcRzYBjSmaVdGxM6ICGBjZlkV0ebrEmY2wg3INQlJ9cANwLPA5Ig4nCa9BUxOw1OBg5nZWlPtfPXWEvVS618qqVlSc1tbW3+3oUft+EkfSZjZyFZ2SEi6HPgZ8J2IeC87LR0BDPr3lyNibUQ0RERDbW3tgC233U/6M7MRrqyQkDSGQkA8HBE/T+W306ki0vuRVD8ETMvMXpdq56vXlahXzOl2P5/DzEa2cu5uErAO2BsRf52ZtBnovEOpCXgiU1+c7nKaDZxIp6W2AnMlTUwXrOcCW9O09yTNTutanFlWRfhIwsxGupoy5v0c8DXgJUkvptqfA98HHpe0BHgDuDNN2wLcCrQAJ4G7ACLimKR7gV2p3T0RcSwNfxN4CLgEeCq9Kqb9rI8kzGxk63dIRMSv6PlF5U5zSrQPYFnOstYD60vUm4Hr+9vHcvl7EmY20vkb1+fR7meGm9kI55A4j3YfSZjZCOeQOI8zviZhZiOcQ+I87nlyD/fvaKl2N8zMqsYh0Ysfbt1X7S6YmVWNQ8LMzHI5JMzMLJdDwszMcjkkShi8nzYyM7u4OCRKGF8zutpdMDMbEhwSGf9u7u8CMG6M/1nMzMAhcY47P1t4YvnHrhxf5Z6YmQ0NDomM37liPPcuuI6H7pp1Tv2sn+FkZiNUOY8KH5a+dkt9j9rJ0+1cMX5M5TtjZlZlPpK4ANv3HuGdD05VuxtmZhXnkLgA33nsRZZubK52N8zMKs4hcYGef/NdTp5ur3Y3quL9j87wt3vepvC7UWY2kjgk+mDv4feq3YWq+O5jL/L1jc3sbj1R7a6YWYU5JHL8h9tmMu+6yXz989NZvegGAFZt2z8i/5r+9WtHAfh/7/62yj0xs0ob8iEhqVHSPkktkpZXar1LPj+dB77WwL+/bSZ/+PtTmFU/iV+1vMOOfUcq1YUhoaMj+PD0WQD+zcPPc9QX8M1GlCF9C6yk0cD9wJeBVmCXpM0RsafC/eCnX5/F3FXP8PUNzUy56hI+Wz+Rxuun8LuTL2fM6FFcPq6Gy8bVMLZmyOdun+x7+/1zxu984NesmP97fHzCJfzelCuQH3RlNqxpKJ8+kXQL8J8iYl4aXwEQEf81b56GhoZobh6cO5EOtH3Aml++xsHjJ9l7+H1O/PZMjzZja0Zxxbgaxo8ZjQSjpO53gKLP1OxoqQ/czv3TtZfinLdzpkfXtFTrHC/axcXLjCgxT5r24al2OiL46Ezp3/seP2YUl46tYezoUYypEWNGjaIjs0J1bnfaWJWqm9mA+C//8p/w2fpJ/ZpX0nMR0VBcH9JHEsBU4GBmvBW4ubiRpKXAUoBPfOITg9aZT9Zezg+/+hkAzpztYNfrxzjy/inaO4IPPjrDB6faef9UOx981M5HZzoIggjoiO73rMgdKXxod32MnvvWFSbd493Ti6d1z6sebc8dV/dwZubRo2DBH0xl2sRL+VXLO3z6Y1fw4P89wI59bfzhZ6YwrmY0p9rPcrq9gzNng9NnO7oCsVSQdQ5E8QabWdkuGTPwDycd6iFxQSJiLbAWCkcSlVjnmNGj+KfXXlOJVQ0Zd9xUB8CPFt5Q5Z6YWaUM9RPoh4BpmfG6VDMzswoY6iGxC5ghabqkscBCYHOV+2RmNmIM6dNNEdEu6VvAVmA0sD4iXqlyt8zMRowhHRIAEbEF2FLtfpiZjURD/XSTmZlVkUPCzMxyOSTMzCyXQ8LMzHIN6cdy9IekNuCNfs5+DfDOAHbnYuBtHhm8zSNDOdv8jyKitrg47EKiHJKaSz27ZDjzNo8M3uaRYTC22aebzMwsl0PCzMxyOSTOtbbaHagCb/PI4G0eGQZ8m31NwszMcvlIwszMcjkkzMwsl0MikdQoaZ+kFknLq92fgSBpmqQdkvZIekXSt1N9kqRtkvan94mpLkmr07/Bbkk3VncL+k/SaEkvSHoyjU+X9GzatsfSo+eRNC6Nt6Tp9VXteD9JmiBpk6RXJe2VdMtw38+Svpv+X78s6RFJ44fbfpa0XtIRSS9nan3er5KaUvv9kpr60geHBIUPFOB+YD4wE1gkaWZ1ezUg2oHvRcRMYDawLG3XcmB7RMwAtqdxKGz/jPRaCqypfJcHzLeBvZnxHwCrIuJa4DiwJNWXAMdTfVVqdzH6MfCLiPg08BkK2z5s97OkqcCfAA0RcT2FnxJYyPDbzw8BjUW1Pu1XSZOAlRR++nkWsLIzWC5IRIz4F3ALsDUzvgJYUe1+DcJ2PgF8GdgHTEm1KcC+NPwAsCjTvqvdxfSi8AuG24EvAk9S+Envd4Ca4v1N4bdKbknDNamdqr0Nfdzeq4DXi/s9nPczMBU4CExK++1JYN5w3M9APfByf/crsAh4IFM/p11vLx9JFHT+h+vUmmrDRjq8vgF4FpgcEYfTpLeAyWl4uPw7/Ai4G+hI41cD70ZEexrPblfXNqfpJ1L7i8l0oA34m3SK7UFJlzGM93NEHAL+CngTOExhvz3H8N7Pnfq6X8va3w6JEUDS5cDPgO9ExHvZaVH402LY3Act6TbgSEQ8V+2+VFANcCOwJiJuAD6k+xQEMCz380RgAYWA/DhwGT1Pywx7ldivDomCQ8C0zHhdql30JI2hEBAPR8TPU/ltSVPS9CnAkVQfDv8OnwO+IukfgEcpnHL6MTBBUucvMWa3q2ub0/SrgKOV7PAAaAVaI+LZNL6JQmgM5/38JeD1iGiLiDPAzyns++G8nzv1db+Wtb8dEgW7gBnpzoixFC6Aba5yn8omScA6YG9E/HVm0mag8w6HJgrXKjrri9NdErOBE5nD2otCRKyIiLqIqKewH5+OiD8CdgB3pGbF29z5b3FHan9R/cUdEW8BByV9KpXmAHsYxvuZwmmm2ZIuTf/PO7d52O7njL7u163AXEkT0xHY3FS7MNW+KDNUXsCtwN8DrwF/Ue3+DNA2fZ7Coehu4MX0upXCudjtwH7gb4FJqb0o3OX1GvAShTtHqr4dZWz/F4An0/Angd8ALcB/B8al+vg03pKmf7La/e7ntv4B0Jz29f8EJg73/Qz8Z+BV4GXgp8C44bafgUcoXHM5Q+GIcUl/9ivwx2nbW4C7+tIHP5bDzMxy+XSTmZnlckiYmVkuh4SZmeVySJiZWS6HhJmZ5XJImJlZLoeEmZnl+v+d8AllPiTs8gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Dense(tf.Module):\n",
    "    def __init__(self, input_dim, output_size, name=None, final=False):\n",
    "        super().__init__(name=name)\n",
    "        self.final = final\n",
    "        init_weights = tf.random.normal([input_dim, output_size])\n",
    "        self.w = tf.Variable(\n",
    "            init_weights, name='w'\n",
    "        )\n",
    "        self.b = tf.Variable(\n",
    "            tf.zeros([output_size]), name='b'\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        result = tf.matmul(x, self.w) + self.b\n",
    "        if self.final:\n",
    "            return result[:,0]\n",
    "        return result\n",
    "\n",
    "        \n",
    "class NeuralNet(Model):\n",
    "    def __init__(self, X_in, X_out, optimizer):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.hidden_layer = Dense(X_in, X_in//2)\n",
    "        self.final_layer = Dense(X_in//2, X_out, final=True)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def call(self, x):\n",
    "        res = self.hidden_layer(x)\n",
    "        return self.final_layer(res)\n",
    "\n",
    "    def step(self, x, y):\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        y = tf.cast(y, tf.float32)\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self.call(x)\n",
    "            loss = mse(pred, y)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "def mse(prediction, truth):\n",
    "    prediction = tf.cast(prediction, tf.float64)\n",
    "    truth = tf.cast(truth, tf.float64)\n",
    "    return tf.metrics.MSE(prediction, truth)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X, y = make_regression(n_samples=1000, n_features=100)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    learning_rate = 0.9\n",
    "    optimizer = tf.optimizers.Adam(learning_rate)\n",
    "    nn = NeuralNet(X_train.shape[1], 1, optimizer)\n",
    "    num_steps = 1000\n",
    "    losses = []\n",
    "    for step in range(num_steps):\n",
    "        nn.step(X_train, y_train)\n",
    "        pred = nn(X_test)\n",
    "        loss = mse(pred, y_test)\n",
    "        losses.append(loss)\n",
    "    plt.plot(losses)\n",
    "\n",
    "losses[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2979fa",
   "metadata": {},
   "source": [
    "As you can see by cutting the capacity of our network in half and removing the non-linearity we get to a very small loss.  And we do it much faster than with the network with a single layer.  Next, let's look at a classification example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13ae13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Dense(tf.Module):\n",
    "    def __init__(self, input_dim, output_size, name=None, final=False):\n",
    "        super().__init__(name=name)\n",
    "        self.final = final\n",
    "        init_weights = tf.random.normal([input_dim, output_size])\n",
    "        self.w = tf.Variable(\n",
    "            init_weights, name='w'\n",
    "        )\n",
    "        self.b = tf.Variable(\n",
    "            tf.zeros([output_size]), name='b'\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        result = tf.matmul(x, self.w) + self.b\n",
    "        if self.final:\n",
    "            return tf.nn.sigmoid(result)[:,0]\n",
    "        return tf.nn.tanh(result)\n",
    "\n",
    "        \n",
    "class NeuralNet(Model):\n",
    "    def __init__(self, X_in, X_out, optimizer):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.hidden_layer = Dense(X_in, X_in)\n",
    "        self.final_layer = Dense(X_in, X_out, final=True)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def call(self, x):\n",
    "        res = self.hidden_layer(x)\n",
    "        return self.final_layer(res)\n",
    "\n",
    "    def step(self, x, y):\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        y = tf.cast(y, tf.float32)\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self.call(x)\n",
    "            loss = binary_cross_entropy(pred, y)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "def binary_cross_entropy(prediction, truth):\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "    return cce(truth, prediction)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X, y = make_classification(n_samples=1000, n_features=100)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    learning_rate = 0.01\n",
    "    optimizer = tf.optimizers.Adam(learning_rate)\n",
    "    nn = NeuralNet(X_train.shape[1], 1, optimizer)\n",
    "    num_steps = 500\n",
    "    losses = []\n",
    "    for step in range(num_steps):\n",
    "        nn.step(X_train, y_train)\n",
    "        pred = nn(X_test)\n",
    "        loss = binary_cross_entropy(pred, y_test)\n",
    "        losses.append(loss)\n",
    "        prev = nn.hidden_layer.w\n",
    "    plt.plot(losses)\n",
    "\n",
    "losses[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a21b1b2",
   "metadata": {},
   "source": [
    "## Our first 'real' model architecture\n",
    "\n",
    "First we will implement a custom loss function, because why not?  And then we'll make our first architecture - the Residue Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba04752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import MeanSquaredError as mean_squared_error\n",
    "\n",
    "def mean_directional_accuracy(actual, predicted):\n",
    "    \"\"\"\n",
    "    prediction period must overlap with actual\n",
    "    \"\"\"\n",
    "    lagged = actual[:-1]\n",
    "    actual = actual[1:]\n",
    "    predicted = predicted[1:]\n",
    "    actual = actual - lagged\n",
    "    actual = actual[~tf.math.is_nan(actual)]\n",
    "    predicted = predicted - lagged\n",
    "    predicted = predicted[~tf.math.is_nan(predicted)]\n",
    "    return np.sum(tf.math.sign(actual) == tf.math.sign(predicted))/(len(actual) + 1)\n",
    "\n",
    "def mse_mda(actual, predicted, mse_weight=0.65):\n",
    "    mda_weight = 1 - mse_weight\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    mda = mean_directional_accuracy(actual, predicted)\n",
    "    return mse_weight * mse + mda_weight * mda\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5798b839",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "a = np.random.normal(0, 10, size=100)\n",
    "b = tf.constant(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efd28356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152.61891410319424"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "debdc3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=152.61891410319421>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d21aac42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=10.2>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.normal(0, 10, size=100)\n",
    "b = tf.constant(a)\n",
    "a = np.random.normal(0, 10, size=100)\n",
    "c = tf.constant(a)\n",
    "tf.cast(tf.reduce_sum(tf.cast(tf.math.sign(b) == tf.math.sign(c), tf.int64)).numpy(), tf.float32)/tf.cast(5, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6afff92",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-22-928c0e8ad652>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-928c0e8ad652>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tf.reduce_sum(tf.cast(tf.math.sign(b) == tf.math.sign(c), tf.int64)\u001b[0m\n\u001b[0m                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "tf.reduce_sum(tf.cast(tf.math.sign(b) == tf.math.sign(c), tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fed66a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=51.0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(\n",
    "    tf.reduce_sum(tf.cast(tf.math.sign(b) == tf.math.sign(c), tf.int64)).numpy()\n",
    "    , tf.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba9a1b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
