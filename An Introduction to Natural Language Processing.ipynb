{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Natural Language Processing\n",
    "\n",
    "The study of natural language processing is primarily concerned with teaching machines to process and \"understand\" text.  It is fairly straight forward for machines to understand structured data like numbers, however to understand language is far more difficult.  Part of this has to do with the nature of numbers versus the nature of language.  Specifically, words can have multiple meanings in multiple contexts.  While the mean behind a number can change, based on its reference, the definition of a number is consistent across all mediums and contexts.\n",
    "\n",
    "To make this discrete, if you have 5 chairs or 5 starfish, while the type of thing being described changes, the quantity of things being described stays constant.  However, with language we cannot make such a claim.  For instance, let's look at the following sentence:\n",
    "\n",
    "\"I hope you do a great job!\"\n",
    "\n",
    "Taken out of context, its meaning is most likely that the narrator is expressing hope to the subject to do a great job.  Let's see the same sentence in another context:\n",
    "\n",
    "Person1: \"I am going to have the best project\"\n",
    "\n",
    "Person2: \"Yea, right, you know I'll do better\"\n",
    "\n",
    "Person1: \"I hope you do a great job.\"\n",
    "\n",
    "Person1: \"Not\"\n",
    "\n",
    "Person2: \"No need to be sarcastic\"\n",
    "\n",
    "Here \"I hope you do a great job\" is intended _sarcastically_ so the meaning of the statement is reversed, despite us not changing the phrasing of the sentence, the _meaning_ has changed completely.  How do we even begin to encode something like sarcasm in our language?  Would a person even do that in a context we care about?  These are just some of the exciting questions of natural language processing!  \n",
    "\n",
    "Below we'll make our way through the following topics:\n",
    "\n",
    "* Building A Simple Recommendation Engine\n",
    "    * Bag Of Words Model\n",
    "    * stop words\n",
    "    * stemming\n",
    "    * lemmatization\n",
    "    * part of speech tagging\n",
    "        * n-gram analysis\n",
    "    * named entity recognition\n",
    "* scikit-learn for POS\n",
    "* Relation Extraction\n",
    "* syntax trees\n",
    "* tf-idf\n",
    "* corpus generation\n",
    "* word2vec\n",
    "* Topic modeling with LDA\n",
    "* Text Classification with Naive Bayes\n",
    "* Text Prediction with Logistic Regression\n",
    "\n",
    "* Fuzzy Matching For Deduplication\n",
    "* Finding Human Traffickers Online\n",
    "\n",
    "References:\n",
    "* https://moj-analytical-services.github.io/NLP-guidance/FeatureSelection.html\n",
    "* https://towardsdatascience.com/feature-selection-on-text-classification-1b86879f548e\n",
    "* https://machinelearningmastery.com/feature-selection-machine-learning-python/\n",
    "* https://nlp.stanford.edu/IR-book/html/htmledition/contents-1.html\n",
    "* http://blog.datumbox.com/using-feature-selection-methods-in-text-classification/\n",
    "* https://www.nltk.org/book/\n",
    "* https://spacy.io/usage/spacy-101/\n",
    "* https://course.spacy.io/\n",
    "* https://pythonspot.com/nltk-stop-words/\n",
    "* https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "* https://medium.com/@kangeugine/hidden-markov-model-7681c22f5b9\n",
    "* https://www.nltk.org/book/ch05.html\n",
    "* https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "* https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da\n",
    "* https://nlpforhackers.io/training-pos-tagger/\n",
    "* https://nlpforhackers.io/named-entity-extraction/\n",
    "* https://nlpforhackers.io/training-ner-large-dataset/\n",
    "\n",
    "## The Bag Of Words Model\n",
    "\n",
    "To get to a point where we can effectively model language, we need to consider it grounded in some task.  Here our task will always be the same, at a high level, how does a machine understand language?  You'll find the specific tasks that follow are mere consequences of this overarching theme of first, how do we understand language?  And as a secondary question, what aspects of language are useful for our specific understanding?  Therefore, any model, in principal will be interested in segmenting natural language into a decomposed or component state.\n",
    "\n",
    "Let's begin with our first model and a specific sub-task, which will inform what's important.  Here we consider the bag of words model and the task of simple information retrieval.  \n",
    "\n",
    "For this system we will need two parts:\n",
    "\n",
    "* a system for transforming the text into numbers\n",
    "* a system for doing the information retrival\n",
    "\n",
    "Let's first define the text into numbers part of the system - \n",
    "\n",
    "## Enter the Bag Of Words Model\n",
    "\n",
    "The Bag of words model is possibly the simplest model you could think of, let's see some code to implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hello': 1, 'there': 1, 'friends': 1, 'how': 1, 'are': 1, 'you': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def get_bag_of_words(text):\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    text = regex.sub('', text)\n",
    "    tokens = [elem.rstrip() for elem in text.split(\" \")]\n",
    "    return {token:tokens.count(token) for token in tokens}\n",
    "\n",
    "sentence = \"Hello there friends, how are you?\"\n",
    "get_bag_of_words(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of things to notice here:\n",
    "\n",
    "1. we don't want to keep the punctuation\n",
    "2. We now have a set of numbers that have lost semantic meaning\n",
    "\n",
    "Now let's go about defining our simplicitic informaiton retrevial system.\n",
    "\n",
    "Let's assume that we have a web application that should query something different depending on what a user types in.  We give them a \"search\" bar to look up information.  Let's assume for simplicity, that they can only type in keywords.\n",
    "\n",
    "A good example for this is job search based on keyword.  Here, someone enters a role, like \"data scientist\" and open data science roles are returned.  How could we return the results?  Well the simplest way is with a look up table.  Now we are in a position to set up the second part of the system.  Let's see how that might get coded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 1), ('scientist', 1), ('engineer', 0), ('manager', 0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Looking for a mid level data scientist'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def get_bag_of_words(text, space_of_words):\n",
    "    text = text.lower()\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    text = regex.sub('', text)\n",
    "    tokens = [elem.rstrip() for elem in text.split(\" \")]\n",
    "    return [(word,tokens.count(word)) for word in space_of_words]\n",
    "\n",
    "def jobs_to_return(job_phrase):\n",
    "    if job_phrase[0][1] >= 1:\n",
    "        if job_phrase[1][1] >= 1:\n",
    "            return \"Looking for a mid level data scientist\"\n",
    "        elif job_phrase[2][1] >= 1:\n",
    "            return \"Looking for a senior data engineer\"\n",
    "        elif job_phrase[3][1] >= 1:\n",
    "            return \"Looking for an experienced data manager\"\n",
    "    else:\n",
    "        return \"Sorry, we don't have any jobs\"\n",
    "\n",
    "space_of_words = \"data scientist engineer manager\".split()\n",
    "sentence = \"Data Scientist\"\n",
    "job_phrase = get_bag_of_words(sentence, space_of_words)\n",
    "print(job_phrase)\n",
    "jobs_to_return(job_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see here this system is extremely simplistic, but it shows a possible design that could be implemented in the real world - If you have a web connection, you can check out a system I helped with called CALC that uses this very idea:\n",
    "\n",
    "[https://calc.gsa.gov/](https://calc.gsa.gov/)\n",
    "\n",
    "Check it out!\n",
    "\n",
    "So far we've built an incredibly symplistic search engine.  One we could make it more natural, is by allowing for more flexible queries that help humans express what they are after, but that aren't important for our query.\n",
    "\n",
    "## Enter Stop Words\n",
    "\n",
    "First we'll see an example of how to do stop words on their own and then we'll add stop words to our system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fred, looking for new car, do you have any recommendations?'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stop_words(sentence):\n",
    "    stop_words = \"hey i a i'm\".split()\n",
    "    return \" \".join([word \n",
    "                     for word in sentence.split() \n",
    "                     if word.lower() not in stop_words])\n",
    "\n",
    "sentence = \"Hey Fred, I'm looking for a new car, do you have any recommendations?\"\n",
    "remove_stop_words(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea here is that we remove words that occur a lot in every day language, but don't hold semantically relevant information.  Most stop word lists are standard and come from analysis of major bodies of text called corpora or corpus in the singular.  From these corpora are massive bodies of text, that are supposed to capture the frequency of language in a general setting.  Of course, domain specific corpora exist as well.  For instance, words in the medical community are likely to have a different frequency and usage than in say the gaming community.  So we can differentiate dialetics and communities, in theory, from the language they use and directly from the frequency and occurence of different types of words.\n",
    "\n",
    "Let's look at a standard set of stop words, from a very popular natural language processing library - Natural Language Toolkit (NLTK):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "\n",
      "[\"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n",
      "\n",
      "['himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself']\n",
      "\n",
      "['they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this']\n",
      "\n",
      "['that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be']\n",
      "\n",
      "['been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing']\n",
      "\n",
      "['a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until']\n",
      "\n",
      "['while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into']\n",
      "\n",
      "['through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down']\n",
      "\n",
      "['in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once']\n",
      "\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n",
      "\n",
      "['few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only']\n",
      "\n",
      "['own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will']\n",
      "\n",
      "['just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o']\n",
      "\n",
      "['re', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\"]\n",
      "\n",
      "['doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\"]\n",
      "\n",
      "['ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn']\n",
      "\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "words = stopwords.words(\"english\")\n",
    "for index in range(10, len(words), 10):\n",
    "    print(words[index-10:index])\n",
    "    print()\n",
    "print(len(stopwords.words(\"english\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are 179 words that occur commonly and don't convey meaning we care about for our information retrieval problem, or for some NLP tasks more generally.  Let's make use of the nltk stop words in or problem to see what we get out now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey Fred, I'm looking new car, recommendations?\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    return \" \".join([word \n",
    "                     for word in sentence.split() \n",
    "                     if word not in stop_words])\n",
    "\n",
    "sentence = \"Hey Fred, I'm looking for a new car, do you have any recommendations?\"\n",
    "remove_stop_words(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we get to the crux of what is being asked for and can now move onto more sophisticated processing.  Let's add the stop words component to our job query engine, so we can add more \"natural\" language querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 2), ('scientist', 1), ('engineer', 1), ('manager', 0)]\n",
      "Looking for a mid level data scientist\n",
      "Looking for a senior data engineer\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    return \" \".join([word \n",
    "                     for word in sentence.split() \n",
    "                     if word not in stop_words])\n",
    "\n",
    "def get_bag_of_words(text, space_of_words):\n",
    "    text = text.lower()\n",
    "    text = remove_stop_words(text)\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    text = regex.sub('', text)\n",
    "    tokens = [elem.rstrip() for elem in text.split(\" \")]\n",
    "    return [(word,tokens.count(word)) for word in space_of_words]\n",
    "    \n",
    "def jobs_to_return(job_phrase):\n",
    "    results = []\n",
    "    if job_phrase[0][1] >= 1:\n",
    "        if job_phrase[1][1] >= 1:\n",
    "            results.append(\"Looking for a mid level data scientist\")\n",
    "        if job_phrase[2][1] >= 1:\n",
    "            results.append(\"Looking for a senior data engineer\")\n",
    "        if job_phrase[3][1] >= 1:\n",
    "            results.append(\"Looking for an experienced data manager\")\n",
    "    else:\n",
    "        results.append(\"Sorry, we don't have any jobs\")\n",
    "    return results\n",
    "\n",
    "space_of_words = \"data scientist engineer manager\".split()\n",
    "sentence = \"I'm looking for a Data Scientist job or a Data Engineer job\"\n",
    "job_phrases = get_bag_of_words(sentence, space_of_words)\n",
    "print(job_phrases)\n",
    "for job in jobs_to_return(job_phrases):\n",
    "    print(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we now have a more flexible search engine because of the preprocessing we've done so far.  However we can pretty easily break our engine if we change our query slightly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 2), ('scientist', 1), ('engineer', 0), ('manager', 0)]\n",
      "Looking for a mid level data scientist\n"
     ]
    }
   ],
   "source": [
    "space_of_words = \"data scientist engineer manager\".split()\n",
    "sentence = \"I'm looking for a Data Scientist job or a Data Engineering job\"\n",
    "job_phrases = get_bag_of_words(sentence, space_of_words)\n",
    "print(job_phrases)\n",
    "for job in jobs_to_return(job_phrases):\n",
    "    print(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened?!  Well it turns out by changing \"data engineer\" to the more natural \"data engineering\", we lose our second search result.  In order to recover it let's introduce our next concept - stemming\n",
    "\n",
    "## Enter Stemming\n",
    "\n",
    "Stemming is the idea of taking the stem of a word.  So in this case, the stem of engineering is engineer.  Stemming will also handle the case of engineer versus engineers.  Basically extra pieces of grammatical syntax are removed.  This is sort of like a form of regularization for words.  Because we create a standard representation for related words that only have slight variance in meaning.\n",
    "\n",
    "Let's look at how we might implement a stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I heard you have the hiccup, have you tried jump up and down?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def get_stem(word):\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    word_no_punc = regex.sub('', word)\n",
    "    diff = [char for char in word if char not in word_no_punc]\n",
    "    if word_no_punc.endswith(\"s\"):\n",
    "        return word_no_punc[:-1] + \"\".join(diff)\n",
    "    elif word_no_punc.endswith(\"ing\"):\n",
    "        return word_no_punc[:-3] + \"\".join(diff)\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "sentence = \"I heard you have the hiccups, have you tried jumping up and down?\"\n",
    "\" \".join([get_stem(word) for word in sentence.split(\" \")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we are able to get the stem of the word jumping - in this case jump.  Let's look at another example where our simplistic stemmer fais: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey!  Are you go runn  later?  I'd love to come with you.\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hey!  Are you going running  later?  I'd love to come with you.\"\n",
    "\" \".join([get_stem(word) for word in sentence.split(\" \")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, \"runn\" is not the stem of \"running\", so we need a quite sophisticated stemmer to handle all the cases, essentially writing down a lot of grammatical rules and edge cases.  Because that in it of itself would be a large enough project, we won't do that here.  Instead we will make use of an off the shelf stemmer, in this case again from NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run caress fli die mule deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def get_stem(word):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "plurals = ['running', 'caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "           'plotted']\n",
    "\n",
    "singles = [get_stem(plural) for plural in plurals]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the \"running\" case is now well handled.  Other cases aren't handled perfectly but it's still decent.  Here's an example of our off the shelf stemmer performing well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous\n"
     ]
    }
   ],
   "source": [
    "print(SnowballStemmer(\"english\").stem(\"generously\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be advised, not all stemmers are created equal, and we can probably always do better by covering more edge cases.  Here is an example of a classic stemmer that doesn't do as well in this case, but is generally pretty good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gener\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "print(PorterStemmer().stem(\"generously\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, the goal of a good stemmer is that the stemmer gives us the singular case.  Sometimes this is easy, other times, not so much.  So there is always a trade off.  Of course, if we can standardize to some typical expectation for a stem and get the root meaning of the word in question, then it doesn't matter if we cover all the edge cases, of which there will always be an ever expanding list.\n",
    "\n",
    "Let's incorporate our Snowball stemmer into our engine to increase the size of our query surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for sentence one:\n",
      "Job phrases [('data', 2), ('scientist', 1), ('engin', 1), ('manager', 0)]\n",
      "Looking for a mid level data scientist\n",
      "Looking for a senior data engineer\n",
      "\n",
      "results for sentence two:\n",
      "Job phrases [('data', 2), ('scientist', 1), ('engin', 1), ('manager', 0)]\n",
      "Looking for a mid level data scientist\n",
      "Looking for a senior data engineer\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_stem(word):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    return \" \".join([word \n",
    "                     for word in sentence.split() \n",
    "                     if word not in stop_words])\n",
    "\n",
    "def get_bag_of_words(text, space_of_words):\n",
    "    text = text.lower()\n",
    "    text = remove_stop_words(text)\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    text = regex.sub('', text)\n",
    "    tokens = [elem.rstrip() for elem in text.split(\" \")]\n",
    "    tokens = [get_stem(token) for token in tokens]\n",
    "    return [(word,tokens.count(word)) for word in space_of_words]\n",
    "    \n",
    "def jobs_to_return(job_phrase):\n",
    "    results = []\n",
    "    if job_phrase[0][1] >= 1:\n",
    "        if job_phrase[1][1] >= 1:\n",
    "            results.append(\"Looking for a mid level data scientist\")\n",
    "        if job_phrase[2][1] >= 1:\n",
    "            results.append(\"Looking for a senior data engineer\")\n",
    "        if job_phrase[3][1] >= 1:\n",
    "            results.append(\"Looking for an experienced data manager\")\n",
    "    else:\n",
    "        results.append(\"Sorry, we don't have any jobs\")\n",
    "    return results\n",
    "\n",
    "def process_query(query):\n",
    "    space_of_words = \"data scientist engin manager\".split()\n",
    "    job_phrases = get_bag_of_words(query, space_of_words)\n",
    "    print(\"Job phrases\", job_phrases)\n",
    "    for job in jobs_to_return(job_phrases):\n",
    "        print(job)\n",
    "        \n",
    "\n",
    "sentence_one = \"I'm looking for a Data Scientist job or a Data Engineer job\"\n",
    "print(\"results for sentence one:\")\n",
    "process_query(sentence_one)\n",
    "print()\n",
    "print(\"results for sentence two:\")\n",
    "sentence_two = \"I'm looking for a Data Scientist job or a Data Engineering job\"\n",
    "process_query(sentence_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that now both of our cases work now!  Of course, we had to change our recognized word for \"engineer\" to \"engin\" which is not ideal.  However, this does mean we can cover more cases.  With all things, there is a trade off between specificity and flexibility.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for sentence two:\n",
      "Job phrases [('data', 2), ('scientist', 1), ('engin', 1), ('manager', 0)]\n",
      "Looking for a mid level data scientist\n",
      "Looking for a senior data engineer\n"
     ]
    }
   ],
   "source": [
    "print(\"results for sentence two:\")\n",
    "sentence_two = \"I'm looking for a Data Scientist job or a Data Engine job\"\n",
    "process_query(sentence_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, we possibly get a false positive.  Of course, I don't think \"data engine job\" is a thing, so the only way this comes up is a typo.  \n",
    "\n",
    "What if we wanted even more flexibility in our query?  Well we can take our processing even further with a technique called lemmatization.  Lemmatization is similar to stemming in that, the central component of the word is preserved while all other pieces are disgarded.  The major difference between stemming and lemmatization is stemming is based on a rules engine, while lemmatization makes us of formal theory to find the root of the word.  The stanford nlp group even goes so far as to call lemmatization the \"right way\", while stemming is seen as \"a crude rules engine\".  \n",
    "\n",
    "## Enter Lemmatization\n",
    "\n",
    "Because lemmatization is sophisticated, we won't attempt implementation here, but instead simply make use of NLTK's solution by way of example first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\", pos=''))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we can even add a part of speech for increases flexability and to ensure a greater degree of correctness.  Let's go back to our example above and see how our lemmatizer does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running caress fly dy mule denied died agreed owned humbled sized meeting stating siezing itemization sensational traditional reference colonizer plotted\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(word)\n",
    "\n",
    "plurals = ['running', 'caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "           'plotted']\n",
    "\n",
    "singles = [get_lemma(plural) for plural in plurals]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, on some of the cases lemmatization doesn't work that well at all, but in other cases, lemmatizing does significantly better.  For example, fly is handled much better with the lemmatizer than the stemmer.  \n",
    "\n",
    "The reason the lemmatizer appears to do not as well, is because it doesn't have the part of speech.  Note that in the first example, we hinted that we'll need part of speech in order to do a good job.  For this we will need to consider some part of speech tagging automatically, or be forced to do this manually ourselves (the horror!!!).\n",
    "\n",
    "Before we move onto explaining part of speech tagging in general, let's look at a motivating example as to why our lemmatizer may be shy about getting the lemmas for our plurals above:\n",
    "\n",
    "`They refuse to permit us to obtain the refuse permit.`\n",
    "\n",
    "In the above sentence, refuse is used twice - once meaning to deny and the second time meaning trash.  The difference in definition is made apparent by the part of speech in use!  So for some other words, the plural maybe the same but without the added context, we can't be sure of the underlying meaning.\n",
    "\n",
    "## Enter Part Of Speech Tagging\n",
    "\n",
    "Part of speech tagging is a wide ranging and increadibly powerful tool.  It's invention is one of the great watershed moments in natural language processing.  With it, we have a basic model of the syntax of natural language and therefore can get a sense of the meaning, via the syntax of the sentence.  \n",
    "\n",
    "Over the years, we've continued to see continual improvement in part of speech tagging because of its central nature to many NLP tasks.  Due to the number of ways we could do part of speech tagging we will only consider a very simple model - n-gram analysis.\n",
    "\n",
    "## N-Gram Analysis\n",
    "\n",
    "The idea behind n-grams are very simple, let's consider the following sentence and it's 1-grams, 2-grams and 3-grams.\n",
    "\n",
    "Sentence: \"I love dogs and cats, they are great!\"\n",
    "\n",
    "1-grams:\n",
    "\n",
    "`[(\"I\"), (\"love\"), (\"dogs\"), (\"and\"), (\"cats\"), (\"they\"), (\"are\"), (\"great\")]`\n",
    "\n",
    "2-grams:\n",
    "\n",
    "`[(\"I love\"), (\"love dogs\"), (\"dogs and\"), (\"and cats\"), (\"cats they\"), (\"they are\"), (\"are great\")]`\n",
    "\n",
    "3-grams:\n",
    "\n",
    "`[(\"I love dogs\"), (\"love dogs and\"), (\"dogs and cats\"), (\"and cats they\"), (\"cats they are\"), (\"they are great\")]`\n",
    "\n",
    "As you can see from the above example an \"n\"-gram is a sliding window that considers sets of words based on the order of their appearance in some text.  The n in question defines how many words occur in each gram, aka the size of the sliding window.  Notice that a single word can appear in more than one gram.\n",
    "\n",
    "Let's see how to code an ngram function in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I',), ('love',), ('dogs',), ('and',), ('cats',), ('they',), ('are',), ('great',)]\n",
      "[('I', 'love'), ('love', 'dogs'), ('dogs', 'and'), ('and', 'cats'), ('cats', 'they'), ('they', 'are'), ('are', 'great')]\n",
      "[('I', 'love', 'dogs'), ('love', 'dogs', 'and'), ('dogs', 'and', 'cats'), ('and', 'cats', 'they'), ('cats', 'they', 'are'), ('they', 'are', 'great')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def ngram(n, text):\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    text = regex.sub('', text)\n",
    "    tokens = text.split(\" \")\n",
    "    return list(zip(*[tokens[i:] for i in range(n)]))\n",
    "\n",
    "text = \"I love dogs and cats, they are great!\"\n",
    "print(ngram(1, text))\n",
    "print(ngram(2, text))\n",
    "print(ngram(3, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we get the same thing from the algorithm as we did from the example above!  \n",
    "\n",
    "So how does this help us with part of speech tagging?  Well we can build what's called a markov model - that is a model that decides what to do next based on the most recent piece of memory.  Specifically we'll use n-grams to make a simple markov model - one that only takes into account the most recent parts of speech.\n",
    "\n",
    "Specifically, we'll take in the current word and the n-1 most recent word's parts of speech and use those as context to figure out what the most likely part of speech is for the current word.  \n",
    "\n",
    "Next let's implement a simple markov model for unigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class MarkovModel:\n",
    "    def __init__(self):\n",
    "        self.parts_of_speech = ['CC', 'CD', 'DT', 'EX', 'FW', \n",
    "                                'IN', 'JJ', 'JJR', 'JJS', 'LS', \n",
    "                                'MD', 'NN', 'NNS', 'NNP', 'NNPS', \n",
    "                                'PDT', 'POS', 'PRP', 'PRP$', 'RB', \n",
    "                                'RBR', 'RBS', 'RP', 'TO', 'UH', \n",
    "                                'VB', 'VBD', 'VBG', 'VBN', 'VBP', \n",
    "                                'VBZ', 'WDT', 'WP', 'WP$', 'WRB', \n",
    "                                \"AT\", \"BER\", \"BEG\", \"CS\", \"BEZ\"]\n",
    "        self.part_of_speech_map = {pos:index \n",
    "                                   for index, pos in enumerate(self.parts_of_speech)}\n",
    "        self.probabilities = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.probabilities = defaultdict(list)\n",
    "        for index, elem in enumerate(X):\n",
    "            if elem not in self.probabilities:\n",
    "                self.probabilities[elem] = [0] * len(self.parts_of_speech)\n",
    "            pos_index = self.part_of_speech_map[y[index]]\n",
    "            self.probabilities[elem][pos_index] += 1\n",
    "    \n",
    "    def _get_max_index(self, elem):\n",
    "        return self.probabilities[elem].index(\n",
    "            max(self.probabilities[elem])\n",
    "        )\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pos = []\n",
    "        for elem in X:\n",
    "            index = self._get_max_index(elem)\n",
    "            pos.append(\n",
    "                self.parts_of_speech[index]\n",
    "            )\n",
    "        return pos\n",
    "\n",
    "def ngram(n, text):\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    text = regex.sub('', text)\n",
    "    tokens = text.split(\" \")\n",
    "    return list(zip(*[tokens[i:] for i in range(n)]))\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    count = 0\n",
    "    for index in range(len(y_true)):\n",
    "        if y_true[index] == y_pred[index]:\n",
    "            count += 1\n",
    "    return count / len(y_true)\n",
    "\n",
    "mm = MarkovModel()\n",
    "X = \"Various of the apartments are of the terrace type, being on the ground floor so that entrace is direct.\"\n",
    "y = [\"JJ\", \"IN\", \"AT\", \"NNS\", \"BER\", \"IN\", \"AT\", \"NN\", \"NN\", \"BEG\", \"IN\", \"AT\", \"NN\", \"NN\", \"CS\", \"CS\", \"NN\", \"BEZ\", \"JJ\"]\n",
    "X = ngram(1, X)\n",
    "mm.fit(X, y)\n",
    "y_pred = mm.predict(X)\n",
    "accuracy(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "of course, our model does really well because we tested on the same set we trained on.  Let's see the nltk equivalent at work on a real set of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Various', 'JJ')\n",
      "('of', 'IN')\n",
      "('the', 'AT')\n",
      "('apartments', 'NNS')\n",
      "('are', 'BER')\n",
      "('of', 'IN')\n",
      "('the', 'AT')\n",
      "('terrace', 'NN')\n",
      "('type', 'NN')\n",
      "(',', ',')\n",
      "('being', 'BEG')\n",
      "('on', 'IN')\n",
      "('the', 'AT')\n",
      "('ground', 'NN')\n",
      "('floor', 'NN')\n",
      "('so', 'QL')\n",
      "('that', 'CS')\n",
      "('entrance', 'NN')\n",
      "('is', 'BEZ')\n",
      "('direct', 'JJ')\n",
      "('.', '.')\n",
      "0.9349006503968017\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "brown_sents = brown.sents(categories='news')\n",
    "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
    "for pair in unigram_tagger.tag(brown_sents[2007]):\n",
    "    print(pair)\n",
    "print(unigram_tagger.evaluate(brown_tagged_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I leave it as an exercise to extend our model to the n-gram case.  But let's look at how easy it is to use Bigrams with nltk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Various', 'JJ')\n",
      "('of', 'IN')\n",
      "('the', 'AT')\n",
      "('apartments', 'NNS')\n",
      "('are', 'BER')\n",
      "('of', 'IN')\n",
      "('the', 'AT')\n",
      "('terrace', 'NN')\n",
      "('type', 'NN')\n",
      "(',', ',')\n",
      "('being', 'BEG')\n",
      "('on', 'IN')\n",
      "('the', 'AT')\n",
      "('ground', 'NN')\n",
      "('floor', 'NN')\n",
      "('so', 'CS')\n",
      "('that', 'CS')\n",
      "('entrance', 'NN')\n",
      "('is', 'BEZ')\n",
      "('direct', 'JJ')\n",
      "('.', '.')\n",
      "0.7892972929967977\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "brown_sents = brown.sents(categories='news')\n",
    "bigram_tagger = nltk.BigramTagger(brown_tagged_sents)\n",
    "for pair in bigram_tagger.tag(brown_sents[2007]):\n",
    "    print(pair)\n",
    "print(bigram_tagger.evaluate(brown_tagged_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, for this dataset using bigrams does not improve our models accuracy.  \n",
    "\n",
    "## Adding Part of Speech Tagging to Lemmatization\n",
    "\n",
    "Now that we know how to build a part of speech tagger, let's make use of the off the shelf one from nltk in order  to our lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run caress fly dy mule deny die agree own humble size meeting state siezing itemization sensational traditional reference colonizer plot\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "def get_pos_map(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def get_lemma(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pos = nltk.pos_tag([word])[0][1]\n",
    "    pos = get_pos_map(pos)\n",
    "    return lemmatizer.lemmatize(word, pos=pos)\n",
    "\n",
    "plurals = ['running', 'caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "           'plotted']\n",
    "\n",
    "singles = [get_lemma(plural) for plural in plurals]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah!  Look at that improvement!  A lot more of the cases get covered now, even for this reasonably hard dataset. \n",
    "\n",
    "## Improving Our Information Retrieval System\n",
    "\n",
    "Now let's expand our information retrieval task.  Let's include lemmatization and n-grams, so now we'll be able to parse phrases from our query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for sentence one:\n",
      "Job phrases [('data scientist', 1), ('data engineer', 1), ('machine learning engineer', 0), ('data science managerdata visualization engineer', 0), ('data engineering lead', 0), ('junior data engineer', 1), ('senior data scientist', 1)]\n",
      "We have a data scientist available\n",
      "We have a data engineer available\n",
      "We have a junior data engineer available\n",
      "We have a senior data scientist available\n",
      "\n",
      "results for sentence two:\n",
      "Job phrases [('data scientist', 1), ('data engineer', 1), ('machine learning engineer', 0), ('data science managerdata visualization engineer', 0), ('data engineering lead', 0), ('junior data engineer', 1), ('senior data scientist', 1)]\n",
      "We have a data scientist available\n",
      "We have a data engineer available\n",
      "We have a junior data engineer available\n",
      "We have a senior data scientist available\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_pos_map(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def get_lemma(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pos = nltk.pos_tag([word])[0][1]\n",
    "    pos = get_pos_map(pos)\n",
    "    return lemmatizer.lemmatize(word, pos=pos)\n",
    "    \n",
    "def remove_stop_words(sentence):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    return \" \".join([word \n",
    "                     for word in sentence.split() \n",
    "                     if word not in stop_words])\n",
    "\n",
    "def get_bag_of_words(text, space_of_words):\n",
    "    text = text.lower()\n",
    "    text = remove_stop_words(text)\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    text = regex.sub('', text)\n",
    "    tokens = [elem.rstrip() for elem in text.split(\" \")]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    text = \" \".join(tokens)\n",
    "    tokens = ngram(1, text) + ngram(2, text) + ngram(3, text)\n",
    "    tokens = post_process_grams(tokens)\n",
    "    return [(word,tokens.count(word)) for word in space_of_words]\n",
    "    \n",
    "def ngram(n, text):\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    text = regex.sub('', text)\n",
    "    tokens = text.split(\" \")\n",
    "    return list(zip(*[tokens[i:] for i in range(n)]))\n",
    "\n",
    "def post_process_grams(grams):\n",
    "    return [\" \".join(elem) for elem in grams]\n",
    "\n",
    "def jobs_to_return(job_phrase):\n",
    "    return [\"We have a \"+job[0]+\" available\" \n",
    "            for job in job_phrase if job[1] >= 1]\n",
    "\n",
    "def process_query(query):\n",
    "    space_of_words = [\"data scientist\", \"data engineer\", \n",
    "                      \"machine learning engineer\", \"data science manager\"\n",
    "                     \"data visualization engineer\", \"data engineering lead\",\n",
    "                     \"junior data engineer\", \"senior data scientist\"]\n",
    "    job_phrases = get_bag_of_words(query, space_of_words)\n",
    "    print(\"Job phrases\", job_phrases)\n",
    "    for job in jobs_to_return(job_phrases):\n",
    "        print(job)\n",
    "        \n",
    "\n",
    "sentence_one = \"I'm looking for a Senior Data Scientist job or a Junior Data Engineer job\"\n",
    "print(\"results for sentence one:\")\n",
    "process_query(sentence_one)\n",
    "print()\n",
    "print(\"results for sentence two:\")\n",
    "sentence_two = \"I'm looking for a Data Scientist job or a Data Engineering job\"\n",
    "process_query(sentence_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now able to process more sophistcated queries such as whether the individual is senior or junior!  And now we don't need the kludgy thing we did before with \"engin\" instead of engineer or engineering!!\n",
    "\n",
    "So far we've look at adding syntactic meaning to our data, next let's see how to add semantic meaning to our data with named entity recognition!\n",
    "\n",
    "## Enter Named Entity Recognition\n",
    "\n",
    "Named entity recognition is the process of adding meta data to proper nouns, identifying them by a category or possibly categories.  The process for training a named entity recognition system is very similar to training a part of speech tagger.  The features that go into the system are of course different as are the labels that come out but other than that the process is very similar.  \n",
    "\n",
    "So let's go ahead and look at how nltk handles named entity recognition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  'm/VBP\n",
      "  looking/VBG\n",
      "  for/IN\n",
      "  a/DT\n",
      "  (ORGANIZATION Senior/NNP Data/NNP Scientist/NNP)\n",
      "  job/NN\n",
      "  or/CC\n",
      "  a/DT\n",
      "  (ORGANIZATION Junior/NNP Data/NNP Engineer/NNP)\n",
      "  job/NN)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def get_entities(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    sent = nltk.ne_chunk(sent)\n",
    "    return sent\n",
    "\n",
    "print(get_entities(\"I'm looking for a Senior Data Scientist job or a Junior Data Engineer job\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK parser thinks that the jobs are organizations, which isn't really accurate, but it doesn pick up that they are named entities!  Let's take a look how spacy, another nlp library does on this task, with the same sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Junior Data Engineer', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# this can be downloaded from the command line with:\n",
    "# python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I'm looking for a Senior Data Scientist job or a Junior Data Engineer job\")\n",
    "print([(token.text, token.label_) for token in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like spacy doesn't do as well.  Let's try another case to see how the two fair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE European/JJ)\n",
      "  authorities/NNS\n",
      "  fined/VBD\n",
      "  (PERSON Google/NNP)\n",
      "  a/DT\n",
      "  record/NN\n",
      "  $/$\n",
      "  5.1/CD\n",
      "  billion/CD\n",
      "  on/IN\n",
      "  Wednesday/NNP\n",
      "  for/IN\n",
      "  abusing/VBG\n",
      "  its/PRP$\n",
      "  power/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  mobile/JJ\n",
      "  phone/NN\n",
      "  market/NN\n",
      "  and/CC\n",
      "  ordered/VBD\n",
      "  the/DT\n",
      "  company/NN\n",
      "  to/TO\n",
      "  alter/VB\n",
      "  its/PRP$\n",
      "  practices/NNS)\n"
     ]
    }
   ],
   "source": [
    "print(get_entities('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here NLTK gets one of the entities wrong - it thinks google is a person, but get's European correct - as it is a geopolitical organization!  Let's see how spacy fairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('European', 'NORP'), ('Google', 'ORG'), ('$5.1 billion', 'MONEY'), ('Wednesday', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')\n",
    "print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this case, spacy does way better!  It correctly identifies all the entities!  Even picking up some of the ones nltk missed!\n",
    "\n",
    "## Training your own Part Of Speech Tagger\n",
    "\n",
    "So far we've implemented our own algorithms for these NLP tasks.  But what if we wanted to use an off the shelf model from scikit-learn?\n",
    "\n",
    "Well that's what we are going to see how to do now!  There won't be a ton of theory here, but we'll get to see how we can possibly improve performance over what people have done implemented in the major libraries.  This will also become incredibly important when we get to the active learning section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('my', 'PRP$'), ('friend', 'NN'), (',', ','), ('John', 'NNP'), ('.', '.')]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           $       1.00      1.00      1.00        25\n",
      "          ''       1.00      1.00      1.00        42\n",
      "           ,       1.00      1.00      1.00       271\n",
      "       -LRB-       1.00      1.00      1.00         4\n",
      "      -NONE-       1.00      1.00      1.00       427\n",
      "       -RRB-       1.00      1.00      1.00         4\n",
      "           .       1.00      1.00      1.00       248\n",
      "           :       1.00      1.00      1.00        33\n",
      "          CC       0.99      0.97      0.98       151\n",
      "          CD       0.99      0.98      0.99       173\n",
      "          DT       0.99      0.98      0.98       527\n",
      "          EX       1.00      1.00      1.00         7\n",
      "          IN       0.97      0.93      0.95       627\n",
      "          JJ       0.77      0.80      0.78       350\n",
      "         JJR       0.79      0.65      0.71        17\n",
      "         JJS       1.00      0.92      0.96        12\n",
      "          MD       1.00      1.00      1.00        41\n",
      "          NN       0.87      0.90      0.88       833\n",
      "         NNP       0.95      0.96      0.95       649\n",
      "        NNPS       1.00      0.27      0.43        11\n",
      "         NNS       0.91      0.96      0.93       394\n",
      "         PDT       0.00      0.00      0.00         1\n",
      "         POS       0.95      1.00      0.97        72\n",
      "         PRP       0.94      0.93      0.93       108\n",
      "        PRP$       0.88      0.91      0.90        58\n",
      "          RB       0.83      0.81      0.82       166\n",
      "         RBR       0.70      0.50      0.58        14\n",
      "         RBS       1.00      1.00      1.00         1\n",
      "          RP       0.75      0.75      0.75        16\n",
      "         SYM       0.00      0.00      0.00         1\n",
      "          TO       1.00      1.00      1.00       117\n",
      "          VB       0.88      0.80      0.84       144\n",
      "         VBD       0.89      0.89      0.89       206\n",
      "         VBG       0.87      0.86      0.87        94\n",
      "         VBN       0.80      0.83      0.81       122\n",
      "         VBP       0.88      0.75      0.81        91\n",
      "         VBZ       0.90      0.78      0.83       138\n",
      "         WDT       0.40      0.94      0.56        18\n",
      "          WP       1.00      0.92      0.96        25\n",
      "         WRB       0.94      1.00      0.97        15\n",
      "          ``       1.00      1.00      1.00        43\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      6296\n",
      "   macro avg       0.87      0.85      0.85      6296\n",
      "weighted avg       0.93      0.92      0.92      6296\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from functools import partial\n",
    "\n",
    "def get_features(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }\n",
    " \n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]\n",
    "  \n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    " \n",
    "    for tagged in tagged_sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(get_features(untag(tagged), index))\n",
    "            y.append(tagged[index][1])\n",
    " \n",
    "    return X, y\n",
    "\n",
    "def part_of_speech_tagger(clf, sentence):\n",
    "    tags = clf.predict([get_features(sentence, index) for index in range(len(sentence))])\n",
    "    return list(zip(sentence, tags))\n",
    "\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    "data = tagged_sentences[:1000]\n",
    "X, y = transform_to_dataset(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=False)),\n",
    "    ('classifier', DecisionTreeClassifier(criterion='entropy'))\n",
    "])\n",
    " \n",
    "clf.fit(X_train, y_train)   \n",
    "y_pred = clf.predict(X_test)\n",
    "pos_tag = partial(part_of_speech_tagger, clf)\n",
    "print(pos_tag(nltk.word_tokenize('This is my friend, John.')))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the crux of the work comes from having an already tagged dataset and doing the right feature engineering.  Then is is just a multiclass classification problem!  Even a somewhat naive model does quiet well as we can see from the classification report.  It does seem like some of the support for some of our classes is quiet small, so we might need to be careful with how much we trust our predictions overall, but for a worked example, this is pretty good!\n",
    "\n",
    "We could do the same thing for Named Entity recognition as this [blog post](https://nlpforhackers.io/named-entity-extraction/) outlines, however downloading the dataset can be a pain, so we won't show the full code here, but we will look at the get_features function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(tokens, index, history):\n",
    "    \"\"\"\n",
    "    `tokens`  = a POS-tagged sentence [(w1, t1), ...]\n",
    "    `index`   = the index of the token we want to extract features for\n",
    "    `history` = the previous predicted IOB tags\n",
    "    \"\"\"\n",
    " \n",
    "    # init the stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    " \n",
    "    # Pad the sequence with placeholders\n",
    "    tokens = [('[START2]', '[START2]'), ('[START1]', '[START1]')] + list(tokens) + [('[END1]', '[END1]'), ('[END2]', '[END2]')]\n",
    "    history = ['[START2]', '[START1]'] + list(history)\n",
    " \n",
    "    # shift the index with 2, to accommodate the padding\n",
    "    index += 2\n",
    " \n",
    "    word, pos = tokens[index]\n",
    "    prevword, prevpos = tokens[index - 1]\n",
    "    prevprevword, prevprevpos = tokens[index - 2]\n",
    "    nextword, nextpos = tokens[index + 1]\n",
    "    nextnextword, nextnextpos = tokens[index + 2]\n",
    "    previob = history[index - 1]\n",
    "    contains_dash = '-' in word\n",
    "    contains_dot = '.' in word\n",
    "    allascii = all([True for c in word if c in string.ascii_lowercase])\n",
    " \n",
    "    allcaps = word == word.capitalize()\n",
    "    capitalized = word[0] in string.ascii_uppercase\n",
    " \n",
    "    prevallcaps = prevword == prevword.capitalize()\n",
    "    prevcapitalized = prevword[0] in string.ascii_uppercase\n",
    " \n",
    "    nextallcaps = prevword == prevword.capitalize()\n",
    "    nextcapitalized = prevword[0] in string.ascii_uppercase\n",
    " \n",
    "    return {\n",
    "        'word': word,\n",
    "        'lemma': stemmer.stem(word),\n",
    "        'pos': pos,\n",
    "        'all-ascii': allascii,\n",
    " \n",
    "        'next-word': nextword,\n",
    "        'next-lemma': stemmer.stem(nextword),\n",
    "        'next-pos': nextpos,\n",
    " \n",
    "        'next-next-word': nextnextword,\n",
    "        'nextnextpos': nextnextpos,\n",
    " \n",
    "        'prev-word': prevword,\n",
    "        'prev-lemma': stemmer.stem(prevword),\n",
    "        'prev-pos': prevpos,\n",
    " \n",
    "        'prev-prev-word': prevprevword,\n",
    "        'prev-prev-pos': prevprevpos,\n",
    " \n",
    "        'prev-iob': previob,\n",
    " \n",
    "        'contains-dash': contains_dash,\n",
    "        'contains-dot': contains_dot,\n",
    " \n",
    "        'all-caps': allcaps,\n",
    "        'capitalized': capitalized,\n",
    " \n",
    "        'prev-all-caps': prevallcaps,\n",
    "        'prev-capitalized': prevcapitalized,\n",
    " \n",
    "        'next-all-caps': nextallcaps,\n",
    "        'next-capitalized': nextcapitalized,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than that the code is more or less the same.  I leave it as an exercise to the reader to download the data and then process it.  Unfortunately, this can take quite a while so you might want to try this [other blog post by the same author](https://nlpforhackers.io/training-ner-large-dataset/), for faster processing time.\n",
    "\n",
    "So we've done some cool stuff - built a toy search engine with a number of features.  But we aren't limited to simply searching over data.  One of the coolest things about the two tools we've looked at - part of speech tagging and named entity recognition, is they give us a sense of the syntax and semantics of text.  In a way, textual data is more complex than other forms of data, because it has both of these features.  Together, the two effect human language in subtle, yet important ways.  \n",
    "\n",
    "For instance:\n",
    "\n",
    "Let's eat Grandma.\n",
    "\n",
    "Is very different than:\n",
    "\n",
    "Let's eat, Grandma.\n",
    "\n",
    "In the first sentence we are going to eat Grandma (oh no!), in the second sentence we are telling Grandma we are going to go eat.\n",
    "\n",
    "And it's overall meaning is entirely change by syntax!  Of course, we as humans know that the first sentence is riddiculous, but machines aren't nearly as smart as us when it comes to processing language, so they are likely to take everything you say literally.  But I digress.\n",
    "\n",
    "Now let's look at another use case for named entity recognition before we move on: Relation Extraction.\n",
    "\n",
    "## Enter Relation Extraction\n",
    "\n",
    "While not directly related to information retrieval or anything we've done so far, this concept is so cool I thought I'd give a quick example of it.  The basic idea is to take pairs of named entities and find a bridge word, to combine them into a tuple of the following kind:\n",
    "\n",
    "(Named entity, bridge word, Named Entity)\n",
    "\n",
    "So for example:\n",
    "\n",
    "(Crystler Building, in, New York City)\n",
    "\n",
    "Let's see a quick programmatic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
      "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
      "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
      "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
      "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
      "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
      "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
      "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
      "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
      "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
      "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
      "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
      "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "    for rel in nltk.sem.extract_rels('ORG', 'LOC', doc,\n",
    "                                     corpus='ieer', pattern = IN):\n",
    "         print(nltk.sem.rtuple(rel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above framework, we can build up a series of node, edge pairs and build a semantic graph of connections.  Then we can define metrics like:\n",
    "\n",
    "* number of times entities have a relationship in a given document\n",
    "* number of times entities have a relationship on average over a series of documents\n",
    "* number of times entities have a relationship on average per paragraph\n",
    "* average number of hops needed to build a relationship between two entities\n",
    "* minimum number of hops between two entities\n",
    "* 10 most frequently occurring pairs\n",
    "* 10 least frequently occurring pairs\n",
    "* most frequent bridge phrase between two entities\n",
    "* least common bridge phrase between two entities\n",
    "* The probability entity A is connected to entity B\n",
    "* The probability entity A is connected to entity B by bridge phrase C\n",
    "* The probability bridge phrase A is used\n",
    "\n",
    "Using these measures we can understand basic features about a set of documents and how different entities inter-relate.  Think of it as the fastest possible reading.  Of course, you lose all the nuance, which can be a problem, so it might be useful to recover the paragraphs around the connection for context reasons.\n",
    "\n",
    "Let's look at an example making use of this notion, first we'll implement each of the metrics as well as write a function for the above code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORG: 'Bayona'] ', which is in the heart of the' [LOC: 'French Quarter']\n",
      "[ORG: 'CBS Studio Center'] 'in' [LOC: 'Studio City']\n",
      "[ORG: 'Smithsonian Institution'] 'in' [LOC: 'Washington']\n",
      "[ORG: 'Metropolitan Museum of Art'] 'in' [LOC: 'New York']\n",
      "[ORG: 'British Museum'] 'door is not in' [LOC: 'Washington']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "class AnalyzeRelationshipsPerDocument:\n",
    "    def __init__(self, relationships):\n",
    "        self.relationships = relationships\n",
    "        self.entities = self.get_entities()\n",
    "        self.bridge_phrases = self.get_bridge_phrases()\n",
    "        self.entity_graph = self._build_graph()\n",
    "        \n",
    "    def get_entities(self):\n",
    "        entities = []\n",
    "        for relationship in self.relationships:\n",
    "            if relationship[0] not in entities:\n",
    "                entities.append(relationship[0])\n",
    "            if relationship[2] not in entities:\n",
    "                entities.append(relationship[2])\n",
    "        return entities\n",
    "    \n",
    "    def get_bridge_phrases(self):\n",
    "        bridge_phrases = []\n",
    "        for relationship in self.relationships:\n",
    "            if relationship[1] not in bridge_phrases:\n",
    "                bridge_phrases.append(relationship[1])\n",
    "        return bridge_phrases\n",
    "    \n",
    "    def entity_freq(self):\n",
    "        frequencies = {}.fromkeys(self.relationships, 0)\n",
    "        for relationship in self.relationships:\n",
    "            frequencies[relationship[0]] += 1\n",
    "            frequencies[relationship[2]] += 1\n",
    "        return frequencies\n",
    "    \n",
    "    def _build_graph(self):\n",
    "        graph = defaultdict(list)\n",
    "        for relationship in relationships:\n",
    "            graph[relationship[0]].append(relationship[2])\n",
    "            graph[relationship[2]].append(relationship[0])\n",
    "        return graph\n",
    "    \n",
    "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980407'):\n",
    "    for rel in nltk.sem.extract_rels('ORG', 'LOC', doc,\n",
    "                                     corpus='ieer', pattern = IN):\n",
    "         print(nltk.sem.rtuple(rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['APW_19980314',\n",
       " 'APW_19980424',\n",
       " 'APW_19980429',\n",
       " 'NYT_19980315',\n",
       " 'NYT_19980403',\n",
       " 'NYT_19980407']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.ieer.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
